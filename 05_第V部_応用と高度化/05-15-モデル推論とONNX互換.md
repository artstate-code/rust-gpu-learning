[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬14ç« ](../04_ç¬¬IVéƒ¨_æ©Ÿæ¢°å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³ã®æ§‹ç¯‰/04-14-ãƒ‡ãƒãƒƒã‚°ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°.md) | [â¡ï¸ ç¬¬16ç« ](05-16-ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©æœ€é©åŒ–ã¨DSLè¨­è¨ˆ.md)

---

# ç¬¬ 12 ç« ã€€ãƒ¢ãƒ‡ãƒ«æ¨è«–ã¨ ONNX äº’æ›

ã“ã®ç« ã§ã¯ã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ï¼ˆInferenceï¼‰ã«ç„¦ç‚¹ã‚’å½“ã¦ã€ONNXï¼ˆOpen Neural Network Exchangeï¼‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨ã®äº’æ›æ€§ã€æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æœ€é©åŒ–ã€Rustã§ã®å®Ÿè£…ã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚

**ç›®çš„**: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®é«˜é€Ÿãƒ»ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãªæ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã€PyTorchã‚„TensorFlowã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’Rustã§å‹•ä½œã•ã›ã‚‹æ–¹æ³•ã‚’ç¿’å¾—ã—ã¾ã™ã€‚

## 12.1 æ¨è«–ã¨å­¦ç¿’ã®é•ã„

### è¨ˆç®—ã‚°ãƒ©ãƒ•ã®é•ã„

**å­¦ç¿’ï¼ˆTrainingï¼‰**:
- Forward + Backward ã®ä¸¡æ–¹ãŒå¿…è¦
- ä¸­é–“å€¤ã‚’å…¨ã¦ä¿å­˜ï¼ˆå‹¾é…è¨ˆç®—ã®ãŸã‚ï¼‰
- ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯å¤§ãã„ï¼ˆä¾‹: 32ã€œ512ï¼‰
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤§ãã„

**æ¨è«–ï¼ˆInferenceï¼‰**:
- Forward ã®ã¿
- ä¸­é–“å€¤ã®ä¿å­˜ãŒä¸è¦
- ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯å°ã•ã„ï¼ˆä¾‹: 1ã€œ16ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã§ã¯1ï¼‰
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒé‡è¦

### æœ€é©åŒ–ã®æ–¹å‘æ€§ã®é•ã„

| é …ç›® | å­¦ç¿’ | æ¨è«– |
|------|------|------|
| **ç›®æ¨™** | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæœ€å¤§åŒ– | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æœ€å°åŒ– |
| **ãƒãƒƒãƒã‚µã‚¤ã‚º** | å¤§ï¼ˆãƒ¡ãƒ¢ãƒªé™ç•Œã¾ã§ï¼‰ | å°ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§å„ªå…ˆï¼‰ |
| **ç²¾åº¦** | FP32/FP16ï¼ˆæ•°å€¤å®‰å®šæ€§ï¼‰ | FP16/INT8/INT4ï¼ˆé€Ÿåº¦å„ªå…ˆï¼‰ |
| **ãƒ¡ãƒ¢ãƒª** | å¤§é‡ä½¿ç”¨å¯ | çœãƒ¡ãƒ¢ãƒª |
| **ä¸¦åˆ—åŒ–** | ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ— | æ¼”ç®—å­ä¸¦åˆ—ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ |
| **æœ€é©åŒ–** | Backwardè€ƒæ…® | Forwardæœ€é©åŒ–ã®ã¿ |

### Pythonï¼ˆPyTorchï¼‰ã§ã®æ¨è«–

```python
import torch

# å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ â†’ æ¨è«–ãƒ¢ãƒ¼ãƒ‰
model.eval()  # Dropout/BatchNormã‚’ç„¡åŠ¹åŒ–

with torch.no_grad():  # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–
    output = model(input_data)

# ã¾ãŸã¯ torch.inference_mode()ï¼ˆPyTorch 1.9+ï¼‰
with torch.inference_mode():
    output = model(input_data)
```

**æœ€é©åŒ–ä¾‹**:

```python
# TorchScript ã§JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
scripted_model = torch.jit.script(model)
scripted_model.save("model.pt")

# ã¾ãŸã¯ traced model
traced_model = torch.jit.trace(model, example_input)
traced_model.save("model_traced.pt")
```

## 12.2 ONNX ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ç†è§£

### ONNX ã¨ã¯

**ONNX (Open Neural Network Exchange)** [^1] ã¯ã€ç•°ãªã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ã§ãƒ¢ãƒ‡ãƒ«ã‚’äº¤æ›ã™ã‚‹ãŸã‚ã®æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã™ã€‚

[^1]: ONNX. https://onnx.ai/

**å¯¾å¿œãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**:
- **ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ**: PyTorch, TensorFlow, scikit-learn, Keras, MXNet
- **ã‚¤ãƒ³ãƒãƒ¼ãƒˆ**: ONNX Runtime, TensorRT, OpenVINO, Core ML, TVM

### ONNX ã®æ§‹é€ 

```mermaid
graph TD
    Model[ModelProto] --> Graph[GraphProto]
    Graph --> Node1[Node: Conv]
    Graph --> Node2[Node: ReLU]
    Graph --> Node3[Node: MaxPool]
    
    Node1 --> Input1[Input Tensor]
    Node1 --> Weight[Weight Tensor]
    Node1 --> Output1[Output Tensor]
    
    Output1 --> Node2
    Node2 --> Output2[Output Tensor]
    
    Output2 --> Node3
    Node3 --> Output3[Output Tensor]
    
    style Model fill:#e1f5ff
    style Graph fill:#ffe1f5
    style Node1 fill:#fff4e1
    style Node2 fill:#fff4e1
    style Node3 fill:#fff4e1
```

**ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**:

1. **ModelProto**: ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®å®šç¾©
2. **GraphProto**: è¨ˆç®—ã‚°ãƒ©ãƒ•
3. **NodeProto**: å€‹åˆ¥ã®æ¼”ç®—å­ï¼ˆConv, ReLUç­‰ï¼‰
4. **TensorProto**: é‡ã¿ãƒ‡ãƒ¼ã‚¿
5. **ValueInfoProto**: ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶ãƒ»å‹æƒ…å ±

### PyTorch ã‹ã‚‰ ONNX ã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

```python
import torch
import torch.onnx

class SimpleModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(3, 64, 3, padding=1)
        self.relu = torch.nn.ReLU()
        self.pool = torch.nn.MaxPool2d(2)
        self.fc = torch.nn.Linear(64 * 16 * 16, 10)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

model = SimpleModel()
dummy_input = torch.randn(1, 3, 32, 32)

# ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    export_params=True,       # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹
    opset_version=17,         # ONNX opset ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    do_constant_folding=True, # å®šæ•°ç•³ã¿è¾¼ã¿æœ€é©åŒ–
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={            # å‹•çš„ãªæ¬¡å…ƒ
        'input': {0: 'batch_size'},
        'output': {0: 'batch_size'}
    }
)
```

### ONNX ã®æ¤œè¨¼ã¨ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³

```python
import onnx

# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼
onnx_model = onnx.load("model.onnx")
onnx.checker.check_model(onnx_model)

# ã‚°ãƒ©ãƒ•æƒ…å ±ã®è¡¨ç¤º
print(onnx.helper.printable_graph(onnx_model.graph))

# Netron ã§ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ãƒ™ãƒ¼ã‚¹ï¼‰
# https://netron.app/ ã§model.onnxã‚’é–‹ã
```

## 12.3 Rust ã§ã® ONNX æ¨è«–

### tract: Rust ãƒã‚¤ãƒ†ã‚£ãƒ–æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³

**tract** [^2] ã¯ã€Rustè£½ã®ONNX/TensorFlowæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚

[^2]: tract: https://github.com/sonos/tract

**ç‰¹å¾´**:
- ã‚¼ãƒ­ä¾å­˜ï¼ˆå¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¸è¦ï¼‰
- WASMå¯¾å¿œ
- è»½é‡ï¼ˆãƒã‚¤ãƒŠãƒªã‚µã‚¤ã‚ºå°ï¼‰
- å‹å®‰å…¨

**Cargo.toml**:

```toml
[dependencies]
tract-onnx = "0.21"
ndarray = "0.15"
```

**åŸºæœ¬çš„ãªæ¨è«–**:

```rust
use tract_onnx::prelude::*;

fn main() -> TractResult<()> {
    // ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    let model = tract_onnx::onnx()
        .model_for_path("model.onnx")?
        .into_optimized()?
        .into_runnable()?;
    
    // å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    let input = ndarray::Array4::<f32>::zeros((1, 3, 32, 32));
    let input_tensor = input.into_tensor();
    
    // æ¨è«–å®Ÿè¡Œ
    let result = model.run(tvec!(input_tensor.into()))?;
    
    // çµæœã®å–å¾—
    let output = result[0].to_array_view::<f32>()?;
    println!("Output shape: {:?}", output.shape());
    println!("Output: {:?}", output);
    
    Ok(())
}
```

### Pythonï¼ˆONNX Runtimeï¼‰ã¨ã®æ¯”è¼ƒ

**Python å®Ÿè£…**:

```python
import onnxruntime as ort
import numpy as np

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
session = ort.InferenceSession(
    "model.onnx",
    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
)

# å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
input_data = np.random.randn(1, 3, 32, 32).astype(np.float32)

# æ¨è«–å®Ÿè¡Œ
outputs = session.run(
    None,  # ã™ã¹ã¦ã®å‡ºåŠ›ã‚’å–å¾—
    {'input': input_data}
)

print(f"Output shape: {outputs[0].shape}")
print(f"Output: {outputs[0]}")
```

**Rust å®Ÿè£…ï¼ˆtractï¼‰**:

```rust
use tract_onnx::prelude::*;
use ndarray::Array4;

fn inference() -> TractResult<()> {
    let model = tract_onnx::onnx()
        .model_for_path("model.onnx")?
        .into_optimized()?
        .into_runnable()?;
    
    let input = Array4::<f32>::from_shape_fn((1, 3, 32, 32), |_| {
        rand::random::<f32>()
    });
    
    let result = model.run(tvec!(input.into_tensor().into()))?;
    let output: ArrayViewD<f32> = result[0].to_array_view()?;
    
    println!("Output shape: {:?}", output.shape());
    println!("Output: {:?}", output.slice(s![0, ..5]));  // æœ€åˆã®5è¦ç´ 
    
    Ok(())
}
```

### onnxruntime-rs: ONNX Runtime ã® Rust ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°

**onnxruntime-rs** [^3] ã¯ã€Microsoft ã® ONNX Runtime ã¸ã® Rust ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã§ã™ã€‚

[^3]: onnxruntime-rs: https://github.com/pykeio/onnxruntime-rs

**åˆ©ç‚¹**:
- ONNX Runtime ã®å…¨æ©Ÿèƒ½ãŒä½¿ãˆã‚‹
- CUDA, TensorRT, DirectMLãªã©ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å¯¾å¿œ
- æœ€é«˜ãƒ¬ãƒ™ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

**æ¬ ç‚¹**:
- ONNX Runtime ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¸ã®ä¾å­˜
- ãƒã‚¤ãƒŠãƒªã‚µã‚¤ã‚ºãŒå¤§ãã„

```rust
use onnxruntime::{environment::Environment, GraphOptimizationLevel, LoggingLevel};
use ndarray::Array4;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // ç’°å¢ƒåˆæœŸåŒ–
    let environment = Environment::builder()
        .with_name("inference")
        .with_log_level(LoggingLevel::Warning)
        .build()?
        .into_arc();
    
    // ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
    let session = environment
        .new_session_builder()?
        .with_optimization_level(GraphOptimizationLevel::Level3)?
        .with_intra_threads(4)?
        .with_model_from_file("model.onnx")?;
    
    // å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æº–å‚™
    let input = Array4::<f32>::zeros((1, 3, 32, 32));
    
    // æ¨è«–å®Ÿè¡Œ
    let outputs = session.run(vec![input.into()])?;
    
    let output = outputs[0]
        .try_extract::<f32>()?
        .view()
        .to_owned();
    
    println!("Output shape: {:?}", output.shape());
    
    Ok(())
}
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| ãƒ©ã‚¤ãƒ–ãƒ©ãƒª | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | ãƒ¡ãƒ¢ãƒª | ãƒã‚¤ãƒŠãƒªã‚µã‚¤ã‚º |
|-----------|------------|----------|--------|--------------|
| **Python (ONNX Runtime)** | é«˜ | ä¸­ | å¤§ | - |
| **Rust (tract)** | ä¸­ | ä½ | å° | å° (5-10 MB) |
| **Rust (onnxruntime-rs)** | é«˜ | ä½ | ä¸­ | å¤§ (50-100 MB) |
| **Rust (candle)** | ä¸­ã€œé«˜ | ä½ | å° | ä¸­ (20-30 MB) |

## 12.4 æ¨è«–æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯

### Graph Fusionï¼ˆæ¼”ç®—å­èåˆï¼‰

**ç›®çš„**: è¤‡æ•°ã®æ¼”ç®—å­ã‚’1ã¤ã®ã‚«ãƒ¼ãƒãƒ«ã«çµ±åˆã—ã€ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’å‰Šæ¸›

**ä¾‹**: Conv + ReLU â†’ ConvReLU

```mermaid
graph LR
    subgraph Before[èåˆå‰]
        I1[Input] --> Conv[Conv2D]
        Conv --> Temp[Temporary<br/>Tensor]
        Temp --> ReLU[ReLU]
        ReLU --> O1[Output]
    end
    
    subgraph After[èåˆå¾Œ]
        I2[Input] --> ConvReLU[ConvReLU<br/>Fused]
        ConvReLU --> O2[Output]
    end
    
    style Before fill:#ffe1e1
    style After fill:#e1ffe1
```

**Pythonï¼ˆTorchScriptï¼‰**:

```python
import torch

model = torch.nn.Sequential(
    torch.nn.Conv2d(3, 64, 3),
    torch.nn.ReLU()
)

# JIT compile with fusion
scripted = torch.jit.script(model)
scripted = torch.jit.freeze(scripted)  # æœ€é©åŒ–

# èåˆã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã‚’ç¢ºèª
print(scripted.graph)
```

**Rustï¼ˆtractï¼‰ã§ã®èåˆ**:

```rust
use tract_onnx::prelude::*;

fn optimize_model() -> TractResult<()> {
    let model = tract_onnx::onnx()
        .model_for_path("model.onnx")?
        .with_input_fact(0, f32::fact([1, 3, 224, 224]).into())?
        .into_optimized()?;  // â† è‡ªå‹•çš„ã«èåˆãŒé©ç”¨ã•ã‚Œã‚‹
    
    // æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ãƒ³ãƒ—
    println!("{:#?}", model);
    
    Ok(())
}
```

### é‡å­åŒ–ï¼ˆQuantizationï¼‰

**é‡å­åŒ–** [^4] ã¯ã€FP32 â†’ INT8 ãªã©ã®ä½ç²¾åº¦åŒ–ã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—ã‚’å‰Šæ¸›ã—ã¾ã™ã€‚

[^4]: Jacob, B., et al. (2018). "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference." CVPR.

**é‡å­åŒ–ã®ç¨®é¡**:

| ç¨®é¡ | ã‚¿ã‚¤ãƒŸãƒ³ã‚° | ç²¾åº¦ | é€Ÿåº¦ |
|------|----------|------|------|
| **Post-Training Quantization (PTQ)** | å­¦ç¿’å¾Œ | ä¸­ | ç°¡å˜ |
| **Quantization-Aware Training (QAT)** | å­¦ç¿’ä¸­ | é«˜ | è¤‡é›‘ |

**Pythonï¼ˆPyTorchï¼‰ã§ã® PTQ**:

```python
import torch
import torch.quantization

model = MyModel()
model.eval()

# Dynamic Quantizationï¼ˆç°¡å˜ï¼‰
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {torch.nn.Linear},  # é‡å­åŒ–ã™ã‚‹å±¤
    dtype=torch.qint8
)

# Static Quantizationï¼ˆç²¾åº¦é«˜ã„ï¼‰
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)

# ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä»£è¡¨çš„ãªãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œï¼‰
with torch.no_grad():
    for data in calibration_loader:
        model(data)

torch.quantization.convert(model, inplace=True)

# ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºæ¯”è¼ƒ
print(f"Original: {model_size(original_model) / 1e6:.2f} MB")
print(f"Quantized: {model_size(quantized_model) / 1e6:.2f} MB")
```

**åŠ¹æœ**:
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: 75% å‰Šæ¸›ï¼ˆFP32 â†’ INT8ï¼‰
- æ¨è«–é€Ÿåº¦: 2ã€œ4å€é«˜é€ŸåŒ–ï¼ˆCPUã§é¡•è‘—ï¼‰
- ç²¾åº¦åŠ£åŒ–: é€šå¸¸ 1% ä»¥å†…

### KV ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆTransformer å°‚ç”¨æœ€é©åŒ–ï¼‰

**KV ã‚­ãƒ£ãƒƒã‚·ãƒ¥** [^5] ã¯ã€Transformerã®è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã§ã€éå»ã®Key/Valueã‚’å†åˆ©ç”¨ã—ã¾ã™ã€‚

[^5]: Pope, R., et al. (2022). "Efficiently Scaling Transformer Inference." MLSys.

**é€šå¸¸ã®å®Ÿè£…**:

```python
def self_attention(query, key, value):
    # Q, K, V: (batch, seq_len, dim)
    scores = torch.matmul(query, key.transpose(-2, -1))  # (batch, seq_len, seq_len)
    scores = scores / math.sqrt(key.size(-1))
    attn = torch.softmax(scores, dim=-1)
    output = torch.matmul(attn, value)  # (batch, seq_len, dim)
    return output

# è‡ªå·±å›å¸°ç”Ÿæˆ
for i in range(max_len):
    # æ¯å›å…¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å†è¨ˆç®— â† éåŠ¹ç‡
    output = model(input_ids[:, :i+1])
```

**KV ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç‰ˆ**:

```python
class TransformerWithCache:
    def __init__(self):
        self.cache = {'key': [], 'value': []}
    
    def forward(self, x, use_cache=False):
        query = self.query_proj(x)
        key = self.key_proj(x)
        value = self.value_proj(x)
        
        if use_cache:
            # éå»ã®K, Vã¨é€£çµ
            if self.cache['key']:
                key = torch.cat([self.cache['key'], key], dim=1)
                value = torch.cat([self.cache['value'], value], dim=1)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
            self.cache['key'] = key
            self.cache['value'] = value
        
        # Attentionè¨ˆç®—ï¼ˆéå»ã®K, Vã‚‚ä½¿ç”¨ï¼‰
        scores = torch.matmul(query, key.transpose(-2, -1))
        attn = torch.softmax(scores / math.sqrt(key.size(-1)), dim=-1)
        output = torch.matmul(attn, value)
        
        return output

# è‡ªå·±å›å¸°ç”Ÿæˆï¼ˆé«˜é€ŸåŒ–ï¼‰
model = TransformerWithCache()
for i in range(max_len):
    # æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿å‡¦ç† â† é«˜é€Ÿ
    output = model(new_token, use_cache=True)
```

**åŠ¹æœ**:
- **è¨ˆç®—é‡**: $O(n^2) \rightarrow O(n)$ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«å¯¾ã—ã¦ç·šå½¢ï¼‰
- **ãƒ¡ãƒ¢ãƒª**: å¢—åŠ ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã®ãŸã‚ï¼‰
- **é€Ÿåº¦**: ç”Ÿæˆæ™‚ 5ã€œ10å€é«˜é€ŸåŒ–

**Rust ã§ã®å®Ÿè£…ä¾‹**:

```rust
use ndarray::{Array2, Array3};
use std::collections::HashMap;

pub struct KVCache {
    key_cache: Vec<Array3<f32>>,    // (layer, batch, seq, dim)
    value_cache: Vec<Array3<f32>>,
}

impl KVCache {
    pub fn new(num_layers: usize) -> Self {
        Self {
            key_cache: Vec::with_capacity(num_layers),
            value_cache: Vec::with_capacity(num_layers),
        }
    }
    
    pub fn update(&mut self, layer: usize, key: Array3<f32>, value: Array3<f32>) {
        if layer >= self.key_cache.len() {
            self.key_cache.push(key);
            self.value_cache.push(value);
        } else {
            // æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨é€£çµ
            let cached_key = &self.key_cache[layer];
            let new_key = ndarray::concatenate(
                ndarray::Axis(1),
                &[cached_key.view(), key.view()]
            ).unwrap();
            self.key_cache[layer] = new_key;
            
            let cached_value = &self.value_cache[layer];
            let new_value = ndarray::concatenate(
                ndarray::Axis(1),
                &[cached_value.view(), value.view()]
            ).unwrap();
            self.value_cache[layer] = new_value;
        }
    }
    
    pub fn get(&self, layer: usize) -> Option<(&Array3<f32>, &Array3<f32>)> {
        if layer < self.key_cache.len() {
            Some((&self.key_cache[layer], &self.value_cache[layer]))
        } else {
            None
        }
    }
    
    pub fn clear(&mut self) {
        self.key_cache.clear();
        self.value_cache.clear();
    }
}
```

## 12.5 å‹•çš„ãƒãƒƒãƒãƒ³ã‚°ï¼ˆDynamic Batchingï¼‰

### å‹•çš„ãƒãƒƒãƒãƒ³ã‚°ã¨ã¯

è¤‡æ•°ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‹•çš„ã«ã¾ã¨ã‚ã¦å‡¦ç†ã—ã€GPUã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æœ€å¤§åŒ–ã™ã‚‹æŠ€è¡“ã§ã™ã€‚

```mermaid
sequenceDiagram
    participant C1 as Client 1
    participant C2 as Client 2
    participant C3 as Client 3
    participant Q as Request Queue
    participant GPU as GPU Inference
    
    C1->>Q: Request (t=0ms)
    C2->>Q: Request (t=10ms)
    C3->>Q: Request (t=15ms)
    
    Note over Q: Wait until<br/>batch size=3<br/>or timeout
    
    Q->>GPU: Batch [R1, R2, R3]
    GPU->>GPU: Process (batch_size=3)
    GPU->>Q: Results [O1, O2, O3]
    
    Q->>C1: Response O1
    Q->>C2: Response O2
    Q->>C3: Response O3
```

### Pythonï¼ˆNVIDIA Tritonï¼‰ã§ã®å®Ÿè£…

**Triton Inference Server** [^6] ã¯ã€å‹•çš„ãƒãƒƒãƒãƒ³ã‚°ã‚’è‡ªå‹•ã§è¡Œã„ã¾ã™ã€‚

[^6]: NVIDIA Triton Inference Server. https://github.com/triton-inference-server/server

```python
# config.pbtxtï¼ˆTritonã®è¨­å®šï¼‰
name: "my_model"
platform: "onnxruntime_onnx"
max_batch_size: 32

dynamic_batching {
  preferred_batch_size: [ 8, 16, 32 ]
  max_queue_delay_microseconds: 5000  # 5ms
}

input [
  {
    name: "input"
    data_type: TYPE_FP32
    dims: [ 3, 224, 224 ]
  }
]

output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [ 1000 ]
  }
]
```

### Rust ã§ã®ç°¡æ˜“å®Ÿè£…

```rust
use tokio::sync::mpsc;
use tokio::time::{timeout, Duration};
use std::sync::Arc;

pub struct DynamicBatcher<I, O> {
    max_batch_size: usize,
    max_wait_time: Duration,
    model: Arc<dyn Fn(Vec<I>) -> Vec<O> + Send + Sync>,
}

impl<I, O> DynamicBatcher<I, O>
where
    I: Send + 'static,
    O: Send + 'static,
{
    pub fn new(
        max_batch_size: usize,
        max_wait_time: Duration,
        model: Arc<dyn Fn(Vec<I>) -> Vec<O> + Send + Sync>,
    ) -> Self {
        Self {
            max_batch_size,
            max_wait_time,
            model,
        }
    }
    
    pub async fn run(
        &self,
        mut rx: mpsc::Receiver<(I, tokio::sync::oneshot::Sender<O>)>,
    ) {
        let mut batch = Vec::new();
        let mut senders = Vec::new();
        
        loop {
            // ãƒãƒƒãƒåé›†
            match timeout(self.max_wait_time, rx.recv()).await {
                Ok(Some((input, sender))) => {
                    batch.push(input);
                    senders.push(sender);
                    
                    // ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰å‡¦ç†
                    if batch.len() >= self.max_batch_size {
                        self.process_batch(&mut batch, &mut senders);
                    }
                }
                Ok(None) => break,  // ãƒãƒ£ãƒãƒ«é–‰ã˜ãŸ
                Err(_) => {
                    // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ â†’ æºœã¾ã£ã¦ã„ã‚‹ãƒãƒƒãƒã‚’å‡¦ç†
                    if !batch.is_empty() {
                        self.process_batch(&mut batch, &mut senders);
                    }
                }
            }
        }
    }
    
    fn process_batch(
        &self,
        batch: &mut Vec<I>,
        senders: &mut Vec<tokio::sync::oneshot::Sender<O>>,
    ) {
        if batch.is_empty() {
            return;
        }
        
        // ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ
        let outputs = (self.model)(batch.drain(..).collect());
        
        // çµæœã‚’å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«è¿”ã™
        for (output, sender) in outputs.into_iter().zip(senders.drain(..)) {
            let _ = sender.send(output);
        }
    }
}

// ä½¿ç”¨ä¾‹
#[tokio::main]
async fn main() {
    let (tx, rx) = mpsc::channel(100);
    
    let model = Arc::new(|inputs: Vec<Array2<f32>>| {
        // ãƒãƒƒãƒæ¨è«–
        inputs.into_iter()
            .map(|input| {
                // å®Ÿéš›ã®æ¨è«–å‡¦ç†
                input.map(|x| x * 2.0)
            })
            .collect()
    });
    
    let batcher = DynamicBatcher::new(
        32,                           // max_batch_size
        Duration::from_millis(10),    // max_wait_time
        model,
    );
    
    // ãƒãƒƒãƒãƒ£ãƒ¼èµ·å‹•
    tokio::spawn(async move {
        batcher.run(rx).await;
    });
    
    // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    let (result_tx, result_rx) = tokio::sync::oneshot::channel();
    tx.send((ndarray::Array2::zeros((224, 224)), result_tx)).await.unwrap();
    
    let result = result_rx.await.unwrap();
    println!("Result: {:?}", result);
}
```

## 12.6 æ¨è«–ã‚µãƒ¼ãƒã®æ§‹ç¯‰

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
graph TD
    Client1[Client 1] --> LB[Load Balancer<br/>nginx/envoy]
    Client2[Client 2] --> LB
    Client3[Client 3] --> LB
    
    LB --> Server1[Inference Server 1<br/>actix-web/axum]
    LB --> Server2[Inference Server 2]
    LB --> Server3[Inference Server 3]
    
    Server1 --> Queue1[Request Queue]
    Server2 --> Queue2[Request Queue]
    Server3 --> Queue3[Request Queue]
    
    Queue1 --> GPU1[GPU Worker 1<br/>tract/onnxruntime]
    Queue2 --> GPU2[GPU Worker 2]
    Queue3 --> GPU3[GPU Worker 3]
    
    GPU1 --> Cache[Model Cache<br/>å…±æœ‰ãƒ¡ãƒ¢ãƒª]
    GPU2 --> Cache
    GPU3 --> Cache
    
    style LB fill:#e1f5ff
    style Server1 fill:#ffe1f5
    style Queue1 fill:#fff4e1
    style GPU1 fill:#e1ffe1
```

### Rustï¼ˆaxumï¼‰ã§ã®å®Ÿè£…

```rust
use axum::{
    extract::State,
    http::StatusCode,
    response::IntoResponse,
    routing::post,
    Json, Router,
};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tokio::sync::Semaphore;
use tract_onnx::prelude::*;

#[derive(Deserialize)]
struct InferenceRequest {
    input: Vec<Vec<Vec<Vec<f32>>>>,  // (batch, channels, height, width)
}

#[derive(Serialize)]
struct InferenceResponse {
    output: Vec<Vec<f32>>,  // (batch, classes)
}

struct AppState {
    model: Arc<RunnableModel<TypedFact, Box<dyn TypedOp>, Graph<TypedFact, Box<dyn TypedOp>>>>,
    semaphore: Arc<Semaphore>,  // ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°åˆ¶é™
}

async fn inference(
    State(state): State<Arc<AppState>>,
    Json(request): Json<InferenceRequest>,
) -> Result<Json<InferenceResponse>, StatusCode> {
    // ã‚»ãƒãƒ•ã‚©ã§ä¸¦è¡Œæ•°åˆ¶å¾¡
    let _permit = state.semaphore.acquire().await
        .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;
    
    // å…¥åŠ›ã‚’ndarrayã«å¤‰æ›
    let input_shape = (
        request.input.len(),
        request.input[0].len(),
        request.input[0][0].len(),
        request.input[0][0][0].len(),
    );
    
    let input_vec: Vec<f32> = request.input
        .into_iter()
        .flat_map(|batch| batch.into_iter()
            .flat_map(|channel| channel.into_iter()
                .flat_map(|row| row.into_iter())))
        .collect();
    
    let input = ndarray::Array4::from_shape_vec(input_shape, input_vec)
        .map_err(|_| StatusCode::BAD_REQUEST)?;
    
    // æ¨è«–å®Ÿè¡Œ
    let result = state.model
        .run(tvec!(input.into_tensor().into()))
        .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;
    
    let output: ArrayViewD<f32> = result[0]
        .to_array_view()
        .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;
    
    // çµæœã‚’Vecã«å¤‰æ›
    let output_vec: Vec<Vec<f32>> = output
        .outer_iter()
        .map(|row| row.to_vec())
        .collect();
    
    Ok(Json(InferenceResponse { output: output_vec }))
}

#[tokio::main]
async fn main() {
    // ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    let model = tract_onnx::onnx()
        .model_for_path("model.onnx")
        .unwrap()
        .into_optimized()
        .unwrap()
        .into_runnable()
        .unwrap();
    
    let state = Arc::new(AppState {
        model: Arc::new(model),
        semaphore: Arc::new(Semaphore::new(4)),  // æœ€å¤§4ä¸¦è¡Œ
    });
    
    let app = Router::new()
        .route("/predict", post(inference))
        .with_state(state);
    
    println!("Starting inference server on 0.0.0.0:3000");
    
    axum::Server::bind(&"0.0.0.0:3000".parse().unwrap())
        .serve(app.into_make_service())
        .await
        .unwrap();
}
```

---

## ã¾ã¨ã‚

| å´é¢ | Python | Rust |
|------|--------|------|
| **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ** | æˆç†Ÿï¼ˆONNX Runtime, TensorRTï¼‰ | ç™ºå±•ä¸­ï¼ˆtract, onnxruntime-rs, candleï¼‰ |
| **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹** | é«˜ï¼ˆãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰ | é«˜ï¼ˆã‚¼ãƒ­ã‚³ã‚¹ãƒˆRCæŠ½è±¡åŒ–ï¼‰ |
| **ãƒ‡ãƒ—ãƒ­ã‚¤** | è¤‡é›‘ï¼ˆä¾å­˜é–¢ä¿‚å¤šã„ï¼‰ | ç°¡å˜ï¼ˆå˜ä¸€ãƒã‚¤ãƒŠãƒªï¼‰ |
| **ãƒ¡ãƒ¢ãƒª** | GCä¾å­˜ | æ˜ç¤ºçš„åˆ¶å¾¡ |
| **å‹å®‰å…¨æ€§** | ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ |

**Rust ã®å„ªä½æ€§**:
- è»½é‡ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆä¾å­˜ãªã—ã€å°ã•ã„ãƒã‚¤ãƒŠãƒªï¼‰
- ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆGCãªã—ï¼‰
- å‹å®‰å…¨ï¼ˆONNXäº’æ›æ€§ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ãƒã‚§ãƒƒã‚¯ï¼‰

**Python ã®å„ªä½æ€§**:
- è±Šå¯Œãªãƒ„ãƒ¼ãƒ«ï¼ˆTriton, TensorRT, OpenVINOï¼‰
- ç°¡å˜ãªãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°

---

## å‚è€ƒæ–‡çŒ®

1. ONNX: Open Neural Network Exchange. https://onnx.ai/
2. tract: Rust Neural Network Inference. https://github.com/sonos/tract
3. onnxruntime-rs. https://github.com/pykeio/onnxruntime-rs
4. Jacob, B., et al. (2018). "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference." CVPR.
5. Pope, R., et al. (2022). "Efficiently Scaling Transformer Inference." MLSys.
6. NVIDIA Triton Inference Server. https://github.com/triton-inference-server/server
7. HuggingFace candle. https://github.com/huggingface/candle
8. Kim, S., et al. (2023). "Full Stack Optimization of Transformer Inference." arXiv:2302.14017
9. ONNX Runtime. https://onnxruntime.ai/docs/
10. Gholami, A., et al. (2021). "A Survey of Quantization Methods for Efficient Neural Network Inference." arXiv:2103.13630
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬14ç« : ãƒ‡ãƒãƒƒã‚°ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°](../04_ç¬¬IVéƒ¨_æ©Ÿæ¢°å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ³ã®æ§‹ç¯‰/04-14-ãƒ‡ãƒãƒƒã‚°ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°.md) | [â¡ï¸ ç¬¬16ç« : ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©æœ€é©åŒ–ã¨DSLè¨­è¨ˆ](05-16-ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©æœ€é©åŒ–ã¨DSLè¨­è¨ˆ.md)