[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬15ç« ](05-15-ãƒ¢ãƒ‡ãƒ«æ¨è«–ã¨ONNXäº’æ›.md) | [â¡ï¸ ç¬¬17ç« ](05-17-åˆ†æ•£ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿å¯¾å¿œ.md)

---

# ç¬¬ 13 ç« ã€€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©æœ€é©åŒ–ã¨ DSL è¨­è¨ˆ

ã“ã®ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©æœ€é©åŒ–æŠ€è¡“ã¨ã€Domain-Specific Languageï¼ˆDSLï¼‰ã®è¨­è¨ˆã«ã¤ã„ã¦å­¦ã³ã¾ã™ã€‚JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã€Rustãƒã‚¯ãƒ­ã€LLVM/MLIRã€ãã—ã¦Tritoné¢¨ã®é«˜æ°´æº–ã‚«ãƒ¼ãƒãƒ«è¨˜è¿°è¨€èªã®å®Ÿè£…ã‚’æ‰±ã„ã¾ã™ã€‚

**ç›®çš„**: è¨ˆç®—ã‚°ãƒ©ãƒ•ã®æœ€é©åŒ–ã€JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«ã‚ˆã‚‹å®Ÿè¡Œæ™‚æœ€é©åŒ–ã€Rustãƒã‚¯ãƒ­ã‚’ä½¿ã£ãŸå‹å®‰å…¨ãªDSLã®è¨­è¨ˆã‚’ç¿’å¾—ã—ã¾ã™ã€‚

## 13.1 æ©Ÿæ¢°å­¦ç¿’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã®æ¦‚è¦

### ãªãœã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãŒå¿…è¦ã‹ï¼Ÿ

**èª²é¡Œ**:
1. **æŠ½è±¡åŒ–ã®ã‚³ã‚¹ãƒˆ**: é«˜ãƒ¬ãƒ™ãƒ«APIã¯æŸ”è»Ÿã ãŒã€æœ€é©åŒ–ã®ä½™åœ°ãŒè¦‹ãˆã«ãã„
2. **ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®å¤šæ§˜æ€§**: CPU, GPU, TPU, NPUãªã©
3. **æ¼”ç®—å­ã®çˆ†ç™º**: Conv, MatMul, Attention... å„ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å‘ã‘ã«æ‰‹æ›¸ãã¯ä¸å¯èƒ½

**è§£æ±ºç­–**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã«ã‚ˆã‚‹è‡ªå‹•æœ€é©åŒ–

```mermaid
graph TD
    Frontend[Frontend<br/>PyTorch/TensorFlow] --> IR[ä¸­é–“è¡¨ç¾ IR<br/>Computational Graph]
    IR --> Opt[æœ€é©åŒ–ãƒ‘ã‚¹<br/>Fusion/Mem Opt/etc]
    Opt --> CodeGen[ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ<br/>CUDA/Metal/etc]
    CodeGen --> Runtime[Runtime<br/>Execution]
    
    style Frontend fill:#e1f5ff
    style IR fill:#ffe1f5
    style Opt fill:#fff4e1
    style CodeGen fill:#e1ffe1
```

### ä¸»è¦ãª ML ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©

| ã‚³ãƒ³ãƒ‘ã‚¤ãƒ© | é–‹ç™ºå…ƒ | è¨€èª | ç‰¹å¾´ |
|-----------|-------|------|------|
| **XLA** | Google | C++ | TensorFlow/JAX ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ |
| **TVM** | Apache | C++/Python | ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢éä¾å­˜ |
| **MLIR** | LLVM Project | C++ | å¤šæ®µéšIRã€æ‹¡å¼µå¯èƒ½ |
| **Glow** | Facebook | C++ | PyTorchå‘ã‘ |
| **Triton** | OpenAI | Python | GPU ã‚«ãƒ¼ãƒãƒ«è¨˜è¿°ç”¨DSL |

## 13.2 JIT ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨å®Ÿè¡Œæ™‚æœ€é©åŒ–

### TorchScript: PyTorch ã® JIT ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©

**TorchScript** [^1] ã¯ã€PyTorchãƒ¢ãƒ‡ãƒ«ã‚’JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚

[^1]: TorchScript. https://pytorch.org/docs/stable/jit.html

**2ã¤ã®ãƒ¢ãƒ¼ãƒ‰**:

1. **Tracing**: å®Ÿè¡Œã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¦ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
2. **Scripting**: Python ASTã‹ã‚‰ç›´æ¥ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰

```python
import torch

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(10, 10)
    
    def forward(self, x):
        return self.linear(x).relu()

# Tracing ãƒ¢ãƒ¼ãƒ‰
model = MyModule()
example = torch.randn(1, 10)
traced = torch.jit.trace(model, example)

# Scripting ãƒ¢ãƒ¼ãƒ‰
scripted = torch.jit.script(model)

# ã‚°ãƒ©ãƒ•ã‚’ç¢ºèª
print(traced.graph)
```

**å‡ºåŠ›ï¼ˆIRã‚°ãƒ©ãƒ•ï¼‰**:

```
graph(%self.1 : __torch__.MyModule,
      %x.1 : Float(1, 10)):
  %linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="linear"](%self.1)
  %4 : Tensor = prim::CallMethod[name="forward"](%linear, %x.1)
  %5 : Tensor = aten::relu(%4)
  return (%5)
```

**æœ€é©åŒ–**:

```python
# ã‚°ãƒ©ãƒ•æœ€é©åŒ–
optimized = torch.jit.optimize_for_inference(scripted)

# æ¼”ç®—å­èåˆã‚’ç¢ºèª
print(optimized.graph)
```

### JAX: JIT ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã®ç‹æ§˜

**JAX** [^2] ã¯ã€NumPyé¢¨ã®APIã§JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚’æä¾›ã—ã¾ã™ã€‚

[^2]: JAX. https://github.com/google/jax

```python
import jax
import jax.numpy as jnp

# é€šå¸¸ã®é–¢æ•°
def slow_fn(x):
    return jnp.dot(x, x.T)

# JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
fast_fn = jax.jit(slow_fn)

x = jnp.ones((1000, 1000))

# åˆå›: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å®Ÿè¡Œ
result = fast_fn(x)  # é…ã„ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“å«ã‚€ï¼‰

# 2å›ç›®ä»¥é™: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿
result = fast_fn(x)  # é€Ÿã„
```

**XLAæœ€é©åŒ–ã®ç¢ºèª**:

```python
# HLO (High-Level Optimizer) IRã‚’è¡¨ç¤º
print(fast_fn.lower(x).compiler_ir(dialect="hlo"))
```

### Rust ã§ã® JIT ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«: cranelift

**cranelift** [^3] ã¯ã€Rustè£½ã®JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã§ã™ã€‚

[^3]: cranelift. https://github.com/bytecodealliance/wasmtime/tree/main/cranelift

```rust
use cranelift::prelude::*;
use cranelift_jit::{JITBuilder, JITModule};
use cranelift_module::{DataContext, Linkage, Module};

fn jit_add() -> Result<fn(i32, i32) -> i32, String> {
    let mut builder = JITBuilder::new(cranelift_module::default_libcall_names())?;
    let mut module = JITModule::new(builder);
    
    // é–¢æ•°ã‚·ã‚°ãƒãƒãƒ£: (i32, i32) -> i32
    let mut sig = module.make_signature();
    sig.params.push(AbiParam::new(types::I32));
    sig.params.push(AbiParam::new(types::I32));
    sig.returns.push(AbiParam::new(types::I32));
    
    // é–¢æ•°å®šç¾©
    let func_id = module
        .declare_function("add", Linkage::Export, &sig)
        .map_err(|e| e.to_string())?;
    
    // ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
    let mut ctx = module.make_context();
    ctx.func.signature = sig;
    
    // é–¢æ•°ãƒ“ãƒ«ãƒ€ãƒ¼
    let mut builder_context = FunctionBuilderContext::new();
    let mut builder = FunctionBuilder::new(&mut ctx.func, &mut builder_context);
    
    // ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒ–ãƒ­ãƒƒã‚¯
    let block = builder.create_block();
    builder.append_block_params_for_function_params(block);
    builder.switch_to_block(block);
    
    // å¼•æ•°å–å¾—
    let arg0 = builder.block_params(block)[0];
    let arg1 = builder.block_params(block)[1];
    
    // åŠ ç®—å‘½ä»¤
    let result = builder.ins().iadd(arg0, arg1);
    
    // æˆ»ã‚Šå€¤
    builder.ins().return_(&[result]);
    builder.seal_all_blocks();
    builder.finalize();
    
    // ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    module
        .define_function(func_id, &mut ctx)
        .map_err(|e| e.to_string())?;
    
    module.clear_context(&mut ctx);
    module.finalize_definitions();
    
    // é–¢æ•°ãƒã‚¤ãƒ³ã‚¿å–å¾—
    let code = module.get_finalized_function(func_id);
    
    Ok(unsafe { std::mem::transmute::<*const u8, fn(i32, i32) -> i32>(code) })
}

fn main() {
    let add = jit_add().unwrap();
    let result = add(2, 3);
    println!("2 + 3 = {}", result);  // 5
}
```

## 13.3 Rust ãƒã‚¯ãƒ­ã«ã‚ˆã‚‹ DSL è¨­è¨ˆ

### å®£è¨€çš„ãƒã‚¯ãƒ­ï¼ˆDeclarative Macrosï¼‰

**ç›®çš„**: ãƒœã‚¤ãƒ©ãƒ¼ãƒ—ãƒ¬ãƒ¼ãƒˆã‚³ãƒ¼ãƒ‰ã®å‰Šæ¸›

```rust
// ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã®DSL
macro_rules! tensor_op {
    ($name:ident, $op:tt) => {
        fn $name(a: &Tensor, b: &Tensor) -> Tensor {
            assert_eq!(a.shape, b.shape);
            let mut result = Tensor::zeros(a.shape);
            for i in 0..a.data.len() {
                result.data[i] = a.data[i] $op b.data[i];
            }
            result
        }
    };
}

// ä½¿ç”¨
tensor_op!(add, +);
tensor_op!(mul, *);
tensor_op!(sub, -);

let a = Tensor::from_vec(vec![1.0, 2.0, 3.0]);
let b = Tensor::from_vec(vec![4.0, 5.0, 6.0]);
let c = add(&a, &b);  // [5.0, 7.0, 9.0]
```

### æ‰‹ç¶šãçš„ãƒã‚¯ãƒ­ï¼ˆProcedural Macrosï¼‰

**derive ãƒã‚¯ãƒ­** ã§å‹å®‰å…¨ãªãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã‚’å®Ÿç¾ï¼š

```rust
// derive ãƒã‚¯ãƒ­ã®å®šç¾©ï¼ˆproc-macro ã‚¯ãƒ¬ãƒ¼ãƒˆå†…ï¼‰
use proc_macro::TokenStream;
use quote::quote;
use syn::{parse_macro_input, DeriveInput};

#[proc_macro_derive(TensorOps)]
pub fn tensor_ops_derive(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    let name = input.ident;
    
    let expanded = quote! {
        impl #name {
            pub fn add(&self, other: &Self) -> Self {
                assert_eq!(self.shape, other.shape);
                let data: Vec<f32> = self.data.iter()
                    .zip(&other.data)
                    .map(|(a, b)| a + b)
                    .collect();
                Self { shape: self.shape, data }
            }
            
            pub fn mul(&self, other: &Self) -> Self {
                assert_eq!(self.shape, other.shape);
                let data: Vec<f32> = self.data.iter()
                    .zip(&other.data)
                    .map(|(a, b)| a * b)
                    .collect();
                Self { shape: self.shape, data }
            }
        }
    };
    
    TokenStream::from(expanded)
}
```

**ä½¿ç”¨å´**:

```rust
#[derive(TensorOps)]
struct Tensor {
    shape: Vec<usize>,
    data: Vec<f32>,
}

let a = Tensor { shape: vec![2, 2], data: vec![1.0, 2.0, 3.0, 4.0] };
let b = Tensor { shape: vec![2, 2], data: vec![5.0, 6.0, 7.0, 8.0] };
let c = a.add(&b);  // ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«å‹ãƒã‚§ãƒƒã‚¯
```

### é–¢æ•°é¢¨ãƒã‚¯ãƒ­ã§è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰

```rust
macro_rules! compute_graph {
    (
        inputs: { $($input:ident: $input_shape:expr),* }
        ops: { $($op_name:ident = $op:expr),* }
        output: $output:ident
    ) => {
        {
            // å…¥åŠ›å®šç¾©
            $(
                let $input = Tensor::placeholder(stringify!($input), $input_shape);
            )*
            
            // æ¼”ç®—å®šç¾©
            $(
                let $op_name = $op;
            )*
            
            // ã‚°ãƒ©ãƒ•æ§‹ç¯‰
            ComputeGraph {
                inputs: vec![$($input),*],
                operations: vec![$($op_name),*],
                output: $output,
            }
        }
    };
}

// ä½¿ç”¨ä¾‹
let graph = compute_graph! {
    inputs: {
        x: vec![None, 784],
        w: vec![784, 10]
    }
    ops: {
        matmul = Op::MatMul(x, w),
        relu = Op::ReLU(matmul)
    }
    output: relu
};
```

## 13.4 LLVM / MLIR ã«ã‚ˆã‚‹æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### MLIR: å¤šæ®µéšä¸­é–“è¡¨ç¾

**MLIR (Multi-Level Intermediate Representation)** [^4] ã¯ã€è¤‡æ•°ã®æŠ½è±¡åº¦ã®IRã‚’æ‰±ã„ã¾ã™ã€‚

[^4]: Lattner, C., et al. (2021). "MLIR: Scaling Compiler Infrastructure for Domain Specific Computation." CGO.

**æŠ½è±¡åº¦ã®ãƒ¬ãƒ™ãƒ«**:

```mermaid
graph TD
    High[High-Level IR<br/>TensorFlow/PyTorch] --> Linalg[Linalg Dialect<br/>ç·šå½¢ä»£æ•°æ¼”ç®—]
    Linalg --> Affine[Affine Dialect<br/>ãƒ«ãƒ¼ãƒ—æœ€é©åŒ–]
    Affine --> SCF[SCF Dialect<br/>æ§‹é€ åŒ–åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼]
    SCF --> LLVM[LLVM IR<br/>ä½ãƒ¬ãƒ™ãƒ«]
    LLVM --> Machine[Machine Code]
    
    style High fill:#e1f5ff
    style Linalg fill:#ffe1f5
    style Affine fill:#fff4e1
    style LLVM fill:#e1ffe1
```

**MLIR ã®ä¾‹**ï¼ˆè¡Œåˆ—ä¹—ç®—ï¼‰:

```mlir
// High-Level: Linalg Dialect
func.func @matmul(%A: tensor<128x256xf32>, %B: tensor<256x512xf32>) -> tensor<128x512xf32> {
  %C = linalg.matmul ins(%A, %B : tensor<128x256xf32>, tensor<256x512xf32>)
                     outs(%C_init : tensor<128x512xf32>) -> tensor<128x512xf32>
  return %C : tensor<128x512xf32>
}

// Mid-Level: Affine Dialectï¼ˆãƒ«ãƒ¼ãƒ—å±•é–‹å¾Œï¼‰
func.func @matmul_affine(%A: memref<128x256xf32>, %B: memref<256x512xf32>, %C: memref<128x512xf32>) {
  affine.for %i = 0 to 128 {
    affine.for %j = 0 to 512 {
      affine.for %k = 0 to 256 {
        %a = affine.load %A[%i, %k] : memref<128x256xf32>
        %b = affine.load %B[%k, %j] : memref<256x512xf32>
        %c = affine.load %C[%i, %j] : memref<128x512xf32>
        %prod = arith.mulf %a, %b : f32
        %sum = arith.addf %c, %prod : f32
        affine.store %sum, %C[%i, %j] : memref<128x512xf32>
      }
    }
  }
  return
}
```

### Rust ã‹ã‚‰ MLIR ã‚’ä½¿ã†

**inkwell**: LLVM ã® Rust ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°

```rust
use inkwell::context::Context;
use inkwell::OptimizationLevel;

fn compile_add() {
    let context = Context::create();
    let module = context.create_module("add_module");
    let builder = context.create_builder();
    
    // é–¢æ•°å‹: i32 add(i32, i32)
    let i32_type = context.i32_type();
    let fn_type = i32_type.fn_type(&[i32_type.into(), i32_type.into()], false);
    let function = module.add_function("add", fn_type, None);
    
    let basic_block = context.append_basic_block(function, "entry");
    builder.position_at_end(basic_block);
    
    // å¼•æ•°å–å¾—
    let x = function.get_nth_param(0).unwrap().into_int_value();
    let y = function.get_nth_param(1).unwrap().into_int_value();
    
    // åŠ ç®—
    let result = builder.build_int_add(x, y, "addtmp");
    builder.build_return(Some(&result));
    
    // LLVM IRã‚’è¡¨ç¤º
    module.print_to_stderr();
    
    // æœ€é©åŒ–
    use inkwell::passes::PassManager;
    let pass_manager = PassManager::create(());
    pass_manager.add_instruction_combining_pass();
    pass_manager.add_reassociate_pass();
    pass_manager.add_gvn_pass();
    pass_manager.add_cfg_simplification_pass();
    pass_manager.run_on(&module);
    
    println!("\nOptimized:");
    module.print_to_stderr();
}
```

## 13.5 Triton é¢¨ã®ã‚«ãƒ¼ãƒãƒ«è¨˜è¿°è¨€èª

### OpenAI Triton ã®æ¦‚è¦

**Triton** [^5] ã¯ã€GPU ã‚«ãƒ¼ãƒãƒ«ã‚’ Python é¢¨ã®æ§‹æ–‡ã§è¨˜è¿°ã§ãã‚‹DSLã§ã™ã€‚

[^5]: Tillet, P., et al. (2019). "Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations." MAPL.

**ç‰¹å¾´**:
- ãƒ–ãƒ­ãƒƒã‚¯ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
- è‡ªå‹•çš„ãªãƒ¡ãƒ¢ãƒªåˆä½“
- è‡ªå‹•çš„ãªå…±æœ‰ãƒ¡ãƒ¢ãƒªç®¡ç†

**Triton ã®ä¾‹**ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŠ ç®—ï¼‰:

```python
import triton
import triton.language as tl

@triton.jit
def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    # ãƒ—ãƒ­ã‚°ãƒ©ãƒ IDï¼ˆãƒ–ãƒ­ãƒƒã‚¯IDï¼‰
    pid = tl.program_id(axis=0)
    
    # ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹ä½ç½®
    block_start = pid * BLOCK_SIZE
    
    # ã‚ªãƒ•ã‚»ãƒƒãƒˆè¨ˆç®—
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    
    # å¢ƒç•Œãƒã‚§ãƒƒã‚¯ç”¨ãƒã‚¹ã‚¯
    mask = offsets < n_elements
    
    # ãƒ­ãƒ¼ãƒ‰
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    
    # è¨ˆç®—
    output = x + y
    
    # ã‚¹ãƒˆã‚¢
    tl.store(output_ptr + offsets, output, mask=mask)

# å‘¼ã³å‡ºã—
def add(x: torch.Tensor, y: torch.Tensor):
    output = torch.empty_like(x)
    n_elements = output.numel()
    
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
    
    return output
```

### Rust ã§ã® Triton é¢¨ DSL å®Ÿè£…

**è¨­è¨ˆæ–¹é‡**:
1. ãƒã‚¯ãƒ­ã§æ§‹æ–‡ã‚’å®šç¾©
2. ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«CUDA/PTXã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
3. å‹å®‰å…¨æ€§ã‚’ä¿è¨¼

```rust
// DSL ãƒã‚¯ãƒ­å®šç¾©
macro_rules! gpu_kernel {
    (
        fn $name:ident($($arg:ident: $arg_ty:ty),*) {
            $($body:tt)*
        }
    ) => {
        pub struct $name;
        
        impl $name {
            pub fn launch(
                grid: (u32, u32, u32),
                block: (u32, u32, u32),
                $($arg: $arg_ty),*
            ) -> Result<(), CudaError> {
                // CUDA ã‚«ãƒ¼ãƒãƒ«æ–‡å­—åˆ—ç”Ÿæˆ
                let kernel_source = format!(
                    r#"
                    extern "C" __global__ void {}({}) {{
                        {}
                    }}
                    "#,
                    stringify!($name),
                    gpu_kernel!(@args $($arg: $arg_ty),*),
                    gpu_kernel!(@body $($body)*)
                );
                
                // ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦å®Ÿè¡Œ
                compile_and_launch(&kernel_source, grid, block, &[$($arg),*])
            }
        }
    };
    
    (@args $($arg:ident: $arg_ty:ty),*) => {
        stringify!($($arg_ty *$arg),*)
    };
    
    (@body $($token:tt)*) => {
        stringify!($($token)*)
    };
}

// ä½¿ç”¨ä¾‹
gpu_kernel! {
    fn vector_add(a: *const f32, b: *const f32, c: *mut f32, n: usize) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx < n) {
            c[idx] = a[idx] + b[idx];
        }
    }
}

// å®Ÿè¡Œ
let a = vec![1.0f32; 1000];
let b = vec![2.0f32; 1000];
let mut c = vec![0.0f32; 1000];

vector_add::launch(
    (10, 1, 1),    // grid
    (100, 1, 1),   // block
    a.as_ptr(),
    b.as_ptr(),
    c.as_mut_ptr(),
    1000
)?;
```

### ã‚ˆã‚Šé«˜åº¦ãª DSL: ãƒ–ãƒ­ãƒƒã‚¯ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°

```rust
use ndarray::Array2;

// DSL for blocked tensor operations
#[macro_export]
macro_rules! blocked_kernel {
    (
        fn $name:ident<$block_size:expr>($($arg:ident: $arg_ty:ty),*) -> $ret:ty {
            $($body:tt)*
        }
    ) => {
        fn $name($($arg: $arg_ty),*) -> $ret {
            const BLOCK_SIZE: usize = $block_size;
            
            // ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§å‡¦ç†
            $($body)*
        }
    };
}

// ä½¿ç”¨ä¾‹: ãƒ–ãƒ­ãƒƒã‚¯åŒ–è¡Œåˆ—ä¹—ç®—
blocked_kernel! {
    fn blocked_matmul<32>(a: &Array2<f32>, b: &Array2<f32>) -> Array2<f32> {
        let (m, k) = a.dim();
        let (_, n) = b.dim();
        let mut c = Array2::zeros((m, n));
        
        // ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§ãƒ«ãƒ¼ãƒ—
        for ii in (0..m).step_by(BLOCK_SIZE) {
            for jj in (0..n).step_by(BLOCK_SIZE) {
                for kk in (0..k).step_by(BLOCK_SIZE) {
                    // ãƒ–ãƒ­ãƒƒã‚¯å†…ã®è¨ˆç®—
                    let i_end = (ii + BLOCK_SIZE).min(m);
                    let j_end = (jj + BLOCK_SIZE).min(n);
                    let k_end = (kk + BLOCK_SIZE).min(k);
                    
                    for i in ii..i_end {
                        for j in jj..j_end {
                            let mut sum = c[[i, j]];
                            for k in kk..k_end {
                                sum += a[[i, k]] * b[[k, j]];
                            }
                            c[[i, j]] = sum;
                        }
                    }
                }
            }
        }
        
        c
    }
}
```

## 13.6 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ¡ã‚¿ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¨å‹ãƒ¬ãƒ™ãƒ«è¨ˆç®—

### const generics ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚è¨ˆç®—

```rust
// å½¢çŠ¶ã‚’å‹ãƒ¬ãƒ™ãƒ«ã§è¡¨ç¾
struct Tensor<T, const N: usize, const M: usize> {
    data: Vec<T>,
}

impl<T: Copy, const N: usize, const M: usize> Tensor<T, N, M> {
    fn new(data: Vec<T>) -> Self {
        assert_eq!(data.len(), N * M);
        Self { data }
    }
    
    // å‹å®‰å…¨ãªè¡Œåˆ—ä¹—ç®—
    fn matmul<const P: usize>(
        &self,
        other: &Tensor<T, M, P>
    ) -> Tensor<T, N, P>
    where
        T: std::ops::Mul<Output = T> + std::ops::Add<Output = T> + Default,
    {
        let mut result = vec![T::default(); N * P];
        
        for i in 0..N {
            for j in 0..P {
                let mut sum = T::default();
                for k in 0..M {
                    sum = sum + self.data[i * M + k] * other.data[k * P + j];
                }
                result[i * P + j] = sum;
            }
        }
        
        Tensor::new(result)
    }
}

// ä½¿ç”¨ä¾‹
let a = Tensor::<f32, 2, 3>::new(vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]);
let b = Tensor::<f32, 3, 2>::new(vec![7.0, 8.0, 9.0, 10.0, 11.0, 12.0]);
let c = a.matmul(&b);  // Tensor<f32, 2, 2>

// ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼ï¼ˆå½¢çŠ¶ãŒåˆã‚ãªã„ï¼‰
// let d = Tensor::<f32, 2, 2>::new(vec![1.0, 2.0, 3.0, 4.0]);
// let e = a.matmul(&d);  // ERROR: expected Tensor<_, 3, _>
```

### typenum ã§å‹ãƒ¬ãƒ™ãƒ«æ•°å€¤

```rust
use typenum::{U2, U3, U4, Unsigned};

trait TensorShape {
    type Rows: Unsigned;
    type Cols: Unsigned;
}

struct Matrix<T, Shape: TensorShape> {
    data: Vec<T>,
    _shape: std::marker::PhantomData<Shape>,
}

impl<T: Clone, S: TensorShape> Matrix<T, S> {
    fn new(data: Vec<T>) -> Self {
        assert_eq!(data.len(), S::Rows::USIZE * S::Cols::USIZE);
        Self {
            data,
            _shape: std::marker::PhantomData,
        }
    }
}

// å‹ãƒ¬ãƒ™ãƒ«è¡Œåˆ—ä¹—ç®—
impl<T, S1, S2> std::ops::Mul<Matrix<T, S2>> for Matrix<T, S1>
where
    T: Clone + std::ops::Mul<Output = T> + std::ops::Add<Output = T> + Default,
    S1: TensorShape,
    S2: TensorShape,
    S1::Cols: std::ops::IsEqual<S2::Rows>,  // å‹ãƒ¬ãƒ™ãƒ«ã§å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
{
    type Output = Matrix<T, MatMulShape<S1, S2>>;
    
    fn mul(self, rhs: Matrix<T, S2>) -> Self::Output {
        // å®Ÿè£…...
        todo!()
    }
}
```

---

## ã¾ã¨ã‚

| å´é¢ | Python | Rust |
|------|--------|------|
| **JIT** | TorchScript, JAXï¼ˆæˆç†Ÿï¼‰ | cranelift, LLVMï¼ˆç™ºå±•ä¸­ï¼‰ |
| **DSL** | Triton, NumPyæ§‹æ–‡ | ãƒã‚¯ãƒ­ã‚·ã‚¹ãƒ†ãƒ å¼·åŠ› |
| **å‹å®‰å…¨æ€§** | ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãƒã‚§ãƒƒã‚¯ | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ä¿è¨¼ |
| **æœ€é©åŒ–** | XLA, TVMï¼ˆè‡ªå‹•ï¼‰ | LLVM, æ‰‹å‹•æœ€é©åŒ– |
| **é–‹ç™ºé€Ÿåº¦** | é€Ÿã„ | ä¸­ï¼ˆå‹å®šç¾©å¿…è¦ï¼‰ |

**Rust ã®å„ªä½æ€§**:
- **å‹ãƒ¬ãƒ™ãƒ«è¨ˆç®—**: å½¢çŠ¶ä¸ä¸€è‡´ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«æ¤œå‡º
- **ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–**: ãƒã‚¯ãƒ­å±•é–‹å¾Œã®ã‚³ã‚¹ãƒˆãªã—
- **æ˜ç¤ºçš„åˆ¶å¾¡**: ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’å®Œå…¨åˆ¶å¾¡

**Python ã®å„ªä½æ€§**:
- **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ **: XLA, TVM, Triton ãªã©æˆç†Ÿ
- **ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°**: é«˜é€Ÿãªå®Ÿé¨“ã‚µã‚¤ã‚¯ãƒ«

---

## å‚è€ƒæ–‡çŒ®

1. TorchScript. https://pytorch.org/docs/stable/jit.html
2. JAX. https://github.com/google/jax
3. Lattner, C., et al. (2021). "MLIR: Scaling Compiler Infrastructure for Domain Specific Computation." CGO.
4. Tillet, P., et al. (2019). "Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations." MAPL.
5. Chen, T., et al. (2018). "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning." OSDI.
6. Sabne, A. (2020). "XLA: Compiling Machine Learning for Peak Performance." IEEE Micro.
7. Roesch, J., et al. (2018). "Relay: A New IR for Machine Learning Frameworks." MAPL.
8. cranelift. https://github.com/bytecodealliance/wasmtime/tree/main/cranelift
9. inkwell. https://github.com/TheDan64/inkwell
10. Rust macro book. https://danielkeep.github.io/tlborm/book/
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬15ç« : ãƒ¢ãƒ‡ãƒ«æ¨è«–ã¨ONNXäº’æ›](05-15-ãƒ¢ãƒ‡ãƒ«æ¨è«–ã¨ONNXäº’æ›.md) | [â¡ï¸ ç¬¬17ç« : åˆ†æ•£ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿å¯¾å¿œ](05-17-åˆ†æ•£ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿å¯¾å¿œ.md)