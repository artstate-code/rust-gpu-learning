[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬16ç« ](05-16-ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©æœ€é©åŒ–ã¨DSLè¨­è¨ˆ.md) | [â¡ï¸ ç¬¬18ç« ](05-18-ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ä¿¡é ¼æ€§.md)

---

# ç¬¬ 14 ç« ã€€åˆ†æ•£ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿å¯¾å¿œ

ã“ã®ç« ã§ã¯ã€è¤‡æ•°ã®GPUã‚„ãƒãƒ¼ãƒ‰ã‚’ä½¿ã£ãŸåˆ†æ•£å­¦ç¿’ã®æŠ€è¡“ã‚’å­¦ã³ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã€ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ã€Pipeline Parallelismã€NCCLé€šä¿¡ã€Rustã§ã®MPIå®Ÿè£…ãªã©ã‚’æ‰±ã„ã¾ã™ã€‚

**ç›®çš„**: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«å¿…è¦ãªåˆ†æ•£æŠ€è¡“ã‚’ç†è§£ã—ã€Rustã§å®Ÿè£…ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

## 14.1 ãªãœåˆ†æ•£å­¦ç¿’ãŒå¿…è¦ã‹ï¼Ÿ

### ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®èª²é¡Œ

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | FP32ã‚µã‚¤ã‚º | å¿…è¦VRAMï¼ˆå­¦ç¿’æ™‚ï¼‰ |
|-------|------------|-----------|------------------|
| **ResNet-50** | 25M | 100 MB | ~4 GB |
| **BERT-Base** | 110M | 440 MB | ~16 GB |
| **GPT-2** | 1.5B | 6 GB | ~24 GB |
| **GPT-3** | 175B | 700 GB | ~2.8 TB |
| **Llama-2-70B** | 70B | 280 GB | ~1.12 TB |

**1æšã®GPUã®é™ç•Œ**:
- NVIDIA A100: 80 GB VRAM
- NVIDIA H100: 80 GB VRAM

â†’ **GPT-3ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã¯1æšã§ã¯å­¦ç¿’ä¸å¯èƒ½**

### ä¸¦åˆ—åŒ–ã®ç¨®é¡

```mermaid
graph TD
    Parallel[ä¸¦åˆ—åŒ–æ‰‹æ³•] --> Data[ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—<br/>Data Parallelism]
    Parallel --> Model[ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—<br/>Model Parallelism]
    
    Model --> Tensor[ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—<br/>Tensor Parallelism]
    Model --> Pipeline[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—<br/>Pipeline Parallelism]
    
    Data --> DDP[DDP<br/>PyTorch]
    Data --> ZeRO[ZeRO<br/>DeepSpeed]
    
    Tensor --> Megatron[Megatron-LM]
    Pipeline --> GPipe[GPipe]
    
    style Data fill:#e1f5ff
    style Model fill:#ffe1f5
    style Tensor fill:#fff4e1
    style Pipeline fill:#e1ffe1
```

## 14.2 ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼ˆData Parallelismï¼‰

### åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—

**ã‚¢ã‚¤ãƒ‡ã‚¢**: åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°GPUã«ã‚³ãƒ”ãƒ¼ã—ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’

```mermaid
graph LR
    Data[Training Data] --> Split[Data Split]
    
    Split --> GPU1[GPU 1<br/>Model Copy]
    Split --> GPU2[GPU 2<br/>Model Copy]
    Split --> GPU3[GPU 3<br/>Model Copy]
    
    GPU1 --> Grad1[Gradients 1]
    GPU2 --> Grad2[Gradients 2]
    GPU3 --> Grad3[Gradients 3]
    
    Grad1 --> AllReduce[AllReduce<br/>å‹¾é…å¹³å‡åŒ–]
    Grad2 --> AllReduce
    Grad3 --> AllReduce
    
    AllReduce --> Update[Parameter Update]
    
    Update --> GPU1
    Update --> GPU2
    Update --> GPU3
    
    style Split fill:#e1f5ff
    style AllReduce fill:#ffe1e1
    style Update fill:#e1ffe1
```

### Pythonï¼ˆPyTorch DDPï¼‰

```python
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

def setup(rank, world_size):
    # åˆ†æ•£ç’°å¢ƒåˆæœŸåŒ–
    dist.init_process_group(
        backend='nccl',  # GPUé€šä¿¡ã«NCCLä½¿ç”¨
        init_method='env://',
        world_size=world_size,
        rank=rank
    )

def cleanup():
    dist.destroy_process_group()

def train(rank, world_size):
    setup(rank, world_size)
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = MyModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆå„GPUã§ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ï¼‰
    dataset = MyDataset()
    sampler = DistributedSampler(
        dataset,
        num_replicas=world_size,
        rank=rank
    )
    dataloader = DataLoader(
        dataset,
        batch_size=32,
        sampler=sampler
    )
    
    optimizer = torch.optim.Adam(ddp_model.parameters())
    
    for epoch in range(10):
        sampler.set_epoch(epoch)  # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ç”¨
        
        for batch in dataloader:
            # Forward
            output = ddp_model(batch)
            loss = criterion(output, target)
            
            # Backwardï¼ˆè‡ªå‹•çš„ã«AllReduceã•ã‚Œã‚‹ï¼‰
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    
    cleanup()

# èµ·å‹•ï¼ˆå„ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œï¼‰
if __name__ == '__main__':
    world_size = 4  # 4 GPU
    torch.multiprocessing.spawn(
        train,
        args=(world_size,),
        nprocs=world_size
    )
```

### Rust ã§ã®å®Ÿè£…ï¼ˆMPIï¼‰

**mpi crate** ã‚’ä½¿ç”¨:

```rust
use mpi::traits::*;
use ndarray::Array2;

fn data_parallel_training() {
    let universe = mpi::initialize().unwrap();
    let world = universe.world();
    let rank = world.rank();
    let size = world.size();
    
    println!("Rank {} of {}", rank, size);
    
    // ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆå…¨ãƒ©ãƒ³ã‚¯ã§åŒã˜åˆæœŸå€¤ï¼‰
    let mut model = Model::new();
    
    // ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆå„ãƒ©ãƒ³ã‚¯ã§ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ï¼‰
    let data = load_data_shard(rank, size);
    
    for epoch in 0..10 {
        let mut local_grads = Array2::zeros((784, 10));
        
        // ãƒ­ãƒ¼ã‚«ãƒ«å‹¾é…è¨ˆç®—
        for batch in &data {
            let grad = model.backward(batch);
            local_grads = local_grads + grad;
        }
        
        // AllReduce ã§å‹¾é…å¹³å‡åŒ–
        let mut global_grads = Array2::zeros((784, 10));
        world.all_reduce_into(
            local_grads.as_slice().unwrap(),
            global_grads.as_slice_mut().unwrap(),
            mpi::collective::SystemOperation::sum(),
        );
        
        global_grads = global_grads / size as f32;
        
        // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ï¼ˆå…¨ãƒ©ãƒ³ã‚¯ã§åŒã˜ï¼‰
        model.update(&global_grads, 0.01);
    }
}
```

### ZeROï¼ˆZero Redundancy Optimizerï¼‰

**ZeRO** [^1] ã¯ã€DeepSpeed ã§ææ¡ˆã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—æ‰‹æ³•ã§ã™ã€‚

[^1]: Rajbhandari, S., et al. (2020). "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models." SC20.

**3ã¤ã®ã‚¹ãƒ†ãƒ¼ã‚¸**:

| ã‚¹ãƒ†ãƒ¼ã‚¸ | åˆ†å‰²å¯¾è±¡ | ãƒ¡ãƒ¢ãƒªå‰Šæ¸› | é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ |
|---------|---------|-----------|-----------------|
| **ZeRO-1** | Optimizer States | 4x | ä½ |
| **ZeRO-2** | + Gradients | 8x | ä¸­ |
| **ZeRO-3** | + Parameters | $N$x | é«˜ |

**Pythonï¼ˆDeepSpeedï¼‰**:

```python
import deepspeed

# DeepSpeedè¨­å®š
ds_config = {
    "train_batch_size": 32,
    "gradient_accumulation_steps": 1,
    "optimizer": {
        "type": "Adam",
        "params": {"lr": 0.001}
    },
    "zero_optimization": {
        "stage": 3,  # ZeRO-3
        "offload_optimizer": {
            "device": "cpu"  # OptimizerçŠ¶æ…‹ã‚’CPUã«
        },
        "offload_param": {
            "device": "cpu"  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’CPUã«
        }
    }
}

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
model_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    model_parameters=model.parameters(),
    config=ds_config
)

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
for batch in dataloader:
    loss = model_engine(batch)
    model_engine.backward(loss)
    model_engine.step()
```

## 14.3 Pipeline Parallelismï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—ï¼‰

### åŸºæœ¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ãƒ¢ãƒ‡ãƒ«ã‚’å±¤ã”ã¨ã«è¤‡æ•°GPUã«åˆ†å‰²

```mermaid
graph LR
    Input[Input] --> GPU1[GPU 1<br/>Layer 1-5]
    GPU1 --> GPU2[GPU 2<br/>Layer 6-10]
    GPU2 --> GPU3[GPU 3<br/>Layer 11-15]
    GPU3 --> GPU4[GPU 4<br/>Layer 16-20]
    GPU4 --> Output[Output]
    
    Output --> Loss[Loss]
    Loss --> BW4[Backward<br/>GPU 4]
    BW4 --> BW3[Backward<br/>GPU 3]
    BW3 --> BW2[Backward<br/>GPU 2]
    BW2 --> BW1[Backward<br/>GPU 1]
    
    style GPU1 fill:#e1f5ff
    style GPU2 fill:#ffe1f5
    style GPU3 fill:#fff4e1
    style GPU4 fill:#e1ffe1
```

**å•é¡Œç‚¹**: GPUã‚¢ã‚¤ãƒ‰ãƒ«æ™‚é–“ï¼ˆãƒãƒ–ãƒ«ï¼‰

### GPipe: ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã§ãƒãƒ–ãƒ«ã‚’å‰Šæ¸›

**GPipe** [^2] ã¯ã€ãƒãƒƒãƒã‚’ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã—ã¾ã™ã€‚

[^2]: Huang, Y., et al. (2019). "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism." NeurIPS.

```mermaid
gantt
    title Pipeline Parallelism with Micro-batches
    dateFormat X
    axisFormat %L
    
    section GPU 1
    F1 :f1, 0, 1
    F2 :f2, 1, 1
    F3 :f3, 2, 1
    F4 :f4, 3, 1
    B1 :b1, 7, 1
    B2 :b2, 8, 1
    B3 :b3, 9, 1
    B4 :b4, 10, 1
    
    section GPU 2
    F1 :f5, 1, 1
    F2 :f6, 2, 1
    F3 :f7, 3, 1
    F4 :f8, 4, 1
    B1 :b5, 6, 1
    B2 :b6, 7, 1
    B3 :b7, 8, 1
    B4 :b8, 9, 1
    
    section GPU 3
    F1 :f9, 2, 1
    F2 :f10, 3, 1
    F3 :f11, 4, 1
    F4 :f12, 5, 1
    B1 :b9, 5, 1
    B2 :b10, 6, 1
    B3 :b11, 7, 1
    B4 :b12, 8, 1
```

**Pythonï¼ˆPyTorch PipelineParallelï¼‰**:

```python
from torch.distributed.pipeline.sync import Pipe

# ãƒ¢ãƒ‡ãƒ«ã‚’å±¤ã«åˆ†å‰²
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Sequential(*[layer for i in range(5)])   # GPU 0
        self.layer2 = nn.Sequential(*[layer for i in range(5)])   # GPU 1
        self.layer3 = nn.Sequential(*[layer for i in range(5)])   # GPU 2
        self.layer4 = nn.Sequential(*[layer for i in range(5)])   # GPU 3
    
    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        return x

model = Model()

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–
model = Pipe(
    model,
    chunks=8,  # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒæ•°
    balance=[5, 5, 5, 5],  # å„GPUã®å±¤æ•°
    devices=[0, 1, 2, 3]
)

# å­¦ç¿’
for batch in dataloader:
    output = model(batch).local_value()  # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    loss = criterion(output, target)
    loss.backward()
```

## 14.4 Tensor Parallelismï¼ˆãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—ï¼‰

### Megatron-LM ã®ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—

**Megatron-LM** [^3] ã¯ã€Transformer ã®è¡Œåˆ—æ¼”ç®—ã‚’è¤‡æ•°GPUã«åˆ†å‰²ã—ã¾ã™ã€‚

[^3]: Shoeybi, M., et al. (2019). "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism." arXiv:1909.08053

**è¡Œåˆ—ä¹—ç®—ã®åˆ†å‰²**:

$$
Y = XW
$$

ã‚’2ã¤ã®GPUã«åˆ†å‰²:

$$
\begin{align}
Y_1 &= X W_1 \quad \text{(GPU 1)} \\
Y_2 &= X W_2 \quad \text{(GPU 2)} \\
Y &= [Y_1 | Y_2]
\end{align}
$$

```mermaid
graph TD
    X[Input X<br/>broadcast] --> GPU1[GPU 1<br/>W1]
    X --> GPU2[GPU 2<br/>W2]
    
    GPU1 --> Y1[Y1]
    GPU2 --> Y2[Y2]
    
    Y1 --> Concat[Concatenate]
    Y2 --> Concat
    
    Concat --> Y[Output Y]
    
    style X fill:#e1f5ff
    style GPU1 fill:#ffe1f5
    style GPU2 fill:#fff4e1
    style Concat fill:#e1ffe1
```

**Python å®Ÿè£…ä¾‹**:

```python
import torch
import torch.distributed as dist

class ColumnParallelLinear(nn.Module):
    """åˆ—æ–¹å‘ã«åˆ†å‰²ã•ã‚ŒãŸç·šå½¢å±¤"""
    def __init__(self, in_features, out_features, world_size):
        super().__init__()
        assert out_features % world_size == 0
        self.out_features_per_partition = out_features // world_size
        
        self.weight = nn.Parameter(
            torch.empty(self.out_features_per_partition, in_features)
        )
        self.bias = nn.Parameter(
            torch.empty(self.out_features_per_partition)
        )
    
    def forward(self, x):
        # ãƒ­ãƒ¼ã‚«ãƒ«è¨ˆç®—
        output = F.linear(x, self.weight, self.bias)
        return output

class RowParallelLinear(nn.Module):
    """è¡Œæ–¹å‘ã«åˆ†å‰²ã•ã‚ŒãŸç·šå½¢å±¤"""
    def __init__(self, in_features, out_features, world_size):
        super().__init__()
        assert in_features % world_size == 0
        self.in_features_per_partition = in_features // world_size
        
        self.weight = nn.Parameter(
            torch.empty(out_features, self.in_features_per_partition)
        )
        self.bias = nn.Parameter(torch.empty(out_features))
    
    def forward(self, x):
        # ãƒ­ãƒ¼ã‚«ãƒ«è¨ˆç®—
        output = F.linear(x, self.weight)
        
        # AllReduce ã§çµæœã‚’é›†ç´„
        dist.all_reduce(output, op=dist.ReduceOp.SUM)
        
        # ãƒã‚¤ã‚¢ã‚¹ã¯1å›ã ã‘åŠ ç®—
        if dist.get_rank() == 0:
            output = output + self.bias
        
        return output
```

## 14.5 NCCL ã¨é›†å›£é€šä¿¡

### NCCLï¼ˆNVIDIA Collective Communications Libraryï¼‰

**NCCL** [^4] ã¯ã€GPUé–“ã®é«˜é€Ÿé€šä¿¡ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚

[^4] NCCL. https://developer.nvidia.com/nccl

**ä¸»è¦ãªé›†å›£é€šä¿¡æ“ä½œ**:

#### AllReduce

å…¨GPUã§å€¤ã‚’é›†ç´„ã—ã€çµæœã‚’å…¨GPUã«é…å¸ƒ:

$$
\text{AllReduce}([a_0, a_1, a_2, a_3]) = [a_0 + a_1 + a_2 + a_3, ...]
$$

```mermaid
graph TD
    GPU0[GPU 0: a] --> Reduce[Reduce]
    GPU1[GPU 1: b] --> Reduce
    GPU2[GPU 2: c] --> Reduce
    GPU3[GPU 3: d] --> Reduce
    
    Reduce --> Sum[Sum = a+b+c+d]
    
    Sum --> Out0[GPU 0: Sum]
    Sum --> Out1[GPU 1: Sum]
    Sum --> Out2[GPU 2: Sum]
    Sum --> Out3[GPU 3: Sum]
    
    style Reduce fill:#ffe1e1
```

**Python**:

```python
import torch.distributed as dist

tensor = torch.tensor([rank], device='cuda')
dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
# rank=0,1,2,3 â†’ ã™ã¹ã¦6ã«ãªã‚‹
```

#### AllGather

å…¨GPUã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†:

```mermaid
graph TD
    GPU0[GPU 0: a] --> Gather[Gather]
    GPU1[GPU 1: b] --> Gather
    GPU2[GPU 2: c] --> Gather
    GPU3[GPU 3: d] --> Gather
    
    Gather --> Out0[GPU 0: [a,b,c,d]]
    Gather --> Out1[GPU 1: [a,b,c,d]]
    Gather --> Out2[GPU 2: [a,b,c,d]]
    Gather --> Out3[GPU 3: [a,b,c,d]]
```

**Python**:

```python
tensor = torch.tensor([rank], device='cuda')
tensor_list = [torch.zeros(1, device='cuda') for _ in range(world_size)]
dist.all_gather(tensor_list, tensor)
# tensor_list = [0, 1, 2, 3]
```

#### Reduce-Scatter

é›†ç´„ã—ã¦åˆ†æ•£:

```mermaid
graph TD
    GPU0[GPU 0: [a0,a1,a2,a3]] --> RS[Reduce-Scatter]
    GPU1[GPU 1: [b0,b1,b2,b3]] --> RS
    GPU2[GPU 2: [c0,c1,c2,c3]] --> RS
    GPU3[GPU 3: [d0,d1,d2,d3]] --> RS
    
    RS --> Out0[GPU 0: a0+b0+c0+d0]
    RS --> Out1[GPU 1: a1+b1+c1+d1]
    RS --> Out2[GPU 2: a2+b2+c2+d2]
    RS --> Out3[GPU 3: a3+b3+c3+d3]
```

### Rust ã§ã® NCCL ä½¿ç”¨

**nccl-sys** crate ã‚’ä½¿ç”¨:

```rust
use nccl_sys::*;
use std::ptr;

unsafe fn nccl_allreduce_example() {
    let device_count = 4;
    let mut comms: Vec<ncclComm_t> = vec![ptr::null_mut(); device_count];
    
    // NCCL åˆæœŸåŒ–
    ncclCommInitAll(
        comms.as_mut_ptr(),
        device_count as i32,
        (0..device_count as i32).collect::<Vec<_>>().as_ptr(),
    );
    
    // å„GPUä¸Šã§AllReduce
    for rank in 0..device_count {
        let mut data = vec![rank as f32; 1000];
        
        // AllReduceå®Ÿè¡Œ
        ncclAllReduce(
            data.as_ptr() as *const _,
            data.as_mut_ptr() as *mut _,
            data.len(),
            ncclDataType_t::ncclFloat,
            ncclRedOp_t::ncclSum,
            comms[rank],
            ptr::null_mut(),
        );
    }
    
    // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    for comm in comms {
        ncclCommDestroy(comm);
    }
}
```

## 14.6 RPCãƒ»gRPC ã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰é€šä¿¡

### gRPC for Rust

**tonic** crate ã§gRPCã‚µãƒ¼ãƒ/ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ:

```rust
// proto ãƒ•ã‚¡ã‚¤ãƒ«å®šç¾©
// distributed_training.proto
syntax = "proto3";

service ParameterServer {
    rpc PushGradients(GradientRequest) returns (EmptyResponse);
    rpc PullParameters(EmptyRequest) returns (ParameterResponse);
}

message GradientRequest {
    repeated float gradients = 1;
    int32 worker_id = 2;
}

message ParameterResponse {
    repeated float parameters = 1;
}
```

**ã‚µãƒ¼ãƒå®Ÿè£…**:

```rust
use tonic::{transport::Server, Request, Response, Status};

pub struct ParameterServerImpl {
    parameters: Arc<RwLock<Vec<f32>>>,
}

#[tonic::async_trait]
impl ParameterServer for ParameterServerImpl {
    async fn push_gradients(
        &self,
        request: Request<GradientRequest>,
    ) -> Result<Response<EmptyResponse>, Status> {
        let req = request.into_inner();
        let mut params = self.parameters.write().unwrap();
        
        // å‹¾é…ã‚’é©ç”¨ï¼ˆç°¡ç•¥åŒ–ï¼‰
        for (param, grad) in params.iter_mut().zip(&req.gradients) {
            *param -= 0.01 * grad;
        }
        
        Ok(Response::new(EmptyResponse {}))
    }
    
    async fn pull_parameters(
        &self,
        _request: Request<EmptyRequest>,
    ) -> Result<Response<ParameterResponse>, Status> {
        let params = self.parameters.read().unwrap();
        
        Ok(Response::new(ParameterResponse {
            parameters: params.clone(),
        }))
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let addr = "0.0.0.0:50051".parse()?;
    let server = ParameterServerImpl {
        parameters: Arc::new(RwLock::new(vec![0.0; 10000])),
    };
    
    Server::builder()
        .add_service(ParameterServerServer::new(server))
        .serve(addr)
        .await?;
    
    Ok(())
}
```

**ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®Ÿè£…**:

```rust
use tonic::transport::Channel;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let mut client = ParameterServerClient::connect("http://127.0.0.1:50051").await?;
    
    // å‹¾é…ã‚’ãƒ—ãƒƒã‚·ãƒ¥
    let request = tonic::Request::new(GradientRequest {
        gradients: vec![0.1; 10000],
        worker_id: 0,
    });
    client.push_gradients(request).await?;
    
    // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ«
    let request = tonic::Request::new(EmptyRequest {});
    let response = client.pull_parameters(request).await?;
    
    println!("Received {} parameters", response.into_inner().parameters.len());
    
    Ok(())
}
```

## 14.7 ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ

### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜

```rust
use serde::{Deserialize, Serialize};
use std::fs::File;
use std::io::Write;

#[derive(Serialize, Deserialize)]
struct Checkpoint {
    epoch: usize,
    parameters: Vec<f32>,
    optimizer_state: Vec<f32>,
    loss: f32,
}

impl Checkpoint {
    fn save(&self, path: &str) -> std::io::Result<()> {
        let serialized = bincode::serialize(&self)
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e))?;
        let mut file = File::create(path)?;
        file.write_all(&serialized)?;
        Ok(())
    }
    
    fn load(path: &str) -> std::io::Result<Self> {
        let data = std::fs::read(path)?;
        bincode::deserialize(&data)
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::Other, e))
    }
}

// ä½¿ç”¨ä¾‹
fn train_with_checkpointing() {
    for epoch in 0..100 {
        // å­¦ç¿’...
        
        // å®šæœŸçš„ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        if epoch % 10 == 0 {
            let checkpoint = Checkpoint {
                epoch,
                parameters: model.parameters(),
                optimizer_state: optimizer.state(),
                loss: current_loss,
            };
            checkpoint.save(&format!("checkpoint_epoch_{}.bin", epoch)).unwrap();
        }
    }
}
```

---

## ã¾ã¨ã‚

| æ‰‹æ³• | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ | é€šä¿¡é‡ | å®Ÿè£…é›£æ˜“åº¦ | ç”¨é€” |
|------|-----------|--------|-----------|------|
| **Data Parallel** | ä½ | ä¸­ | æ˜“ | å°ã€œä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ« |
| **ZeRO** | é«˜ | ä¸­ã€œé«˜ | ä¸­ | ä¸­ã€œå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ« |
| **Pipeline Parallel** | é«˜ | ä½ | ä¸­ | æ·±ã„ãƒ¢ãƒ‡ãƒ« |
| **Tensor Parallel** | æœ€é«˜ | é«˜ | é›£ | è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ« |

**Rust ã®å„ªä½æ€§**:
- **ä½ãƒ¬ãƒ™ãƒ«åˆ¶å¾¡**: NCCLã€MPIã®ç›´æ¥åˆ¶å¾¡
- **å‹å®‰å…¨**: åˆ†æ•£é€šä¿¡ã®å‹ãƒã‚§ãƒƒã‚¯
- **ã‚¼ãƒ­ã‚³ãƒ”ãƒ¼**: åŠ¹ç‡çš„ãªãƒ¡ãƒ¢ãƒªè»¢é€

**Python ã®å„ªä½æ€§**:
- **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ **: PyTorch DDP, DeepSpeed, Megatron
- **ç°¡å˜**: é«˜ãƒ¬ãƒ™ãƒ«APIã§è‡ªå‹•åŒ–

---

## å‚è€ƒæ–‡çŒ®

1. Rajbhandari, S., et al. (2020). "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models." SC20.
2. Huang, Y., et al. (2019). "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism." NeurIPS.
3. Shoeybi, M., et al. (2019). "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism." arXiv:1909.08053
4. NCCL. https://developer.nvidia.com/nccl
5. Li, M., et al. (2014). "Scaling Distributed Machine Learning with the Parameter Server." OSDI.
6. Narayanan, D., et al. (2021). "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM." SC21.
7. Rasley, J., et al. (2020). "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters." KDD.
8. tonic (gRPC for Rust). https://github.com/hyperium/tonic
9. mpi crate. https://github.com/rsmpi/rsmpi
10. PyTorch Distributed. https://pytorch.org/tutorials/beginner/dist_overview.html
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬16ç« : ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©æœ€é©åŒ–ã¨DSLè¨­è¨ˆ](05-16-ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©æœ€é©åŒ–ã¨DSLè¨­è¨ˆ.md) | [â¡ï¸ ç¬¬18ç« : ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ä¿¡é ¼æ€§](05-18-ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ä¿¡é ¼æ€§.md)