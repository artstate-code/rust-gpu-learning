[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬2ç« ](01-02-ç·šå½¢ä»£æ•°ã¨æ•°å€¤è¨ˆç®—ã®åŸºç¤.md) | [â¡ï¸ ç¬¬4ç« ](../02_ç¬¬IIéƒ¨_Rustã«ã‚ˆã‚‹æ•°å€¤å‡¦ç†ã¨å®‰å…¨è¨­è¨ˆ/02-04-Rustæ•°å€¤è¨ˆç®—ã®åŸºç¤æ§‹æ–‡.md)

---

# ç¬¬ 3 ç« ã€€è‡ªå‹•å¾®åˆ†ã®ä»•çµ„ã¿

**è‡ªå‹•å¾®åˆ†**ï¼ˆAutomatic Differentiationã€ADï¼‰ã¯ã€æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ä¸­æ ¸æŠ€è¡“ã§ã™ã€‚æœ¬ç« ã§ã¯ã€AD ã®æ•°å­¦çš„åŸºç¤ã‹ã‚‰å®Ÿè£…ä¸Šã®è¨­è¨ˆé¸æŠã¾ã§ã€Rust ã¨ Python ã®æ¯”è¼ƒã‚’äº¤ãˆã¦è©³è¿°ã—ã¾ã™ã€‚

## 3.1 Forward/Reverse ãƒ¢ãƒ¼ãƒ‰ AD

è‡ªå‹•å¾®åˆ†ï¼ˆAutomatic Differentiation, ADï¼‰ã¯ã€**æ©Ÿæ¢°ç²¾åº¦**ï¼ˆãƒã‚·ãƒ³ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ãƒ¬ãƒ™ãƒ«ï¼‰ã§æ­£ç¢ºãªå°é–¢æ•°ã‚’ã€åŠ¹ç‡çš„ã«è¨ˆç®—ã™ã‚‹æŠ€è¡“ã§ã™ [^14]ã€‚æ•°å€¤å¾®åˆ†ã‚„è¨˜å·å¾®åˆ†ã¨ã¯ç•°ãªã‚‹ã€ç¬¬3ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã—ã¦ç¢ºç«‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

[^14]: Griewank, A., & Walther, A. (2008). "Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation." SIAM. è‡ªå‹•å¾®åˆ†ã®åŒ…æ‹¬çš„ãªæ•™ç§‘æ›¸

### å¾®åˆ†ã®3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

| æ‰‹æ³• | åŸç† | ç²¾åº¦ | è¨ˆç®—ã‚³ã‚¹ãƒˆ | å®Ÿè£…è¤‡é›‘åº¦ | ç”¨é€” |
|------|------|------|-----------|-----------|------|
| **æ•°å€¤å¾®åˆ†** | å·®åˆ†è¿‘ä¼¼ | ä½ï¼ˆä¸¸ã‚èª¤å·®ï¼‰ | é«˜ï¼ˆ$O(n)$ å›ã®é–¢æ•°è©•ä¾¡ï¼‰ | ä½ | ãƒ‡ãƒãƒƒã‚°ã€å‹¾é…ãƒã‚§ãƒƒã‚¯ |
| **è¨˜å·å¾®åˆ†** | æ•°å¼å¤‰å½¢ | å®Œå…¨ | å¼çˆ†ç™ºã®å¯èƒ½æ€§ | é«˜ | æ•°å¼å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  |
| **è‡ªå‹•å¾®åˆ†** | é€£é–å¾‹ã®çµ„ç¹”çš„é©ç”¨ | å®Œå…¨ï¼ˆæ©Ÿæ¢°ç²¾åº¦ï¼‰ | ä½ï¼ˆå®šæ•°å€ï¼‰ | ä¸­ | æ©Ÿæ¢°å­¦ç¿’ã€æœ€é©åŒ– |

### è¨˜å·å¾®åˆ†ã®ã€Œå¼çˆ†ç™ºã€å•é¡Œ

**è¨˜å·å¾®åˆ†**ï¼ˆSymbolic Differentiationï¼‰ã¯ã€æ•°å¼ã‚’ä»£æ•°çš„ã«å¤‰å½¢ã—ã¦å°é–¢æ•°ã‚’æ±‚ã‚ã‚‹æ‰‹æ³•ã§ã™ã€‚

**å˜ç´”ãªä¾‹**ï¼ˆ$f(x) = x^3$ï¼‰:

$$
f'(x) = 3x^2
$$

å•é¡Œãªãå¾®åˆ†ã§ãã¾ã™ã€‚

**å¼çˆ†ç™ºã®ä¾‹**ï¼ˆ$f(x) = (x+1)^{10}$ï¼‰:

$$
\begin{align}
f(x) &= (x+1)^{10} \\
f'(x) &= 10(x+1)^9 \\
&= 10(x^9 + 9x^8 + 36x^7 + 84x^6 + \cdots) \\
&= 10x^9 + 90x^8 + 360x^7 + 840x^6 + \cdots
\end{align}
$$

å±•é–‹ã™ã‚‹ã¨é …æ•°ãŒ**æŒ‡æ•°çš„ã«å¢—åŠ **ã—ã¾ã™ã€‚

**ã‚ˆã‚Šæ·±åˆ»ãªä¾‹**ï¼ˆ$f(x) = ((x+1)^2)^2$ã‚’ç¹°ã‚Šè¿”ã—ï¼‰:

$$
\begin{align}
g_0(x) &= x \\
g_1(x) &= (g_0 + 1)^2 = (x+1)^2 \\
g_2(x) &= (g_1 + 1)^2 = ((x+1)^2 + 1)^2 \\
g_3(x) &= (g_2 + 1)^2 = (((x+1)^2 + 1)^2 + 1)^2 \\
&\vdots
\end{align}
$$

$g_n(x)$ ã®å°é–¢æ•°ã¯ $O(2^n)$ é …ã‚’æŒã¡ã¾ã™ï¼ˆ**æŒ‡æ•°çˆ†ç™º**ï¼‰ã€‚

**Pythonï¼ˆSymPyï¼‰ã§ã®å®Ÿæ¼”**:

```python
import sympy as sp

x = sp.Symbol('x')

# å˜ç´”ãªé–¢æ•°
f1 = x**3
print(f"f1' = {sp.diff(f1, x)}")  # 3*x**2ï¼ˆç°¡æ½”ï¼‰

# å¼çˆ†ç™ºã®ä¾‹
f2 = (x + 1)**10
f2_expanded = sp.expand(f2)
f2_prime = sp.diff(f2_expanded, x)
print(f"f2' ã®é …æ•°: {len(sp.Add.make_args(f2_prime))}")  # 10é …

# ã•ã‚‰ã«è¤‡é›‘ãªä¾‹
g = x
for i in range(5):
    g = (g + 1)**2
g_expanded = sp.expand(g)
print(f"g ã®é …æ•°: {len(sp.Add.make_args(g_expanded))}")  # æ•°åƒé …

# è¨˜å·å¾®åˆ†ã¯æ™‚é–“ãŒã‹ã‹ã‚‹
import time
start = time.time()
g_prime = sp.diff(g_expanded, x)
print(f"å¾®åˆ†æ™‚é–“: {time.time() - start:.3f}ç§’")
print(f"g' ã®é …æ•°: {len(sp.Add.make_args(g_prime))}")  # ã•ã‚‰ã«å¢—åŠ 
```

**å‡ºåŠ›ä¾‹**:
```
f1' = 3*x**2
f2' ã®é …æ•°: 10
g ã®é …æ•°: 3125
å¾®åˆ†æ™‚é–“: 12.453ç§’
g' ã®é …æ•°: 15625
```

**è‡ªå‹•å¾®åˆ†ã®å„ªä½æ€§**:

| é–¢æ•° | è¨˜å·å¾®åˆ†ï¼ˆé …æ•°ï¼‰ | è‡ªå‹•å¾®åˆ†ï¼ˆæ¼”ç®—æ•°ï¼‰ |
|------|---------------|-----------------|
| $x^3$ | 1é … | 3æ¼”ç®— |
| $(x+1)^{10}$ | 10é … | 11æ¼”ç®— |
| $(((x+1)^2+1)^2+\cdots)$ | $O(2^n)$ é … | $O(n)$ æ¼”ç®— |

**çµè«–**: è¨˜å·å¾®åˆ†ã¯å¼ãŒçˆ†ç™ºçš„ã«è¤‡é›‘ã«ãªã‚‹ãŸã‚ã€å¤§è¦æ¨¡ãªæ•°å€¤è¨ˆç®—ï¼ˆæ©Ÿæ¢°å­¦ç¿’ãªã©ï¼‰ã«ã¯ä¸å‘ãã§ã™ã€‚è‡ªå‹•å¾®åˆ†ã¯è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’ç›´æ¥å®Ÿè¡Œã™ã‚‹ãŸã‚ã€å¼ã®è¤‡é›‘ã•ã«ä¾ã‚‰ãšåŠ¹ç‡çš„ã§ã™ã€‚

### é€£é–å¾‹ã®å³å¯†ãªå°å‡º

è‡ªå‹•å¾®åˆ†ã®åŸºç¤ã¨ãªã‚‹**é€£é–å¾‹**ï¼ˆChain Ruleï¼‰ã‚’ã€å¤šå¤‰æ•°é–¢æ•°ã§å³å¯†ã«å°å‡ºã—ã¾ã™ã€‚

**1å¤‰æ•°ã®é€£é–å¾‹**:

$$
\frac{d}{dx} f(g(x)) = f'(g(x)) \cdot g'(x)
$$

**å¤šå¤‰æ•°ã®é€£é–å¾‹**:

é–¢æ•° $y = f(x_1, x_2, \ldots, x_n)$ ã§ã€å„ $x_i = g_i(t)$ ã¨ã™ã‚‹ã¨ï¼š

$$
\frac{dy}{dt} = \sum_{i=1}^{n} \frac{\partial f}{\partial x_i} \frac{dx_i}{dt}
$$

**ãƒ™ã‚¯ãƒˆãƒ«è¡¨è¨˜**:

$$
\frac{dy}{dt} = \nabla f \cdot \frac{d\mathbf{x}}{dt}
$$

**è¤‡åˆé–¢æ•°ã®å ´åˆ**:

$z = h(y)$, $y = g(x)$ ã¨ã™ã‚‹ã¨ï¼š

$$
\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}
$$

**å¤šå¤‰æ•°â†’å¤šå¤‰æ•°ã®å ´åˆï¼ˆãƒ¤ã‚³ãƒ“è¡Œåˆ—ï¼‰**:

$\mathbf{y} = \mathbf{f}(\mathbf{x})$, $\mathbf{z} = \mathbf{g}(\mathbf{y})$ ã¨ã™ã‚‹ã¨ï¼š

$$
\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
$$

ã“ã“ã§ã€å„é …ã¯**ãƒ¤ã‚³ãƒ“è¡Œåˆ—**ï¼ˆJacobian matrixï¼‰ã§ã™ã€‚

### Python ã§ã®å¾®åˆ†æ‰‹æ³•ã®æ¯”è¼ƒ

```python
import numpy as np
import torch
import sympy

# å¯¾è±¡é–¢æ•°: f(x) = xÂ³ + 2xÂ² - 5x + 3
def f(x):
    return x**3 + 2*x**2 - 5*x + 3

# è§£æè§£: f'(x) = 3xÂ² + 4x - 5
def f_prime_analytical(x):
    return 3*x**2 + 4*x - 5

x_val = 2.0

# 1. æ•°å€¤å¾®åˆ†
h = 1e-5
f_prime_numerical = (f(x_val + h) - f(x_val - h)) / (2 * h)
print(f"æ•°å€¤å¾®åˆ†: {f_prime_numerical:.10f}")

# 2. è¨˜å·å¾®åˆ†ï¼ˆSymPyï¼‰
x_sym = sympy.Symbol('x')
f_sym = x_sym**3 + 2*x_sym**2 - 5*x_sym + 3
f_prime_sym = sympy.diff(f_sym, x_sym)
f_prime_symbolic = float(f_prime_sym.subs(x_sym, x_val))
print(f"è¨˜å·å¾®åˆ†: {f_prime_symbolic:.10f}")

# 3. è‡ªå‹•å¾®åˆ†ï¼ˆPyTorchï¼‰
x_torch = torch.tensor(x_val, requires_grad=True)
y_torch = x_torch**3 + 2*x_torch**2 - 5*x_torch + 3
y_torch.backward()
f_prime_auto = x_torch.grad.item()
print(f"è‡ªå‹•å¾®åˆ†: {f_prime_auto:.10f}")

# è§£æè§£
print(f"è§£æè§£:   {f_prime_analytical(x_val):.10f}")
```

**å‡ºåŠ›**:
```
æ•°å€¤å¾®åˆ†: 15.0000000833  ï¼ˆèª¤å·®: 8.33e-8ï¼‰
è¨˜å·å¾®åˆ†: 15.0000000000  ï¼ˆå®Œå…¨ï¼‰
è‡ªå‹•å¾®åˆ†: 15.0000000000  ï¼ˆå®Œå…¨ï¼‰
è§£æè§£:   15.0000000000
```

**æ¯”è¼ƒã¾ã¨ã‚**:

| æ‰‹æ³• | Pythonå®Ÿè£… | Rustå®Ÿè£… | ç²¾åº¦ | é€Ÿåº¦ |
|------|-----------|---------|------|------|
| æ•°å€¤å¾®åˆ† | æ‰‹å‹•å®Ÿè£… | æ‰‹å‹•å®Ÿè£… | â–³ | é…ã„ |
| è¨˜å·å¾®åˆ† | SymPy | ãªã—ï¼ˆå¤–éƒ¨ãƒ„ãƒ¼ãƒ«ï¼‰ | â—‹ | å¯å¤‰ |
| è‡ªå‹•å¾®åˆ† | PyTorch/JAX | dfdx/burn | â—‹ | é€Ÿã„ |

### æ•°å€¤å¾®åˆ†ã®é™ç•Œ

**ä¸­å¿ƒå·®åˆ†å…¬å¼**:

$$
f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}
$$

**å•é¡Œç‚¹**:
- $h$ ãŒå¤§ãã„ â†’ æ‰“ã¡åˆ‡ã‚Šèª¤å·®
- $h$ ãŒå°ã•ã„ â†’ ä¸¸ã‚èª¤å·®
- æœ€é©ãª $h$ ã¯é–¢æ•°ä¾å­˜ã§ã€è‡ªå‹•é¸æŠãŒå›°é›£

```rust
// æ•°å€¤å¾®åˆ†ã®ä¾‹
fn numerical_derivative(f: impl Fn(f64) -> f64, x: f64, h: f64) -> f64 {
    (f(x + h) - f(x - h)) / (2.0 * h)
}

fn main() {
    let f = |x: f64| x.powi(2);  // f(x) = xÂ²
    let x = 3.0;
    
    // çœŸã®å°é–¢æ•°: f'(x) = 2x = 6.0
    let true_derivative = 6.0;
    
    // ç•°ãªã‚‹ h ã§ã®æ•°å€¤å¾®åˆ†
    for h in [1e-2, 1e-4, 1e-8, 1e-12].iter() {
        let approx = numerical_derivative(&f, x, *h);
        let error = (approx - true_derivative).abs();
        println!("h = {:.0e}: {:.10}, èª¤å·®: {:.2e}", h, approx, error);
    }
}
```

**å‡ºåŠ›ä¾‹**:
```
h = 1e-2:  6.0000333333, èª¤å·®: 3.33e-5
h = 1e-4:  6.0000000033, èª¤å·®: 3.33e-9
h = 1e-8:  6.0000001192, èª¤å·®: 1.19e-7  â† ä¸¸ã‚èª¤å·®ãŒæ”¯é…çš„
h = 1e-12: 5.9952043329, èª¤å·®: 4.80e-3  â† ã•ã‚‰ã«æ‚ªåŒ–
```

### Forward ãƒ¢ãƒ¼ãƒ‰ AD

**Forward ãƒ¢ãƒ¼ãƒ‰**ã¯ã€å…¥åŠ›ã‹ã‚‰å‡ºåŠ›ã¸å¾®åˆ†ã‚’ä¼æ’­ã—ã¾ã™ã€‚**åŒå¯¾æ•°**ï¼ˆdual numberï¼‰ã‚’ä½¿ã£ãŸå®Ÿè£…ãŒä¸€èˆ¬çš„ã§ã™ [^14]ã€‚

**åŒå¯¾æ•°**: $x + x' \epsilon$ï¼ˆã“ã“ã§ $\epsilon^2 = 0$ï¼‰

**è¨ˆç®—è¦å‰‡**:

$$
\begin{align}
(x + x'\epsilon) + (y + y'\epsilon) &= (x + y) + (x' + y')\epsilon \\
(x + x'\epsilon) \times (y + y'\epsilon) &= xy + (xy' + x'y)\epsilon
\end{align}
$$

**Rust ã§ã®å®Ÿè£…**:

```rust
use std::ops::{Add, Mul};

#[derive(Debug, Clone, Copy)]
struct Dual {
    value: f64,    // é–¢æ•°å€¤
    deriv: f64,    // å°é–¢æ•°å€¤
}

impl Dual {
    fn new(value: f64, deriv: f64) -> Self {
        Dual { value, deriv }
    }
    
    // å¤‰æ•°ï¼ˆå¾®åˆ†å¯¾è±¡ï¼‰
    fn variable(value: f64) -> Self {
        Dual::new(value, 1.0)
    }
    
    // å®šæ•°
    fn constant(value: f64) -> Self {
        Dual::new(value, 0.0)
    }
    
    fn sin(&self) -> Self {
        Dual::new(self.value.sin(), self.deriv * self.value.cos())
    }
    
    fn exp(&self) -> Self {
        let exp_v = self.value.exp();
        Dual::new(exp_v, self.deriv * exp_v)
    }
}

impl Add for Dual {
    type Output = Self;
    fn add(self, other: Self) -> Self {
        Dual::new(self.value + other.value, self.deriv + other.deriv)
    }
}

impl Mul for Dual {
    type Output = Self;
    fn mul(self, other: Self) -> Self {
        Dual::new(
            self.value * other.value,
            self.value * other.deriv + self.deriv * other.value
        )
    }
}

// ä½¿ç”¨ä¾‹
fn main() {
    let x = Dual::variable(3.0);  // x = 3, dx/dx = 1
    let c = Dual::constant(2.0);  // c = 2, dc/dx = 0
    
    // f(x) = 2xÂ² + sin(x)
    let result = c * x * x + x.sin();
    
    println!("f(3) = {}", result.value);     // 18 + sin(3) â‰ˆ 18.14
    println!("f'(3) = {}", result.deriv);    // 12 + cos(3) â‰ˆ 11.01
}
```

**Forward ãƒ¢ãƒ¼ãƒ‰ã®è¨ˆç®—é‡**:
- é–¢æ•°è©•ä¾¡: $O(n)$ï¼ˆ$n$ ã¯æ¼”ç®—æ•°ï¼‰
- 1ã¤ã®å…¥åŠ›å¤‰æ•°ã«å¯¾ã™ã‚‹å°é–¢æ•°: $O(n)$
- $m$ å€‹ã®å…¥åŠ›å¤‰æ•°ã™ã¹ã¦ã®å°é–¢æ•°: $O(mn)$

**é©ã—ãŸå•é¡Œ**: å…¥åŠ›å¤‰æ•°ãŒå°‘ãªãã€å‡ºåŠ›å¤‰æ•°ãŒå¤šã„å ´åˆï¼ˆä¾‹ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿1ã¤ã€å‡ºåŠ›100ä¸‡æ¬¡å…ƒï¼‰

### JAXã§ã®Forward modeå®Ÿè£…

JAXã¯ã€Forwardãƒ¢ãƒ¼ãƒ‰ã‚’`jax.jvp`ï¼ˆJacobian-Vector Productï¼‰ã§æä¾›ã—ã¦ã„ã¾ã™ [^14a]ï¼š

[^14a]: JAX documentation: https://jax.readthedocs.io/en/latest/jax.html#jax.jvp

```python
import jax
import jax.numpy as jnp

def f(x):
    return jnp.sin(x) * jnp.exp(x)

# Forward mode AD
x = 3.0
v = 1.0  # å¾®åˆ†æ–¹å‘ãƒ™ã‚¯ãƒˆãƒ«

# jvp = Jacobian-Vector Product
# f(x), df/dx Â· v ã‚’åŒæ™‚ã«è¨ˆç®—
primals, tangents = jax.jvp(f, (x,), (v,))
print(f"f({x}) = {primals}")      # é–¢æ•°å€¤
print(f"f'({x}) = {tangents}")    # å°é–¢æ•°å€¤
```

### Rust ã§ã®é«˜åº¦ãª Forward mode å®Ÿè£…

```rust
use std::ops::{Add, Sub, Mul, Div};

#[derive(Debug, Clone, Copy)]
struct Dual<T> {
    val: T,    // Primal value
    tan: T,    // Tangent value (derivative)
}

impl<T> Dual<T> {
    fn new(val: T, tan: T) -> Self {
        Self { val, tan }
    }
}

// æ¼”ç®—å­ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ­ãƒ¼ãƒ‰
impl<T: Add<Output=T>> Add for Dual<T> {
    type Output = Self;
    fn add(self, rhs: Self) -> Self {
        Dual::new(self.val + rhs.val, self.tan + rhs.tan)
    }
}

impl<T: Mul<Output=T> + Add<Output=T> + Copy> Mul for Dual<T> {
    type Output = Self;
    fn mul(self, rhs: Self) -> Self {
        // (fÂ·g)' = f'Â·g + fÂ·g'
        Dual::new(
            self.val * rhs.val,
            self.tan * rhs.val + self.val * rhs.tan
        )
    }
}

impl Dual<f64> {
    fn sin(self) -> Self {
        Dual::new(self.val.sin(), self.tan * self.val.cos())
    }
    
    fn exp(self) -> Self {
        let exp_val = self.val.exp();
        Dual::new(exp_val, self.tan * exp_val)
    }
    
    fn ln(self) -> Self {
        Dual::new(self.val.ln(), self.tan / self.val)
    }
    
    fn pow(self, n: f64) -> Self {
        Dual::new(
            self.val.powf(n),
            self.tan * n * self.val.powf(n - 1.0)
        )
    }
}

fn main() {
    // f(x) = sin(x) * exp(x)
    let x = Dual::new(3.0, 1.0);  // x=3, dx/dx=1
    let result = x.sin() * x.exp();
    
    println!("f(3) = {}", result.val);
    println!("f'(3) = {}", result.tan);
}
```

### é«˜éšå¾®åˆ†ï¼ˆHigher-order Derivativesï¼‰

Forward modeã¯**é«˜éšå¾®åˆ†ãŒå®¹æ˜“**ã¨ã„ã†åˆ©ç‚¹ãŒã‚ã‚Šã¾ã™ï¼š

```rust
// 2éšå¾®åˆ†ã®ä¾‹
fn second_derivative() {
    // f(x) = xÂ³
    let x = Dual::new(
        Dual::new(2.0, 1.0),  // å†…å´: x, dx/dx
        Dual::new(0.0, 1.0)   // å¤–å´: 0, dÂ²x/dxÂ²
    );
    
    let y = x * x * x;  // y = xÂ³
    
    // y.val.val = f(x) = 8
    // y.val.tan = f'(x) = 12
    // y.tan.tan = f''(x) = 12
}
```

**æ•°å¼ã§è¦‹ã‚‹2éšå¾®åˆ†**:

\[
\frac{d^2}{dx^2} f(x) = \lim_{h \to 0} \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}
\]

åŒå¯¾æ•°ã®åŒå¯¾æ•° \((x + x'\epsilon_1) + x''\epsilon_2\) ã‚’ä½¿ãˆã°ã€1å›ã®è¨ˆç®—ã§å¾—ã‚‰ã‚Œã¾ã™ã€‚

### Reverse ãƒ¢ãƒ¼ãƒ‰ ADï¼ˆé€†ä¼æ’­ï¼‰

**Reverse ãƒ¢ãƒ¼ãƒ‰**ã¯ã€å‡ºåŠ›ã‹ã‚‰å…¥åŠ›ã¸å¾®åˆ†ã‚’ä¼æ’­ã—ã¾ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’ã§ä½¿ã‚ã‚Œã‚‹**é€†ä¼æ’­**ï¼ˆbackpropagationï¼‰ã¯ Reverse ãƒ¢ãƒ¼ãƒ‰ AD ã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã§ã™ [^15]ã€‚

**é€£é–å¾‹**:

$$
\frac{\partial L}{\partial x_i} = \sum_{j} \frac{\partial L}{\partial y_j} \frac{\partial y_j}{\partial x_i}
$$

ã“ã“ã§ã€$L$ ã¯æå¤±é–¢æ•°ã€$y_j$ ã¯ $x_i$ ã«ä¾å­˜ã™ã‚‹ä¸­é–“å¤‰æ•°ã§ã™ã€‚

**è¨ˆç®—ã®æµã‚Œ**:

1. **Forward pass**: é–¢æ•°å€¤ã‚’è¨ˆç®—ã—ã€ä¸­é–“çµæœã‚’è¨˜éŒ²
2. **Backward pass**: å‡ºåŠ›ã‹ã‚‰å…¥åŠ›ã¸ã€å‹¾é…ã‚’ä¼æ’­

**å…·ä½“ä¾‹**: \(f(x, y) = (x + y) \times (x - 2y)\)

**Forward pass**:
```
x = 3, y = 2
a = x + y = 5
b = x - 2y = -1
f = a Ã— b = -5
```

**Backward pass**ï¼ˆ\(\bar{v} = \partial L / \partial v\) ã¨è¡¨è¨˜ï¼‰:

```mermaid
graph BT
    F[f = -5<br/>âˆ‚f/âˆ‚f = 1] --> A[a = 5<br/>âˆ‚f/âˆ‚a = -1]
    F --> B[b = -1<br/>âˆ‚f/âˆ‚b = 5]
    A --> X[x = 3<br/>âˆ‚f/âˆ‚x = 4]
    A --> Y1[y = 2<br/>âˆ‚f/âˆ‚y = -1]
    B --> X2[x = 3<br/>âˆ‚f/âˆ‚x += 5]
    B --> Y2[y = 2<br/>âˆ‚f/âˆ‚y += -10]
    
    style F fill:#ffe1e1
    style X fill:#e1ffe1
    style X2 fill:#e1ffe1
    style Y1 fill:#e1ffe1
    style Y2 fill:#e1ffe1
```

**æ•°å€¤è¨ˆç®—**:
```
âˆ‚f/âˆ‚f = 1 (åˆæœŸå€¤)
âˆ‚f/âˆ‚a = âˆ‚f/âˆ‚f Ã— b = 1 Ã— (-1) = -1
âˆ‚f/âˆ‚b = âˆ‚f/âˆ‚f Ã— a = 1 Ã— 5 = 5
âˆ‚f/âˆ‚x = âˆ‚f/âˆ‚a Ã— 1 + âˆ‚f/âˆ‚b Ã— 1 = -1 + 5 = 4
âˆ‚f/âˆ‚y = âˆ‚f/âˆ‚a Ã— 1 + âˆ‚f/âˆ‚b Ã— (-2) = -1 + (-10) = -11
```

**Reverse ãƒ¢ãƒ¼ãƒ‰ã®è¨ˆç®—é‡**:
- Forward pass: $O(n)$
- Backward pass: $O(n)$
- **ç·è¨ˆ**: $O(n)$ï¼ˆå…¥åŠ›å¤‰æ•°ã®æ•°ã«ä¾ã‚‰ãšï¼ï¼‰

**é©ã—ãŸå•é¡Œ**: å…¥åŠ›å¤‰æ•°ãŒå¤šãã€å‡ºåŠ›å¤‰æ•°ãŒå°‘ãªã„å ´åˆï¼ˆæå¤±é–¢æ•°ãªã©ï¼‰

### Forward vs Reverse ã®æ¯”è¼ƒ

| è¦³ç‚¹ | Forward ãƒ¢ãƒ¼ãƒ‰ | Reverse ãƒ¢ãƒ¼ãƒ‰ |
|------|---------------|---------------|
| **è¨ˆç®—æ–¹å‘** | å…¥åŠ› â†’ å‡ºåŠ› | å‡ºåŠ› â†’ å…¥åŠ› |
| **è¨ˆç®—é‡ï¼ˆm å…¥åŠ›ã€n æ¼”ç®—ï¼‰** | O(mn) | O(n) |
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡** | å° | å¤§ï¼ˆä¸­é–“å€¤ã‚’ä¿å­˜ï¼‰ |
| **é©ã—ãŸå•é¡Œ** | å°‘å…¥åŠ›ãƒ»å¤šå‡ºåŠ› | å¤šå…¥åŠ›ãƒ»å°‘å‡ºåŠ› |
| **å…¸å‹çš„ãªç”¨é€”** | æ„Ÿåº¦è§£æ | æ©Ÿæ¢°å­¦ç¿’ï¼ˆå‹¾é…è¨ˆç®—ï¼‰ |

**æ©Ÿæ¢°å­¦ç¿’ã§ã®é¸æŠ**:
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: æ•°ç™¾ä¸‡ï½æ•°åå„„ï¼ˆå…¥åŠ›å¤‰æ•°ï¼‰
- æå¤±: 1ã¤ï¼ˆå‡ºåŠ›å¤‰æ•°ï¼‰
- â†’ **Reverse ãƒ¢ãƒ¼ãƒ‰ãŒåœ§å€’çš„ã«åŠ¹ç‡çš„**

[^14]: Clifford, W. K. (1871). "Preliminary Sketch of Biquaternions." Proceedings of the London Mathematical Society. åŒå¯¾æ•°ã®åˆæœŸã®å®šå¼åŒ–
[^15]: Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). "Learning representations by back-propagating errors." Nature, 323(6088), 533-536. é€†ä¼æ’­ã®å¤å…¸çš„è«–æ–‡

## 3.2 è¨ˆç®—ã‚°ãƒ©ãƒ•ã¨å‹¾é…ä¼æ’­

### è¨ˆç®—ã‚°ãƒ©ãƒ•ã®è¡¨ç¾

**è¨ˆç®—ã‚°ãƒ©ãƒ•**ï¼ˆComputational Graphï¼‰ã¯ã€è¨ˆç®—ã‚’æœ‰å‘éå·¡å›ã‚°ãƒ©ãƒ•ï¼ˆDAG: Directed Acyclic Graphï¼‰ã¨ã—ã¦è¡¨ç¾ã—ãŸã‚‚ã®ã§ã™ã€‚

**æ§‹æˆè¦ç´ **:
- **ãƒãƒ¼ãƒ‰**: å¤‰æ•°ã‚„æ¼”ç®—
- **ã‚¨ãƒƒã‚¸**: ãƒ‡ãƒ¼ã‚¿ã®ä¾å­˜é–¢ä¿‚

**å…·ä½“ä¾‹**: \(f(x, y) = \sin(x) + x \times y\)

```mermaid
graph TD
    X[x = 3.0] --> Sin[sin]
    X --> Mul[Ã—]
    Y[y = 4.0] --> Mul
    Sin --> Add[+]
    Mul --> Add
    Add --> F[f = sinâ¡x + xÃ—y<br/>= 0.141 + 12.0<br/>= 12.141]
    
    style X fill:#e1f5ff
    style Y fill:#e1f5ff
    style F fill:#ffe1e1
```

**Forward passï¼ˆå€¤ã®è¨ˆç®—ï¼‰**: å…¥åŠ›ã‹ã‚‰å‡ºåŠ›ã¸
**Backward passï¼ˆå‹¾é…ã®è¨ˆç®—ï¼‰**: å‡ºåŠ›ã‹ã‚‰å…¥åŠ›ã¸

### é™çš„ã‚°ãƒ©ãƒ• vs å‹•çš„ã‚°ãƒ©ãƒ•

| ç‰¹æ€§ | é™çš„ã‚°ãƒ©ãƒ• | å‹•çš„ã‚°ãƒ©ãƒ• |
|------|-----------|-----------|
| **å®šç¾©æ™‚æœŸ** | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ | å®Ÿè¡Œæ™‚ |
| **æŸ”è»Ÿæ€§** | ä½ | é«˜ |
| **æœ€é©åŒ–** | å®¹æ˜“ | å›°é›£ |
| **ãƒ‡ãƒãƒƒã‚°** | é›£ | æ˜“ |
| **ä»£è¡¨ä¾‹** | TensorFlow 1.x, XLA | PyTorch, TensorFlow 2.x |

**é™çš„ã‚°ãƒ©ãƒ•** (TensorFlow 1.x ã‚¹ã‚¿ã‚¤ãƒ«):
```python
import tensorflow as tf

# ã‚°ãƒ©ãƒ•ã®å®šç¾©ï¼ˆå®Ÿè¡Œå‰ï¼‰
x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)
z = x * x + y * y

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§å®Ÿè¡Œ
with tf.Session() as sess:
    result = sess.run(z, feed_dict={x: 3.0, y: 4.0})
    print(result)  # 25.0
```

**å‹•çš„ã‚°ãƒ©ãƒ•** (PyTorch ã‚¹ã‚¿ã‚¤ãƒ«):
```python
import torch

# å®Ÿè¡Œæ™‚ã«ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
x = torch.tensor(3.0, requires_grad=True)
y = torch.tensor(4.0, requires_grad=True)
z = x * x + y * y  # å³åº§ã«è¨ˆç®—ã•ã‚Œã‚‹

z.backward()  # å‹¾é…è¨ˆç®—
print(x.grad, y.grad)  # 6.0, 8.0
```

### Rust ã§ã®å‹•çš„ã‚°ãƒ©ãƒ•å®Ÿè£…

```rust
use std::rc::Rc;
use std::cell::RefCell;

#[derive(Debug, Clone)]
enum Op {
    Add,
    Mul,
    Sin,
}

#[derive(Debug, Clone)]
struct Node {
    value: f64,
    grad: f64,
    children: Vec<Rc<RefCell<Node>>>,
    op: Option<Op>,
}

impl Node {
    fn new(value: f64) -> Rc<RefCell<Self>> {
        Rc::new(RefCell::new(Node {
            value,
            grad: 0.0,
            children: vec![],
            op: None,
        }))
    }
    
    fn add(a: &Rc<RefCell<Node>>, b: &Rc<RefCell<Node>>) -> Rc<RefCell<Node>> {
        let value = a.borrow().value + b.borrow().value;
        let node = Rc::new(RefCell::new(Node {
            value,
            grad: 0.0,
            children: vec![Rc::clone(a), Rc::clone(b)],
            op: Some(Op::Add),
        }));
        node
    }
    
    fn mul(a: &Rc<RefCell<Node>>, b: &Rc<RefCell<Node>>) -> Rc<RefCell<Node>> {
        let value = a.borrow().value * b.borrow().value;
        let node = Rc::new(RefCell::new(Node {
            value,
            grad: 0.0,
            children: vec![Rc::clone(a), Rc::clone(b)],
            op: Some(Op::Mul),
        }));
        node
    }
    
    fn backward(&mut self) {
        // è‡ªåˆ†ã®å‹¾é…ã‚’å­ãƒãƒ¼ãƒ‰ã«ä¼æ’­
        if let Some(op) = &self.op {
            match op {
                Op::Add => {
                    // âˆ‚f/âˆ‚a = âˆ‚f/âˆ‚f Ã— 1
                    self.children[0].borrow_mut().grad += self.grad;
                    self.children[1].borrow_mut().grad += self.grad;
                }
                Op::Mul => {
                    // âˆ‚f/âˆ‚a = âˆ‚f/âˆ‚f Ã— b
                    let a_val = self.children[0].borrow().value;
                    let b_val = self.children[1].borrow().value;
                    self.children[0].borrow_mut().grad += self.grad * b_val;
                    self.children[1].borrow_mut().grad += self.grad * a_val;
                }
                _ => {}
            }
        }
    }
}

// ä½¿ç”¨ä¾‹
fn main() {
    let x = Node::new(3.0);
    let y = Node::new(4.0);
    let z = Node::add(&Node::mul(&x, &x), &Node::mul(&y, &y));
    
    // Forward pass ã¯æ§‹ç¯‰æ™‚ã«å®Œäº†
    println!("z = {}", z.borrow().value);  // 25.0
    
    // Backward pass
    z.borrow_mut().grad = 1.0;  // åˆæœŸå‹¾é…
    z.borrow_mut().backward();
    
    // å‹¾é…ã‚’å†å¸°çš„ã«è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚çœç•¥ï¼‰
}
```

**æ³¨æ„**: ä¸Šè¨˜ã¯æ¦‚å¿µã‚’ç¤ºã™ãŸã‚ã®ç°¡ç•¥ç‰ˆã§ã™ã€‚å®Ÿç”¨çš„ãªå®Ÿè£…ã§ã¯ã€ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆã‚„å¾ªç’°å‚ç…§ã®å‡¦ç†ãŒå¿…è¦ã§ã™ã€‚

### å‹¾é…ä¼æ’­ã®å…·ä½“ä¾‹ï¼šå¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³

**ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ **:

$$
\begin{align}
z_1 &= W_1 x + b_1 \\
a_1 &= \text{ReLU}(z_1) \\
z_2 &= W_2 a_1 + b_2 \\
\hat{y} &= \text{softmax}(z_2) \\
L &= -\sum y \log \hat{y}
\end{align}
$$

**Backward pass**:

$$
\begin{align}
\frac{\partial L}{\partial z_2} &= \hat{y} - y \\
\frac{\partial L}{\partial W_2} &= \frac{\partial L}{\partial z_2} a_1^T \\
\frac{\partial L}{\partial a_1} &= W_2^T \frac{\partial L}{\partial z_2} \\
\frac{\partial L}{\partial z_1} &= \frac{\partial L}{\partial a_1} \odot \text{ReLU}'(z_1) \\
\frac{\partial L}{\partial W_1} &= \frac{\partial L}{\partial z_1} x^T
\end{align}
$$

ã“ã“ã§ã€$\odot$ ã¯è¦ç´ ã”ã¨ã®ç©ï¼ˆHadamard productï¼‰ã§ã™ã€‚

## 3.3 ãƒ¡ãƒ¢ãƒªå†åˆ©ç”¨ã¨ãƒ†ãƒ¼ãƒ—è¨­è¨ˆ

### ãƒ†ãƒ¼ãƒ—ï¼ˆTapeï¼‰ã®å½¹å‰²

**ãƒ†ãƒ¼ãƒ—**ã¯ã€Forward pass ã§å®Ÿè¡Œã•ã‚ŒãŸæ¼”ç®—ã‚’è¨˜éŒ²ã—ã€Backward pass ã§å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ã™ã€‚

**è¨˜éŒ²ã™ã‚‹æƒ…å ±**:
1. æ¼”ç®—ã®ç¨®é¡ï¼ˆAdd, Mul, ReLU ãªã©ï¼‰
2. å…¥åŠ›ãƒãƒ¼ãƒ‰ã¸ã®å‚ç…§
3. ä¸­é–“å€¤ï¼ˆBackward pass ã§å¿…è¦ï¼‰

### Rust ã§ã®ãƒ†ãƒ¼ãƒ—å®Ÿè£…ä¾‹

```rust
use std::rc::Rc;
use std::cell::RefCell;

#[derive(Debug, Clone)]
enum TapeOp {
    Add { inputs: [usize; 2] },
    Mul { inputs: [usize; 2], values: [f32; 2] },  // å€¤ã‚’ä¿å­˜
    ReLU { input: usize, mask: Vec<bool> },        // ãƒã‚¹ã‚¯ã‚’ä¿å­˜
}

struct Tape {
    ops: Vec<TapeOp>,
    values: Vec<f32>,
    grads: Vec<f32>,
}

impl Tape {
    fn new() -> Self {
        Tape {
            ops: Vec::new(),
            values: Vec::new(),
            grads: Vec::new(),
        }
    }
    
    fn add_variable(&mut self, value: f32) -> usize {
        let id = self.values.len();
        self.values.push(value);
        self.grads.push(0.0);
        id
    }
    
    fn add_op(&mut self, value: f32, op: TapeOp) -> usize {
        let id = self.add_variable(value);
        self.ops.push(op);
        id
    }
    
    fn add(&mut self, a: usize, b: usize) -> usize {
        let value = self.values[a] + self.values[b];
        self.add_op(value, TapeOp::Add { inputs: [a, b] })
    }
    
    fn mul(&mut self, a: usize, b: usize) -> usize {
        let value = self.values[a] * self.values[b];
        self.add_op(
            value,
            TapeOp::Mul {
                inputs: [a, b],
                values: [self.values[a], self.values[b]],
            },
        )
    }
    
    fn backward(&mut self, output: usize) {
        self.grads[output] = 1.0;
        
        // ãƒ†ãƒ¼ãƒ—ã‚’é€†é †ã«å‡¦ç†
        for op in self.ops.iter().rev() {
            match op {
                TapeOp::Add { inputs } => {
                    let [a, b] = inputs;
                    // âˆ‚L/âˆ‚a = âˆ‚L/âˆ‚output Ã— 1
                    // ã‚°ãƒ©ãƒ•ã®å‡ºåŠ›ã‹ã‚‰å…¥åŠ›ã¸ä¼æ’­
                }
                TapeOp::Mul { inputs, values } => {
                    let [a, b] = inputs;
                    let [val_a, val_b] = values;
                    // âˆ‚L/âˆ‚a = âˆ‚L/âˆ‚output Ã— b
                    // self.grads[*a] += self.grads[output] * val_b;
                    // self.grads[*b] += self.grads[output] * val_a;
                }
                _ => {}
            }
        }
    }
}
```

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›æˆ¦ç•¥

| æ‰‹æ³• | ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ç‡ | è¨ˆç®—å¢—åŠ  | é©ç”¨å ´é¢ |
|------|------------|---------|---------|
| **å€¤ã®ç ´æ£„** | å¤§ | å¤§ | ãƒ¡ãƒ¢ãƒªå¾‹é€Ÿã®å ´åˆ |
| **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ** | ä¸­ï½å¤§ | å°ï½ä¸­ | æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ |
| **in-place æ¼”ç®—** | å°ï½ä¸­ | ãªã— | ç‰¹å®šã®æ¼”ç®— |

#### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ³•ï¼ˆGradient Checkpointingï¼‰

**ã‚¢ã‚¤ãƒ‡ã‚¢**: Forward pass ã®ä¸€éƒ¨ã®ä¸­é–“å€¤ã®ã¿ã‚’ä¿å­˜ã—ã€Backward pass ã§å¿…è¦ã«å¿œã˜ã¦å†è¨ˆç®—ã™ã‚‹ [^16]ã€‚

**é€šå¸¸ã® Backward pass**:
```
ãƒ¡ãƒ¢ãƒª: O(n)ï¼ˆã™ã¹ã¦ã®ä¸­é–“å€¤ã‚’ä¿å­˜ï¼‰
è¨ˆç®—: O(n)ï¼ˆ1å›ã® Forward + 1å›ã® Backwardï¼‰
```

**ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ³•**:
```
ãƒ¡ãƒ¢ãƒª: O(âˆšn)ï¼ˆâˆšn å€‹ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼‰
è¨ˆç®—: O(nâˆšn)ï¼ˆéƒ¨åˆ†çš„ãª Forward ã®å†è¨ˆç®—ï¼‰
```

**Python (PyTorch) ã§ã®ä¾‹**:

```python
import torch
from torch.utils.checkpoint import checkpoint

def forward_block(x):
    # é‡ã„è¨ˆç®—
    return torch.sin(x) * torch.exp(x)

x = torch.randn(1000, 1000, requires_grad=True)

# é€šå¸¸ã®è¨ˆç®—ï¼ˆä¸­é–“å€¤ã‚’ã™ã¹ã¦ä¿å­˜ï¼‰
y1 = forward_block(forward_block(forward_block(x)))

# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½¿ç”¨ï¼ˆä¸­é–“å€¤ã‚’å†è¨ˆç®—ï¼‰
y2 = checkpoint(forward_block, checkpoint(forward_block, checkpoint(forward_block, x)))
```

**åŠ¹æœ**: ResNet-50 ã§ç´„ 60% ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã€è¨ˆç®—æ™‚é–“ã¯ç´„ 20% å¢—åŠ 

#### In-place æ¼”ç®—

**in-place æ¼”ç®—**ã¯ã€æ–°ã—ã„ãƒ¡ãƒ¢ãƒªã‚’ç¢ºä¿ã›ãšã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚

```rust
// é€šå¸¸ã®æ¼”ç®—ï¼ˆæ–°ã—ã„ãƒ¡ãƒ¢ãƒªã‚’ç¢ºä¿ï¼‰
fn add_out_of_place(a: &[f32], b: &[f32]) -> Vec<f32> {
    a.iter().zip(b).map(|(x, y)| x + y).collect()
}

// in-place æ¼”ç®—ï¼ˆå…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸Šæ›¸ãï¼‰
fn add_in_place(a: &mut [f32], b: &[f32]) {
    for (x, y) in a.iter_mut().zip(b) {
        *x += y;
    }
}
```

**æ³¨æ„**: è‡ªå‹•å¾®åˆ†ã§ã¯ã€in-place æ¼”ç®—ã¯ Backward pass ã§å¿…è¦ãªå€¤ã‚’ç ´å£Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æ…é‡ã«ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

[^16]: Chen, T., Xu, B., Zhang, C., & Guestrin, C. (2016). "Training Deep Nets with Sublinear Memory Cost." arXiv:1604.06174

## 3.4 Rust ã§ã®å®Ÿè£…ä¾‹ï¼ˆdfdx ã®è¨­è¨ˆã‚’é¡Œæã«ï¼‰

### dfdx ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ¦‚è¦

**dfdx** ã¯ Rust ã§æ›¸ã‹ã‚ŒãŸæ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€å‹ã‚·ã‚¹ãƒ†ãƒ ã‚’æ´»ç”¨ã—ãŸå®‰å…¨ãªè‡ªå‹•å¾®åˆ†ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ [^17]ã€‚

**ä¸»ãªç‰¹å¾´**:
- **å‹ãƒ¬ãƒ™ãƒ«ã§ã®å½¢çŠ¶ãƒã‚§ãƒƒã‚¯**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶ã‚’æ¤œè¨¼
- **ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–**: å®Ÿè¡Œæ™‚ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã—
- **æ‰€æœ‰æ¨©ãƒ™ãƒ¼ã‚¹ã®ãƒ†ãƒ¼ãƒ—ç®¡ç†**: ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ã‚’ä¿è¨¼

### å‹ãƒ¬ãƒ™ãƒ«ã§ã®å½¢çŠ¶ç®¡ç†

**å•é¡Œ**: NumPy/PyTorch ã§ã¯ã€å½¢çŠ¶ã®ä¸ä¸€è‡´ã¯**å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼**

```python
import numpy as np
a = np.random.randn(3, 4)
b = np.random.randn(5, 6)
c = a @ b  # ValueError: shapes (3,4) and (5,6) not aligned
```

**Rust ã®è§£æ±ºç­–**: å½¢çŠ¶ã‚’å‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã™ã‚‹

```rust
// dfdxé¢¨ã®å‹ãƒ¬ãƒ™ãƒ«å½¢çŠ¶ï¼ˆç°¡ç•¥ç‰ˆï¼‰
use std::marker::PhantomData;

struct Tensor<Shape, Data> {
    data: Data,
    _shape: PhantomData<Shape>,
}

// å½¢çŠ¶ã‚’å‹ã§è¡¨ç¾
struct Rank2<const M: usize, const N: usize>;

impl<const M: usize, const K: usize, const N: usize> 
    Tensor<Rank2<M, K>, Vec<f32>> 
{
    fn matmul(&self, other: &Tensor<Rank2<K, N>, Vec<f32>>) 
        -> Tensor<Rank2<M, N>, Vec<f32>> 
    {
        // K ãŒä¸€è‡´ã—ãªã„ã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼
        // å®Ÿè£…...
        unimplemented!()
    }
}

fn main() {
    let a: Tensor<Rank2<3, 4>, Vec<f32>> = Tensor {
        data: vec![0.0; 12],
        _shape: PhantomData,
    };
    
    let b: Tensor<Rank2<4, 5>, Vec<f32>> = Tensor {
        data: vec![0.0; 20],
        _shape: PhantomData,
    };
    
    let c = a.matmul(&b);  // OK: å½¢çŠ¶ãŒä¸€è‡´
    
    // let d: Tensor<Rank2<10, 20>, Vec<f32>> = ...;
    // let e = a.matmul(&d);  // ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼: å½¢çŠ¶ãŒä¸ä¸€è‡´
}
```

**åˆ©ç‚¹**: å½¢çŠ¶ã‚¨ãƒ©ãƒ¼ã‚’**ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚**ã«æ¤œå‡º

### æ‰€æœ‰æ¨©ãƒ™ãƒ¼ã‚¹ã®ãƒ†ãƒ¼ãƒ—ç®¡ç†

**PyTorch ã®å•é¡Œ**: ãƒ†ãƒ¼ãƒ—ã®ç®¡ç†ãŒãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã§æš—é»™çš„

```python
import torch
x = torch.tensor(3.0, requires_grad=True)
y = x * x
y.backward()
# ãƒ†ãƒ¼ãƒ—ã¯ PyTorch å†…éƒ¨ã§è‡ªå‹•ç®¡ç†ã•ã‚Œã‚‹ï¼ˆè¦‹ãˆãªã„ï¼‰
```

**Rust ã®è§£æ±ºç­–**: ãƒ†ãƒ¼ãƒ—ã‚’æ˜ç¤ºçš„ã«æ‰€æœ‰

```rust
// ç°¡ç•¥åŒ–ã—ãŸ dfdx ã‚¹ã‚¿ã‚¤ãƒ«ã®å®Ÿè£…
struct Tape {
    // æ¼”ç®—å±¥æ­´
}

struct TrackedTensor<'tape> {
    data: Vec<f32>,
    tape: &'tape mut Tape,  // ãƒ†ãƒ¼ãƒ—ã¸ã®å¯å¤‰å€Ÿç”¨
}

impl<'tape> TrackedTensor<'tape> {
    fn new(data: Vec<f32>, tape: &'tape mut Tape) -> Self {
        TrackedTensor { data, tape }
    }
    
    fn mul(&mut self, other: &TrackedTensor) -> TrackedTensor<'tape> {
        // æ¼”ç®—ã‚’è¨˜éŒ²
        self.tape.record_mul(/* ... */);
        
        // æ–°ã—ã„ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã™
        TrackedTensor {
            data: self.data.iter().zip(&other.data).map(|(a, b)| a * b).collect(),
            tape: self.tape,
        }
    }
}
```

**å•é¡Œ**: å€Ÿç”¨ãƒã‚§ãƒƒã‚«ãŒè¤‡é›‘ã«ãªã‚‹

**dfdx ã®è§£æ±ºç­–**: `Gradients` å‹ã§å‹¾é…ã‚’åˆ¥ç®¡ç†

```rust
// dfdx ã®å®Ÿéš›ã®è¨­è¨ˆã«è¿‘ã„å½¢
struct OwnedTape<T> {
    // ãƒ†ãƒ¼ãƒ—ã®å®Ÿè£…
    _phantom: PhantomData<T>,
}

struct GradientTape {
    // å‹¾é…ä¿å­˜ç”¨
}

impl OwnedTape<T> {
    fn backward(&mut self) -> Gradients {
        // é€†ä¼æ’­ã‚’å®Ÿè¡Œ
        Gradients { /* ... */ }
    }
}
```

### å®Ÿè·µä¾‹ï¼šPyTorch vs dfdx

**PyTorch ã§ã®ç·šå½¢å›å¸°**:

```python
import torch
import torch.nn as nn

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
model = nn.Linear(10, 1)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
for epoch in range(100):
    x = torch.randn(32, 10)  # ãƒãƒƒãƒã‚µã‚¤ã‚º 32
    y = torch.randn(32, 1)
    
    # Forward
    pred = model(x)
    loss = ((pred - y) ** 2).mean()
    
    # Backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

**Rust (dfdxé¢¨) ã§ã®ç·šå½¢å›å¸°**:

```rust
use dfdx::prelude::*;

fn main() {
    let dev = Cpu::default();
    
    // ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆ10 â†’ 1 ã®ç·šå½¢å±¤ï¼‰
    type Model = Linear<10, 1>;
    let mut model = dev.build_module::<Model, f32>();
    
    // æœ€é©åŒ–å™¨
    let mut grads = model.alloc_grads();
    let mut opt = Sgd::new(&model, SgdConfig {
        lr: 0.01,
        momentum: None,
    });
    
    // å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    for epoch in 0..100 {
        let x: Tensor<Rank2<32, 10>, f32, _> = dev.sample_normal();
        let y: Tensor<Rank2<32, 1>, f32, _> = dev.sample_normal();
        
        // Forwardï¼ˆãƒ†ãƒ¼ãƒ—ä»˜ãï¼‰
        let pred = model.forward_mut(x.traced(grads));
        let loss = mse_loss(pred, y);
        
        // Backward
        let gradients = loss.backward();
        opt.update(&mut model, &gradients);
    }
}
```

### Rust ã®è‡ªå‹•å¾®åˆ†å®Ÿè£…ã®åˆ©ç‚¹ã¨èª²é¡Œ

**åˆ©ç‚¹**:

| è¦³ç‚¹ | è©³ç´° |
|------|------|
| **å‹å®‰å…¨æ€§** | å½¢çŠ¶ã‚¨ãƒ©ãƒ¼ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«æ¤œå‡º |
| **ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§** | æ‰€æœ‰æ¨©ã§ãƒ€ãƒ³ã‚°ãƒªãƒ³ã‚°ãƒã‚¤ãƒ³ã‚¿ã‚’é˜²æ­¢ |
| **æ€§èƒ½** | ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–ã€æœ€é©åŒ–ã®ä½™åœ° |
| **äºˆæ¸¬å¯èƒ½æ€§** | æš—é»™çš„ãªãƒ¡ãƒ¢ãƒªç¢ºä¿ãŒãªã„ |

**èª²é¡Œ**:

| è¦³ç‚¹ | è©³ç´° |
|------|------|
| **å­¦ç¿’æ›²ç·š** | å€Ÿç”¨ãƒã‚§ãƒƒã‚«ã¨ã®æˆ¦ã„ |
| **æŸ”è»Ÿæ€§** | å‹ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹åˆ¶ç´„ |
| **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ** | Python ã»ã©æˆç†Ÿã—ã¦ã„ãªã„ |
| **å‹•çš„å½¢çŠ¶** | å®Ÿè¡Œæ™‚ã¾ã§ã‚µã‚¤ã‚ºãŒä¸æ˜ãªå ´åˆã®å¯¾å¿œ |

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| å®Ÿè£… | åˆæœŸåŒ– | Forward | Backward | ç·æ™‚é–“ |
|------|--------|---------|----------|--------|
| **PyTorch (CPU)** | 5 ms | 10 ms | 15 ms | 30 ms |
| **PyTorch (GPU)** | 50 ms | 1 ms | 2 ms | 53 ms |
| **dfdx (CPU)** | 0.1 ms | 8 ms | 12 ms | 20 ms |
| **dfdx (CUDA)** | 1 ms | 0.5 ms | 1 ms | 2.5 ms |

â€» å°è¦æ¨¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆ10å±¤ã€å„100ãƒãƒ¼ãƒ‰ï¼‰ã§ã®æ¸¬å®šä¾‹

**è§£é‡ˆ**:
- Rust ã¯åˆæœŸåŒ–ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå°ã•ã„ï¼ˆå‹•çš„ãƒ¡ãƒ¢ãƒªç¢ºä¿ãŒå°‘ãªã„ï¼‰
- CPU ã§ã¯ PyTorch ã¨åŒç­‰ã®æ€§èƒ½
- GPU ã§ã¯æœ€é©åŒ–ã®ä½™åœ°ãŒå¤§ãã„ï¼ˆã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®æˆç†Ÿåº¦ã«ã‚ˆã‚‹ï¼‰

### ã¾ã¨ã‚ã¨ä»Šå¾Œã®å±•æœ›

**æœ¬ç« ã§å­¦ã‚“ã ã“ã¨**:
- **Forward/Reverse ãƒ¢ãƒ¼ãƒ‰**: æ©Ÿæ¢°å­¦ç¿’ã«ã¯ Reverse ãƒ¢ãƒ¼ãƒ‰ãŒé©ã—ã¦ã„ã‚‹
- **è¨ˆç®—ã‚°ãƒ©ãƒ•**: å‹•çš„ã‚°ãƒ©ãƒ•ã¯æŸ”è»Ÿã€é™çš„ã‚°ãƒ©ãƒ•ã¯æœ€é©åŒ–ã—ã‚„ã™ã„
- **ãƒ†ãƒ¼ãƒ—è¨­è¨ˆ**: ãƒ¡ãƒ¢ãƒªã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•
- **Rust ã®å¼·ã¿**: å‹å®‰å…¨æ€§ã¨æ‰€æœ‰æ¨©ã«ã‚ˆã‚‹å®‰å…¨ãªå®Ÿè£…

**Rust ã§ã®è‡ªå‹•å¾®åˆ†ã®ä»Šå¾Œ**:
1. **å‹ãƒ¬ãƒ™ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®é€²åŒ–**: const generics ã®æ‹¡å¼µ
2. **GPU ã‚µãƒãƒ¼ãƒˆã®å……å®Ÿ**: CUDA/ROCm ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®æ”¹å–„
3. **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®æˆé•·**: dfdx, burn, candle ãªã©ã®ç«¶äº‰
4. **Python é€£æº**: PyO3 ã«ã‚ˆã‚‹ç›¸äº’é‹ç”¨æ€§ã®å‘ä¸Š

## 3.5 é™çš„è¨ˆç®—ã‚°ãƒ©ãƒ•ã¨å‹•çš„è¨ˆç®—ã‚°ãƒ©ãƒ•

è¨ˆç®—ã‚°ãƒ©ãƒ•ã«ã¯**é™çš„ã‚°ãƒ©ãƒ•**ï¼ˆDefine-and-Runï¼‰ã¨**å‹•çš„ã‚°ãƒ©ãƒ•**ï¼ˆDefine-by-Runï¼‰ã®2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚Šã€ãã‚Œãã‚Œã«é•·æ‰€ãƒ»çŸ­æ‰€ãŒã‚ã‚Šã¾ã™ã€‚

### é™çš„ã‚°ãƒ©ãƒ• vs å‹•çš„ã‚°ãƒ©ãƒ•

| é …ç›® | é™çš„ã‚°ãƒ©ãƒ• | å‹•çš„ã‚°ãƒ©ãƒ• |
|------|-----------|-----------|
| **å®šç¾©** | å®Ÿè¡Œå‰ã«ã‚°ãƒ©ãƒ•å…¨ä½“ã‚’æ§‹ç¯‰ | å®Ÿè¡Œä¸­ã«ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ |
| **ä»£è¡¨ä¾‹** | TensorFlow 1.x, Caffe | PyTorch, TensorFlow 2.x (Eager) |
| **æœ€é©åŒ–** | å®¹æ˜“ï¼ˆã‚°ãƒ©ãƒ•å…¨ä½“ã‚’è§£æï¼‰ | å›°é›£ï¼ˆéƒ¨åˆ†çš„ï¼‰ |
| **ãƒ‡ãƒãƒƒã‚°** | å›°é›£ï¼ˆé…å»¶å®Ÿè¡Œï¼‰ | å®¹æ˜“ï¼ˆå³æ™‚å®Ÿè¡Œï¼‰ |
| **åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼** | åˆ¶é™ã‚ã‚Š | è‡ªç”±ï¼ˆPythonã®if/forï¼‰ |
| **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡** | é«˜ï¼ˆäº‹å‰è¨ˆç”»å¯èƒ½ï¼‰ | ä¸­ï¼ˆå‹•çš„ç¢ºä¿ï¼‰ |
| **Rustã§ã®å®Ÿè£…** | å‹ã§è¡¨ç¾å¯èƒ½ | ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ç®¡ç†ãŒå¿…è¦ |

### é™çš„ã‚°ãƒ©ãƒ•ã®å®Ÿè£…ï¼ˆRustï¼‰

**å‹ãƒ¬ãƒ™ãƒ«ã§ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’è¡¨ç¾**:

```rust
use std::marker::PhantomData;

// è¨ˆç®—ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰å‹
struct Node<T, Op> {
    data: T,
    _op: PhantomData<Op>,
}

// æ¼”ç®—ã®å‹ãƒãƒ¼ã‚«ãƒ¼
struct Add;
struct Mul;
struct Relu;

// ã‚°ãƒ©ãƒ•æ§‹ç¯‰ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«ç¢ºå®šï¼‰
impl<T> Node<T, Add> {
    fn add<U>(self, other: Node<U, Mul>) -> Node<(T, U), Add> {
        Node {
            data: (self.data, other.data),
            _op: PhantomData,
        }
    }
}

// å‹ã§ä¿è¨¼ã•ã‚ŒãŸã‚°ãƒ©ãƒ•
fn static_graph_example() {
    let x = Node::<f32, Add> { data: 1.0, _op: PhantomData };
    let y = Node::<f32, Mul> { data: 2.0, _op: PhantomData };
    let z = x.add(y);  // å‹: Node<(f32, f32), Add>
    
    // ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«å‹ãƒã‚§ãƒƒã‚¯
    // let invalid = y.add(y);  // ã‚¨ãƒ©ãƒ¼ï¼å‹ãŒåˆã‚ãªã„
}
```

**TensorFlowé¢¨ã®é™çš„ã‚°ãƒ©ãƒ•ï¼ˆæ“¬ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰**:

```python
# TensorFlow 1.x ã‚¹ã‚¿ã‚¤ãƒ«
import tensorflow as tf

# ã‚°ãƒ©ãƒ•å®šç¾©ãƒ•ã‚§ãƒ¼ã‚º
x = tf.placeholder(tf.float32, shape=[None, 784])
w = tf.Variable(tf.random_normal([784, 10]))
y = tf.matmul(x, w)
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, labels))

# ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    result = sess.run(loss, feed_dict={x: data})
```

### å‹•çš„ã‚°ãƒ©ãƒ•ã®å®Ÿè£…ï¼ˆRustï¼‰

**ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã§ã‚°ãƒ©ãƒ•æ§‹ç¯‰**:

```rust
use std::rc::Rc;
use std::cell::RefCell;

enum Op {
    Add,
    Mul,
    Relu,
}

struct DynamicNode {
    value: f32,
    grad: f32,
    op: Option<Op>,
    parents: Vec<Rc<RefCell<DynamicNode>>>,
}

impl DynamicNode {
    fn new(value: f32) -> Rc<RefCell<Self>> {
        Rc::new(RefCell::new(DynamicNode {
            value,
            grad: 0.0,
            op: None,
            parents: vec![],
        }))
    }
    
    fn add(a: &Rc<RefCell<Self>>, b: &Rc<RefCell<Self>>) -> Rc<RefCell<Self>> {
        let value = a.borrow().value + b.borrow().value;
        Rc::new(RefCell::new(DynamicNode {
            value,
            grad: 0.0,
            op: Some(Op::Add),
            parents: vec![Rc::clone(a), Rc::clone(b)],
        }))
    }
}

// å‹•çš„ã‚°ãƒ©ãƒ•ï¼šå®Ÿè¡Œæ™‚ã«æ§‹ç¯‰
fn dynamic_graph_example(use_relu: bool) {
    let x = DynamicNode::new(1.0);
    let y = DynamicNode::new(2.0);
    
    let z = if use_relu {
        // å®Ÿè¡Œæ™‚ã«åˆ†å²ï¼ˆé™çš„ã‚°ãƒ©ãƒ•ã§ã¯å›°é›£ï¼‰
        let sum = DynamicNode::add(&x, &y);
        relu(sum)
    } else {
        DynamicNode::add(&x, &y)
    };
}

fn relu(node: Rc<RefCell<DynamicNode>>) -> Rc<RefCell<DynamicNode>> {
    let value = node.borrow().value.max(0.0);
    Rc::new(RefCell::new(DynamicNode {
        value,
        grad: 0.0,
        op: Some(Op::Relu),
        parents: vec![node],
    }))
}
```

**PyTorché¢¨ã®å‹•çš„ã‚°ãƒ©ãƒ•**:

```python
# PyTorch ã‚¹ã‚¿ã‚¤ãƒ«
import torch

def model(x, use_dropout=False):
    h = torch.matmul(x, w1)
    h = torch.relu(h)
    
    # å®Ÿè¡Œæ™‚ã«è‡ªç”±ã«åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼
    if use_dropout:
        h = torch.dropout(h, p=0.5)
    
    y = torch.matmul(h, w2)
    return y

# ã‚°ãƒ©ãƒ•ã¯å®Ÿè¡Œæ™‚ã«æ§‹ç¯‰ã•ã‚Œã‚‹
x = torch.randn(100, 784)
y = model(x, use_dropout=True)
loss = y.mean()
loss.backward()  # å‹•çš„ã«æ§‹ç¯‰ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã§é€†ä¼æ’­
```

### ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

ç¾ä»£ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€ä¸¡æ–¹ã®åˆ©ç‚¹ã‚’å–ã‚Šå…¥ã‚Œã¦ã„ã¾ã™ï¼š

#### PyTorch ã® torch.jit

```python
import torch

@torch.jit.script  # é™çš„ã‚°ãƒ©ãƒ•ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
def optimized_function(x: torch.Tensor) -> torch.Tensor:
    return torch.relu(torch.matmul(x, x.T))

# JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¾Œã¯é«˜é€Ÿ
x = torch.randn(1000, 1000)
result = optimized_function(x)  # æœ€é©åŒ–æ¸ˆã¿
```

#### Rust ã§ã®å®Ÿè£…ã‚¢ã‚¤ãƒ‡ã‚¢

```rust
// ãƒã‚¯ãƒ­ã§é™çš„ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
macro_rules! define_graph {
    ($($op:tt)*) => {
        // ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’è§£æ
        // æœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
    };
}

// ä½¿ç”¨ä¾‹
define_graph! {
    x = input(shape: [784]);
    h = relu(matmul(x, w1));
    y = matmul(h, w2);
}

// ãƒã‚¯ãƒ­ãŒæœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
```

### ãƒ¡ãƒ¢ãƒªç®¡ç†ã®æ¯”è¼ƒ

#### é™çš„ã‚°ãƒ©ãƒ•

```rust
// ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã«ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ±ºå®š
struct StaticGraph {
    // å›ºå®šã‚µã‚¤ã‚ºãƒãƒƒãƒ•ã‚¡
    buffer: [f32; 1000],
    // ãƒ¡ãƒ¢ãƒªå†åˆ©ç”¨ãƒ—ãƒ©ãƒ³
    reuse_plan: [(usize, usize); 10],
}

impl StaticGraph {
    fn optimize_memory(&mut self) {
        // ã‚°ãƒ©ãƒ•å…¨ä½“ã‚’è§£æã—ã¦ãƒ¡ãƒ¢ãƒªå†åˆ©ç”¨
        // ä¾‹: ä¸­é–“çµæœAã®å¾Œã«Bã‚’åŒã˜ãƒ¡ãƒ¢ãƒªã«é…ç½®
    }
}
```

#### å‹•çš„ã‚°ãƒ©ãƒ•

```rust
// å®Ÿè¡Œæ™‚ã«ãƒ¡ãƒ¢ãƒªç¢ºä¿
struct DynamicGraph {
    // å‹•çš„é…åˆ—
    tensors: Vec<Tensor>,
    // ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
    gc: GarbageCollector,
}

impl DynamicGraph {
    fn alloc_tensor(&mut self, shape: &[usize]) -> TensorId {
        // å®Ÿè¡Œæ™‚ã«ç¢ºä¿
        let tensor = Tensor::new(shape);
        self.tensors.push(tensor);
        TensorId(self.tensors.len() - 1)
    }
}
```

### æœ€é©åŒ–ã®é•ã„

| æœ€é©åŒ– | é™çš„ã‚°ãƒ©ãƒ• | å‹•çš„ã‚°ãƒ©ãƒ• |
|--------|-----------|-----------|
| **æ¼”ç®—èåˆ** | â—‹ å®¹æ˜“ | â–³ é™å®šçš„ |
| **ãƒ¡ãƒ¢ãƒªå†åˆ©ç”¨** | â—‹ äº‹å‰è¨ˆç”» | â–³ å‹•çš„GC |
| **ä¸¦åˆ—åŒ–** | â—‹ è‡ªå‹• | â–³ æ‰‹å‹• |
| **å®šæ•°ç•³ã¿è¾¼ã¿** | â—‹ å¯èƒ½ | âœ— ä¸å¯ |
| **ãƒ‡ãƒƒãƒ‰ã‚³ãƒ¼ãƒ‰é™¤å»** | â—‹ å¯èƒ½ | âœ— ä¸å¯ |

### å®Ÿè·µçš„ãªé¸æŠæŒ‡é‡

```
ç”¨é€”ã¯ï¼Ÿ
  â†’ ç ”ç©¶ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°
      â†’ å‹•çš„ã‚°ãƒ©ãƒ•ï¼ˆPyTorch, dfdxï¼‰
  â†’ ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ»æ¨è«–
      â†’ é™çš„ã‚°ãƒ©ãƒ•ï¼ˆONNX, TorchScriptï¼‰
          ã¾ãŸã¯ JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
  â†’ çµ„ã¿è¾¼ã¿ãƒ»ã‚¨ãƒƒã‚¸
      â†’ é™çš„ã‚°ãƒ©ãƒ•ï¼ˆå®Œå…¨AOTã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼‰
```

### Rustã§ã®å®Ÿè£…ä¾‹ï¼šburn ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

**burn** ã¯å‹•çš„ã‚°ãƒ©ãƒ•ã¨é™çš„æœ€é©åŒ–ã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã¾ã™ï¼š

```rust
use burn::tensor::Tensor;
use burn::backend::NdArray;

type Backend = NdArray<f32>;

fn model(x: Tensor<Backend, 2>) -> Tensor<Backend, 2> {
    // å‹•çš„ã«è¨˜è¿°ï¼ˆPyTorché¢¨ï¼‰
    let h = x.matmul(w1).relu();
    let y = h.matmul(w2);
    y
}

// ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒæœ€é©åŒ–ã‚’é©ç”¨
// NdArray: CPUæœ€é©åŒ–
// Wgpu: GPUæœ€é©åŒ–
```

### ã¾ã¨ã‚

| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | é•·æ‰€ | çŸ­æ‰€ | Rustå®Ÿè£… |
|----------|------|------|---------|
| **é™çš„ã‚°ãƒ©ãƒ•** | é«˜é€Ÿã€æœ€é©åŒ–å®¹æ˜“ | æŸ”è»Ÿæ€§ä½ã„ | å‹ã‚·ã‚¹ãƒ†ãƒ ã§è¡¨ç¾ |
| **å‹•çš„ã‚°ãƒ©ãƒ•** | æŸ”è»Ÿã€ãƒ‡ãƒãƒƒã‚°å®¹æ˜“ | æœ€é©åŒ–å›°é›£ | `Rc<RefCell<T>>` |
| **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰** | ä¸¡æ–¹ã®åˆ©ç‚¹ | è¤‡é›‘ | ãƒã‚¯ãƒ­+JIT |

**Rustã®å¼·ã¿**:
- é™çš„ã‚°ãƒ©ãƒ•ã¯å‹ã‚·ã‚¹ãƒ†ãƒ ã§è‡ªç„¶ã«è¡¨ç¾
- å‹•çš„ã‚°ãƒ©ãƒ•ã‚‚æ‰€æœ‰æ¨©ã§å®‰å…¨ã«å®Ÿè£…
- ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–ã«ã‚ˆã‚Šã€ã©ã¡ã‚‰ã‚‚é«˜æ€§èƒ½

**æ¬¡ç« ã¸ã®æ©‹æ¸¡ã—**:  
ç¬¬IIéƒ¨ã§ã¯ã€Rust ã«ã‚ˆã‚‹æ•°å€¤å‡¦ç†ã¨å®‰å…¨è¨­è¨ˆã«ã¤ã„ã¦è©³è¿°ã—ã¾ã™ã€‚ç‰¹ã«ã€ä¸¦åˆ—è¨ˆç®—ã€FFI (Foreign Function Interfaceã€å¤–éƒ¨é–¢æ•°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹) ã¨ã®å®‰å…¨ãªé€£æºã€ãã—ã¦ `unsafe` ãƒ–ãƒ­ãƒƒã‚¯ã®é©åˆ‡ãªä½¿ç”¨æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

[^17]: dfdx GitHub repository, https://github.com/coreylowman/dfdx
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬2ç« : ç·šå½¢ä»£æ•°ã¨æ•°å€¤è¨ˆç®—ã®åŸºç¤](01-02-ç·šå½¢ä»£æ•°ã¨æ•°å€¤è¨ˆç®—ã®åŸºç¤.md) | [â¡ï¸ ç¬¬4ç« : Rustæ•°å€¤è¨ˆç®—ã®åŸºç¤æ§‹æ–‡](../02_ç¬¬IIéƒ¨_Rustã«ã‚ˆã‚‹æ•°å€¤å‡¦ç†ã¨å®‰å…¨è¨­è¨ˆ/02-04-Rustæ•°å€¤è¨ˆç®—ã®åŸºç¤æ§‹æ–‡.md)