[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬6ç« ](03-06-GPUã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç†è§£.md) | [â¡ï¸ ç¬¬8ç« ](03-08-GPUãƒ¡ãƒ¢ãƒªç®¡ç†ã¨æœ€é©åŒ–.md)

---

# ç¬¬ 7 ç« ã€€Rust ã‹ã‚‰ GPU ã‚’æ“ä½œã™ã‚‹

ã“ã®ç« ã§ã¯ã€Rustã‹ã‚‰GPUã‚’åˆ¶å¾¡ã™ã‚‹å„ç¨®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨APIã‚’å­¦ã³ã¾ã™ã€‚CUDAã€ROCmã€wgpuã®3ã¤ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’æ¯”è¼ƒã—ã€ç”¨é€”ã«å¿œã˜ãŸé¸æŠæ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚

**ç›®çš„**: Rustã§å®Ÿç”¨çš„ãªGPUãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ›¸ã‘ã‚‹ã‚ˆã†ã«ãªã‚Šã€Pythonï¼ˆCuPy/PyTorchï¼‰ã¨ã®é•ã„ã‚’ç†è§£ã—ã¾ã™ã€‚

## 7.1 CUDA ã¨ ROCm ã®åŸºæœ¬ API

### CUDA vs ROCm ã®æ¯”è¼ƒ

| é …ç›® | NVIDIA CUDA | AMD ROCm |
|------|------------|----------|
| å¯¾å¿œGPU | NVIDIA GPU | AMD GPU (ä¸€éƒ¨Intel) |
| è¨€èª | CUDA C/C++ | HIP C/C++ |
| ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  | cudarc, cust, cuda-rs | ãªã—ï¼ˆCUDAäº’æ›ï¼‰ |
| æˆç†Ÿåº¦ | é«˜ | ä¸­ |
| ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ  | è±Šå¯Œï¼ˆcuBLAS, cuDNNç­‰ï¼‰ | æˆé•·ä¸­ï¼ˆrocBLAS, MIOpenç­‰ï¼‰ |
| Rustã‚µãƒãƒ¼ãƒˆ | cudarcï¼ˆæ¨å¥¨ï¼‰ | é™å®šçš„ |

### Pythonï¼ˆCuPyï¼‰ã¨ã®æ¯”è¼ƒ

| æ“ä½œ | Python (CuPy) | Rust (cudarc) |
|------|--------------|--------------|
| åˆæœŸåŒ– | è‡ªå‹• | æ˜ç¤ºçš„ |
| ãƒ¡ãƒ¢ãƒªç¢ºä¿ | `cp.array()` | `device.alloc()` |
| ãƒ‡ãƒ¼ã‚¿è»¢é€ | æš—é»™çš„ | `htod_copy()`, `dtoh_copy()` |
| ã‚«ãƒ¼ãƒãƒ« | æ–‡å­—åˆ— or ãƒ•ã‚¡ã‚¤ãƒ« | PTXæ–‡å­—åˆ— or ãƒ•ã‚¡ã‚¤ãƒ« |
| ã‚¨ãƒ©ãƒ¼å‡¦ç† | ä¾‹å¤– | `Result<T, E>` |

### cudarc ã®åŸºæœ¬

**cudarc** [^1] ã¯ã€Rust-nativeãªCUDAãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã§ã€å®‰å…¨æ€§ã¨æ€§èƒ½ã‚’ä¸¡ç«‹ã—ã¾ã™ã€‚

[^1]: cudarc: https://github.com/coreylowman/cudarc

#### ãƒ‡ãƒã‚¤ã‚¹ã®åˆæœŸåŒ–

```rust
use cudarc::driver::*;

fn main() -> Result<(), CudaError> {
    // ãƒ‡ãƒã‚¤ã‚¹æ•°ã®å–å¾—
    let device_count = CudaDevice::count()?;
    println!("Found {} CUDA devices", device_count);
    
    // ãƒ‡ãƒã‚¤ã‚¹0ã‚’ä½¿ç”¨
    let device = CudaDevice::new(0)?;
    
    // ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±
    println!("Device: {}", device.name());
    println!("Compute Capability: {}.{}", 
             device.compute_cap().0, device.compute_cap().1);
    
    Ok(())
}
```

**Pythonï¼ˆCuPyï¼‰ç‰ˆ**:

```python
import cupy as cp

# ãƒ‡ãƒã‚¤ã‚¹æ•°
device_count = cp.cuda.runtime.getDeviceCount()
print(f"Found {device_count} CUDA devices")

# ãƒ‡ãƒã‚¤ã‚¹0ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
cp.cuda.Device(0).use()

# ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±
props = cp.cuda.Device().attributes
print(f"Device: {props['Name'].decode()}")
print(f"Compute Capability: {props['ComputeCapabilityMajor']}.{props['ComputeCapabilityMinor']}")
```

#### ãƒ¡ãƒ¢ãƒªæ“ä½œ

```rust
use cudarc::driver::*;

fn memory_operations() -> Result<(), CudaError> {
    let device = CudaDevice::new(0)?;
    
    // CPUå´ãƒ‡ãƒ¼ã‚¿
    let host_data: Vec<f32> = (0..1000).map(|i| i as f32).collect();
    
    // CPU â†’ GPUï¼ˆãƒ›ã‚¹ãƒˆãƒ»ãƒˆã‚¥ãƒ»ãƒ‡ãƒã‚¤ã‚¹ï¼‰
    let device_data = device.htod_copy(host_data.clone())?;
    
    // GPUä¸Šã§ãƒ¡ãƒ¢ãƒªç¢ºä¿
    let mut output = device.alloc_zeros::<f32>(1000)?;
    
    // ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œï¼ˆçœç•¥ï¼‰
    
    // GPU â†’ CPUï¼ˆãƒ‡ãƒã‚¤ã‚¹ãƒ»ãƒˆã‚¥ãƒ»ãƒ›ã‚¹ãƒˆï¼‰
    let result: Vec<f32> = device.dtoh_sync_copy(&output)?;
    
    Ok(())
}
```

**ãƒ¡ãƒ¢ãƒªè»¢é€ã®ã‚³ã‚¹ãƒˆ**:

| è»¢é€ | PCIe 3.0 x16 | PCIe 4.0 x16 | PCIe 5.0 x16 |
|------|-------------|-------------|-------------|
| ç†è«–å¸¯åŸŸ | 16 GB/s | 32 GB/s | 64 GB/s |
| å®ŸåŠ¹å¸¯åŸŸ | ~12 GB/s | ~25 GB/s | ~50 GB/s |

**100MBã®ãƒ‡ãƒ¼ã‚¿è»¢é€æ™‚é–“**:
- PCIe 3.0: ~8.3 ms
- PCIe 4.0: ~4.0 ms
- GPUè¨ˆç®—ï¼ˆ1 TFLOPSï¼‰: ~0.1 ms

â†’ **ãƒ‡ãƒ¼ã‚¿è»¢é€ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚Šã‚„ã™ã„**

#### ã‚«ãƒ¼ãƒãƒ«å®šç¾©ã¨èµ·å‹•

```rust
use cudarc::driver::*;
use cudarc::nvrtc::compile_ptx;

fn launch_kernel() -> Result<(), Box<dyn std::error::Error>> {
    let device = CudaDevice::new(0)?;
    
    // PTXã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    let ptx = compile_ptx(r#"
        extern "C" __global__ void saxpy(
            int n, float a, const float* x, float* y
        ) {
            int i = blockIdx.x * blockDim.x + threadIdx.x;
            if (i < n) {
                y[i] = a * x[i] + y[i];  // y = ax + y
            }
        }
    "#)?;
    
    device.load_ptx(ptx, "saxpy_module", &["saxpy"])?;
    
    // ãƒ‡ãƒ¼ã‚¿æº–å‚™
    let n = 1_000_000;
    let a = 2.0f32;
    let x = vec![1.0f32; n];
    let y = vec![3.0f32; n];
    
    let x_gpu = device.htod_copy(x)?;
    let mut y_gpu = device.htod_copy(y)?;
    
    // ã‚«ãƒ¼ãƒãƒ«èµ·å‹•
    let f = device.get_func("saxpy_module", "saxpy").unwrap();
    let cfg = LaunchConfig {
        grid_dim: ((n + 255) / 256, 1, 1),
        block_dim: (256, 1, 1),
        shared_mem_bytes: 0,
    };
    
    unsafe {
        f.launch(cfg, (n as i32, a, &x_gpu, &mut y_gpu))?;
    }
    
    // çµæœå–å¾—
    let result = device.dtoh_sync_copy(&y_gpu)?;
    println!("result[0] = {}", result[0]);  // 5.0 = 2*1 + 3
    
    Ok(())
}
```

**Pythonï¼ˆCuPyï¼‰ç‰ˆ**:

```python
import cupy as cp

# ã‚«ãƒ¼ãƒãƒ«å®šç¾©
saxpy_kernel = cp.RawKernel(r'''
extern "C" __global__
void saxpy(int n, float a, const float* x, float* y) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        y[i] = a * x[i] + y[i];
    }
}
''', 'saxpy')

n = 1000000
a = 2.0
x = cp.ones(n, dtype=cp.float32)
y = cp.ones(n, dtype=cp.float32) * 3

# ã‚«ãƒ¼ãƒãƒ«èµ·å‹•
grid = ((n + 255) // 256,)
block = (256,)
saxpy_kernel(grid, block, (n, a, x, y))

print(f"result[0] = {y[0]}")  # 5.0
```

## 7.2 cust/cudarc ã‚’ä½¿ã£ãŸã‚«ãƒ¼ãƒãƒ«å‘¼ã³å‡ºã—

### cudarc vs cust

| ç‰¹å¾´ | cudarc | cust |
|------|--------|------|
| è¨­è¨ˆæ€æƒ³ | ãƒŸãƒ‹ãƒãƒ«ã€Rustã‚‰ã—ã„ | é«˜ãƒ¬ãƒ™ãƒ«API |
| API | Driver API | Runtime API |
| å‹å®‰å…¨æ€§ | é«˜ | ä¸­ |
| ä½¿ã„ã‚„ã™ã• | ä¸­ | é«˜ |
| æ€§èƒ½ | æœ€é«˜ï¼ˆç›´æ¥åˆ¶å¾¡ï¼‰ | é«˜ |
| ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ | æ´»ç™º | åœæ»æ°—å‘³ |

### cudarc ã®é«˜åº¦ãªä½¿ç”¨ä¾‹

#### ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«ã‚ˆã‚‹ä¸¦è¡Œå®Ÿè¡Œ

```rust
use cudarc::driver::*;

fn concurrent_streams() -> Result<(), CudaError> {
    let device = CudaDevice::new(0)?;
    
    // è¤‡æ•°ã‚¹ãƒˆãƒªãƒ¼ãƒ ä½œæˆ
    let stream1 = device.fork_default_stream()?;
    let stream2 = device.fork_default_stream()?;
    
    // ãƒ‡ãƒ¼ã‚¿æº–å‚™
    let data1 = vec![1.0f32; 100000];
    let data2 = vec![2.0f32; 100000];
    
    let d1 = device.htod_copy(data1)?;
    let d2 = device.htod_copy(data2)?;
    
    // ä¸¦è¡Œå®Ÿè¡Œ
    unsafe {
        kernel.launch_on_stream(&stream1, cfg1, (&d1,))?;
        kernel.launch_on_stream(&stream2, cfg2, (&d2,))?;
    }
    
    // ä¸¡æ–¹ã®å®Œäº†ã‚’å¾…æ©Ÿ
    stream1.synchronize()?;
    stream2.synchronize()?;
    
    Ok(())
}
```

**Pythonï¼ˆPyTorchï¼‰ã§ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ **:

```python
import torch

stream1 = torch.cuda.Stream()
stream2 = torch.cuda.Stream()

with torch.cuda.stream(stream1):
    result1 = model1(input1)

with torch.cuda.stream(stream2):
    result2 = model2(input2)

torch.cuda.synchronize()  # å…¨ã‚¹ãƒˆãƒªãƒ¼ãƒ å¾…æ©Ÿ
```

#### ã‚¤ãƒ™ãƒ³ãƒˆã«ã‚ˆã‚‹è¨ˆæ¸¬

```rust
use cudarc::driver::*;

fn timing_with_events() -> Result<(), CudaError> {
    let device = CudaDevice::new(0)?;
    
    // ã‚¤ãƒ™ãƒ³ãƒˆä½œæˆ
    let start = device.create_event()?;
    let end = device.create_event()?;
    
    // è¨ˆæ¸¬é–‹å§‹
    device.record_event(&start)?;
    
    // ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œ
    unsafe {
        kernel.launch(cfg, args)?;
    }
    
    // è¨ˆæ¸¬çµ‚äº†
    device.record_event(&end)?;
    device.synchronize()?;
    
    // çµŒéæ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰
    let elapsed_ms = device.elapsed_millis(&start, &end)?;
    println!("Kernel time: {:.3} ms", elapsed_ms);
    
    Ok(())
}
```

## 7.3 wgpu ã«ã‚ˆã‚‹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ éä¾å­˜ GPU å®Ÿè£…

**wgpu** [^2] ã¯ã€WebGPUæ¨™æº–ã«åŸºã¥ãã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ GPU APIã§ã™ã€‚

[^2]: wgpu: https://wgpu.rs/

### wgpu ã®ç‰¹å¾´

| é …ç›® | CUDA (cudarc) | wgpu |
|------|--------------|------|
| å¯¾å¿œGPU | NVIDIA ã®ã¿ | NVIDIA, AMD, Intel, Apple |
| ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ | CUDA | Vulkan, Metal, DX12, WebGL |
| API | Driver API | WebGPUæ¨™æº– |
| Pythonå¯¾å¿œ | CuPy | wgpu-py |
| ãƒ–ãƒ©ã‚¦ã‚¶å®Ÿè¡Œ | ä¸å¯ | å¯èƒ½ï¼ˆWebAssemblyï¼‰ |
| æˆç†Ÿåº¦ | é«˜ | ä¸­ï¼ˆæ€¥æˆé•·ï¼‰ |

### wgpu ã®åŸºæœ¬ã‚³ãƒ¼ãƒ‰

```rust
use wgpu;

async fn run_wgpu() {
    // ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    let instance = wgpu::Instance::new(wgpu::InstanceDescriptor::default());
    
    // ã‚¢ãƒ€ãƒ—ã‚¿é¸æŠï¼ˆGPUé¸æŠï¼‰
    let adapter = instance.request_adapter(&wgpu::RequestAdapterOptions {
        power_preference: wgpu::PowerPreference::HighPerformance,
        ..Default::default()
    }).await.unwrap();
    
    println!("Using GPU: {}", adapter.get_info().name);
    
    // ãƒ‡ãƒã‚¤ã‚¹ã¨ã‚­ãƒ¥ãƒ¼å–å¾—
    let (device, queue) = adapter.request_device(
        &wgpu::DeviceDescriptor {
            label: None,
            required_features: wgpu::Features::empty(),
            required_limits: wgpu::Limits::default(),
        },
        None,
    ).await.unwrap();
    
    // ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆã‚·ã‚§ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ
    let shader = device.create_shader_module(wgpu::ShaderModuleDescriptor {
        label: Some("Compute shader"),
        source: wgpu::ShaderSource::Wgsl(r#"
            @group(0) @binding(0) var<storage, read> input: array<f32>;
            @group(0) @binding(1) var<storage, read_write> output: array<f32>;
            
            @compute @workgroup_size(256)
            fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
                let idx = global_id.x;
                output[idx] = input[idx] * 2.0;
            }
        "#.into()),
    });
    
    // ãƒãƒƒãƒ•ã‚¡ä½œæˆ
    let input_data: Vec<f32> = (0..1000).map(|i| i as f32).collect();
    let input_buffer = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
        label: Some("Input"),
        contents: bytemuck::cast_slice(&input_data),
        usage: wgpu::BufferUsages::STORAGE,
    });
    
    let output_buffer = device.create_buffer(&wgpu::BufferDescriptor {
        label: Some("Output"),
        size: (input_data.len() * 4) as u64,
        usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_SRC,
        mapped_at_creation: false,
    });
    
    // ãƒã‚¤ãƒ³ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆãƒªã‚½ãƒ¼ã‚¹ã®ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
    let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
        label: None,
        entries: &[
            wgpu::BindGroupLayoutEntry {
                binding: 0,
                visibility: wgpu::ShaderStages::COMPUTE,
                ty: wgpu::BindingType::Buffer {
                    ty: wgpu::BufferBindingType::Storage { read_only: true },
                    has_dynamic_offset: false,
                    min_binding_size: None,
                },
                count: None,
            },
            wgpu::BindGroupLayoutEntry {
                binding: 1,
                visibility: wgpu::ShaderStages::COMPUTE,
                ty: wgpu::BindingType::Buffer {
                    ty: wgpu::BufferBindingType::Storage { read_only: false },
                    has_dynamic_offset: false,
                    min_binding_size: None,
                },
                count: None,
            },
        ],
    });
    
    // ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
    let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
        label: None,
        layout: Some(&device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: None,
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        })),
        module: &shader,
        entry_point: "main",
    });
    
    // å®Ÿè¡Œ
    let mut encoder = device.create_command_encoder(&Default::default());
    {
        let mut cpass = encoder.begin_compute_pass(&Default::default());
        cpass.set_pipeline(&compute_pipeline);
        cpass.set_bind_group(0, &bind_group, &[]);
        cpass.dispatch_workgroups((input_data.len() as u32 + 255) / 256, 1, 1);
    }
    
    queue.submit(Some(encoder.finish()));
    device.poll(wgpu::Maintain::Wait);
}
```

### WGSL vs CUDA ã®æ¯”è¼ƒ

| ç‰¹å¾´ | CUDA C | WGSL (WebGPU Shading Language) |
|------|---------|-------------------------------|
| æ§‹æ–‡ | C/C++ | Rusté¢¨ |
| å‹ã‚·ã‚¹ãƒ†ãƒ  | Cå‹ | å¼·åŠ›ãªå‹ã‚·ã‚¹ãƒ†ãƒ  |
| ãƒã‚¤ãƒ³ã‚¿ | ã‚ã‚Š | ãªã—ï¼ˆå‚ç…§ã®ã¿ï¼‰ |
| å®‰å…¨æ€§ | ä½ | é«˜ |
| è¡¨ç¾åŠ› | é«˜ | ä¸­ |

**WGSL ã®ä¾‹**:

```wgsl
// WGSL: Rusté¢¨ã®æ§‹æ–‡
@group(0) @binding(0) var<storage, read> a: array<f32>;
@group(0) @binding(1) var<storage, read> b: array<f32>;
@group(0) @binding(2) var<storage, read_write> c: array<f32>;

@compute @workgroup_size(256)
fn matmul(@builtin(global_invocation_id) global_id: vec3<u32>) {
    let idx = global_id.x;
    c[idx] = a[idx] + b[idx];
}
```

**CUDA ã®ä¾‹**:

```c
// CUDA: Cé¢¨ã®æ§‹æ–‡
extern "C" __global__ void matmul(
    const float* a, const float* b, float* c, int n
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}
```

## 7.4 Rust-GPU / SPIR-V ã§ã‚·ã‚§ãƒ¼ãƒ€ã‚’æ›¸ã

**Rust-GPU** [^3] ã¯ã€Rustã‚³ãƒ¼ãƒ‰ã‚’GPUã‚·ã‚§ãƒ¼ãƒ€ãƒ¼ï¼ˆSPIR-Vï¼‰ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚

[^3]: Rust-GPU: https://github.com/EmbarkStudios/rust-gpu

### Rust-GPU ã®åˆ©ç‚¹

| é …ç›® | CUDA/WGSL | Rust-GPU |
|------|-----------|----------|
| è¨€èª | C/WGSL | Rust |
| å‹å®‰å…¨æ€§ | ä½ã€œä¸­ | é«˜ |
| ã‚¨ãƒ©ãƒ¼æ¤œå‡º | å®Ÿè¡Œæ™‚ | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ |
| ãƒã‚¯ãƒ­ | ãªã— | Rustãƒã‚¯ãƒ­ |
| ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ  | ç‹¬è‡ª | Rustã‚¯ãƒ¬ãƒ¼ãƒˆ |

### Rust-GPU ã‚³ãƒ¼ãƒ‰ä¾‹

```rust
#![cfg_attr(target_arch = "spirv", no_std)]

use spirv_std::glam::{Vec3, vec3};
use spirv_std::spirv;

#[spirv(compute(threads(256)))]
pub fn main_cs(
    #[spirv(global_invocation_id)] id: Vec3,
    #[spirv(storage_buffer, descriptor_set = 0, binding = 0)] input: &[f32],
    #[spirv(storage_buffer, descriptor_set = 0, binding = 1)] output: &mut [f32],
) {
    let idx = id.x as usize;
    if idx < input.len() {
        output[idx] = input[idx] * 2.0 + 1.0;
    }
}
```

**ãƒ“ãƒ«ãƒ‰è¨­å®š**ï¼ˆCargo.tomlï¼‰:

```toml
[dependencies]
spirv-std = { version = "0.9", features = ["glam"] }

[profile.dev]
opt-level = 3

[profile.release]
lto = "fat"
```

**ãƒ“ãƒ«ãƒ‰ã‚³ãƒãƒ³ãƒ‰**:

```bash
cargo build --target spirv-unknown-vulkan1.2
```

### Python ã«ã¯ãªã„ Rust-GPU ã®å¼·ã¿

```rust
// âœ… Rustã®å‹å®‰å…¨æ€§ã‚’GPUã§ã‚‚äº«å—
#[spirv(compute(threads(256)))]
pub fn type_safe_kernel(
    #[spirv(storage_buffer)] data: &mut [Vec3],  // å‹ãƒã‚§ãƒƒã‚¯æ¸ˆã¿
) {
    // Rustã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚‚ä½¿ç”¨å¯èƒ½
    for vec in data.iter_mut() {
        *vec = vec.normalize();  // ãƒ™ã‚¯ãƒˆãƒ«æ­£è¦åŒ–
    }
}
```

## 7.5 PTX ã‚¢ã‚»ãƒ³ãƒ–ãƒªã¨ä½ãƒ¬ãƒ™ãƒ«æœ€é©åŒ–

**PTX**ï¼ˆParallel Thread Executionï¼‰ã¯ã€CUDAã®ä¸­é–“è¨€èªã§ã™ [^4]ã€‚

[^4]: PTX ISA: https://docs.nvidia.com/cuda/parallel-thread-execution/

### PTX ã®åŸºæœ¬

```ptx
.version 8.0
.target sm_80
.address_size 64

.visible .entry saxpy (
    .param .u64 .ptr .global .align 4 n,
    .param .f32 a,
    .param .u64 .ptr .global .const .align 4 x,
    .param .u64 .ptr .global .align 4 y
) {
    .reg .f32 %f<10>;
    .reg .u64 %r<10>;
    .reg .pred %p<5>;
    
    // ã‚¹ãƒ¬ãƒƒãƒ‰IDè¨ˆç®—
    mov.u32 %r1, %tid.x;
    mov.u32 %r2, %ctaid.x;
    mov.u32 %r3, %ntid.x;
    mad.lo.u32 %r4, %r2, %r3, %r1;  // idx = blockIdx * blockDim + threadIdx
    
    // ... è¨ˆç®— ...
}
```

### ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ PTX

Rustã‹ã‚‰ç›´æ¥PTXã‚’ä½¿ç”¨ã§ãã¾ã™ï¼š

```rust
use core::arch::asm;

#[no_mangle]
pub unsafe extern "ptx-kernel" fn optimized_kernel(
    data: *mut f32,
    n: i32,
) {
    let idx: u32;
    
    // ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³PTX
    asm!(
        "mov.u32 {idx}, %tid.x;",
        idx = out(reg32) idx,
    );
    
    if (idx as i32) < n {
        *data.add(idx as usize) *= 2.0;
    }
}
```

### SASS æœ€é©åŒ–

**SASS**ï¼ˆShader Assemblyï¼‰ã¯ã€PTXã‹ã‚‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚ŒãŸå®Ÿéš›ã®ãƒã‚·ãƒ³ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚

```bash
# PTX â†’ SASS é€†ã‚¢ã‚»ãƒ³ãƒ–ãƒ«
cuobjdump -sass my_kernel.cubin

# å‡ºåŠ›ä¾‹:
# IMAD R2, R0, R1, R3;  // æ•´æ•°ç©å’Œ
# FFMA R4, R5, R6, R7;  // æµ®å‹•å°æ•°ç‚¹èåˆç©å’Œ
```

**æœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ**:
- **èåˆç©å’Œï¼ˆFMAï¼‰**: \(a \times b + c\) ã‚’1å‘½ä»¤ã§
- **ãƒ«ãƒ¼ãƒ—å±•é–‹**: åˆ†å²ã‚’æ¸›ã‚‰ã™
- **å‘½ä»¤ãƒ¬ãƒ™ãƒ«ä¸¦åˆ—æ€§ï¼ˆILPï¼‰**: ç‹¬ç«‹ã—ãŸæ¼”ç®—ã‚’ä¸¦ã¹ã‚‹

### ã¾ã¨ã‚ï¼šGPU API ã®é¸æŠæŒ‡é‡

```mermaid
flowchart TD
    Start([GPU APIé¸æŠ]) --> Q1{å¯¾å¿œGPU}
    
    Q1 -->|NVIDIAå°‚ç”¨| Q2{æ€§èƒ½è¦ä»¶}
    Q1 -->|ãƒãƒ«ãƒãƒ™ãƒ³ãƒ€ãƒ¼| Q3{å®Ÿè¡Œç’°å¢ƒ}
    
    Q2 -->|æœ€é«˜æ€§èƒ½| CUDA[cudarc<br/>CUDAç›´æ¥åˆ¶å¾¡]
    Q2 -->|é–‹ç™ºé€Ÿåº¦| PyO3[PyO3 + PyTorch]
    
    Q3 -->|ãƒã‚¤ãƒ†ã‚£ãƒ–| wgpu[wgpu<br/>Vulkan/Metal/DX12]
    Q3 -->|ãƒ–ãƒ©ã‚¦ã‚¶| WebGPU[wgpu + WASM<br/>WebGPU]
    Q3 -->|å‹å®‰å…¨é‡è¦–| RustGPU[Rust-GPU<br/>SPIR-V]
    
    style CUDA fill:#76B900
    style PyO3 fill:#4B8BBE
    style wgpu fill:#CE422B
    style WebGPU fill:#FFD43B
    style RustGPU fill:#CE422B
```

| ç”¨é€” | æ¨å¥¨ | ç†ç”± |
|------|------|------|
| ç ”ç©¶ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ— | PyTorch | ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ  |
| NVIDIAå°‚ç”¨ãƒ»æœ€é«˜æ€§èƒ½ | cudarc | ç›´æ¥åˆ¶å¾¡ |
| ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  | wgpu | ç§»æ¤æ€§ |
| ãƒ–ãƒ©ã‚¦ã‚¶å®Ÿè¡Œ | wgpu (WASM) | WebGPU |
| å‹å®‰å…¨æ€§ | Rust-GPU | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚æ¤œè¨¼ |

æ¬¡ç« ã§ã¯ã€GPUãƒ¡ãƒ¢ãƒªã®åŠ¹ç‡çš„ãªç®¡ç†ã¨ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ã®æœ€é©åŒ–æ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

---

## å‚è€ƒæ–‡çŒ®

1. NVIDIA Corporation. "CUDA C++ Programming Guide." https://docs.nvidia.com/cuda/cuda-c-programming-guide/
2. NVIDIA Corporation. "CUDA Runtime API." https://docs.nvidia.com/cuda/cuda-runtime-api/
3. NVIDIA Corporation. "PTX ISA." https://docs.nvidia.com/cuda/parallel-thread-execution/
4. coreylowman. "cudarc." https://github.com/coreylowman/cudarc
5. wgpu project. "wgpu Documentation." https://wgpu.rs/
6. Embark Studios. "Rust-GPU." https://github.com/EmbarkStudios/rust-gpu
7. Khronos Group. "WebGPU Specification." https://www.w3.org/TR/webgpu/
8. Nickolls, J., & Dally, W. J. (2010). "The GPU Computing Era." IEEE Micro, 30(2), 56-69.
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬6ç« : GPUã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç†è§£](03-06-GPUã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç†è§£.md) | [â¡ï¸ ç¬¬8ç« : GPUãƒ¡ãƒ¢ãƒªç®¡ç†ã¨æœ€é©åŒ–](03-08-GPUãƒ¡ãƒ¢ãƒªç®¡ç†ã¨æœ€é©åŒ–.md)