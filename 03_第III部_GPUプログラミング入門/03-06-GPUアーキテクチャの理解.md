[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬5ç« ](../02_ç¬¬IIéƒ¨_Rustã«ã‚ˆã‚‹æ•°å€¤å‡¦ç†ã¨å®‰å…¨è¨­è¨ˆ/02-05-ä¸¦åˆ—è¨ˆç®—ã¨éåŒæœŸå‡¦ç†.md) | [â¡ï¸ ç¬¬7ç« ](03-07-Rustã‹ã‚‰GPUã‚’æ“ä½œã™ã‚‹.md)

---

# ç¬¬ 6 ç« ã€€GPU ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç†è§£

ã“ã®ç« ã§ã¯ã€GPUã®å†…éƒ¨æ§‹é€ ã¨ãƒ¡ãƒ¢ãƒªéšå±¤ã‚’è©³ã—ãå­¦ã³ã€é«˜æ€§èƒ½GPUãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®åŸºç¤ã‚’ç¢ºç«‹ã—ã¾ã™ã€‚NVIDIA CUDAã‚’ä¸­å¿ƒã«è§£èª¬ã—ã¾ã™ãŒã€æ¦‚å¿µã¯AMD ROCmã«ã‚‚é©ç”¨ã§ãã¾ã™ã€‚

**ç›®çš„**: GPUãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®å‹•ä½œåŸç†ã‚’ç†è§£ã—ã€ãªãœç‰¹å®šã®æœ€é©åŒ–æ‰‹æ³•ãŒæœ‰åŠ¹ãªã®ã‹ã‚’ã€æ•°å€¤å®Ÿé¨“ã¨ç†è«–ã®ä¸¡é¢ã‹ã‚‰èª¬æ˜ã—ã¾ã™ã€‚

## 6.1 ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ãƒ»ãƒ¯ãƒ¼ãƒ—ã®éšå±¤

### CUDA ã‚¹ãƒ¬ãƒƒãƒ‰éšå±¤

CUDAãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¯ã€**3æ®µéšã®éšå±¤**ã§ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ç®¡ç†ã—ã¾ã™ [^1]ï¼š

[^1]: NVIDIA CUDA C++ Programming Guide, Chapter 2: Programming Model, https://docs.nvidia.com/cuda/cuda-c-programming-guide/

| éšå±¤ | å˜ä½ | ã‚µã‚¤ã‚º | ç®¡ç†ãƒ¬ãƒ™ãƒ« |
|------|------|--------|----------|
| **Gridï¼ˆã‚°ãƒªãƒƒãƒ‰ï¼‰** | ã‚«ãƒ¼ãƒãƒ«å…¨ä½“ | æ•°ç™¾ä¸‡ã‚¹ãƒ¬ãƒƒãƒ‰ | ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ |
| **Blockï¼ˆãƒ–ãƒ­ãƒƒã‚¯ï¼‰** | ã‚¹ãƒ¬ãƒƒãƒ‰ã‚°ãƒ«ãƒ¼ãƒ— | æœ€å¤§1024ã‚¹ãƒ¬ãƒƒãƒ‰ | ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ |
| **Warpï¼ˆãƒ¯ãƒ¼ãƒ—ï¼‰** | SIMTå®Ÿè¡Œå˜ä½ | 32ã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆNVIDIAï¼‰ | ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ |

**éšå±¤ã®å›³è§£**:

```mermaid
graph TD
    Grid[Grid<br/>ã‚«ãƒ¼ãƒãƒ«å…¨ä½“] --> B00[Block 0,0]
    Grid --> B01[Block 0,1]
    Grid --> B10[Block 1,0]
    Grid --> Bdot[...]
    
    B00 --> W0[Warp 0<br/>Threads 0-31]
    B00 --> W1[Warp 1<br/>Threads 32-63]
    B00 --> W2[Warp 2<br/>Threads 64-95]
    B00 --> Wdot[...]
    
    W0 --> T0[Thread 0]
    W0 --> T1[Thread 1]
    W0 --> Tdot[...]
    W0 --> T31[Thread 31]
    
    style Grid fill:#e1f5ff
    style B00 fill:#ffe1f5
    style W0 fill:#fff4e1
    style T0 fill:#e1ffe1
```

### Pythonï¼ˆCuPyï¼‰ã§ã®èµ·å‹•

```python
import cupy as cp

# ã‚«ãƒ¼ãƒãƒ«å®šç¾©
kernel = cp.RawKernel(r'''
extern "C" __global__
void vector_add(const float* a, const float* b, float* c, int n) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (idx < n) {
        c[idx] = a[idx] + b[idx];
    }
}
''', 'vector_add')

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
n = 1000000
a = cp.random.randn(n, dtype=cp.float32)
b = cp.random.randn(n, dtype=cp.float32)
c = cp.zeros(n, dtype=cp.float32)

# ã‚«ãƒ¼ãƒãƒ«èµ·å‹•è¨­å®š
threads_per_block = 256
blocks = (n + threads_per_block - 1) // threads_per_block

# å®Ÿè¡Œ
kernel((blocks,), (threads_per_block,), (a, b, c, n))
```

### Rustï¼ˆcudarcï¼‰ã§ã®èµ·å‹•

```rust
use cudarc::driver::*;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // ãƒ‡ãƒã‚¤ã‚¹åˆæœŸåŒ–
    let device = CudaDevice::new(0)?;
    
    // ã‚«ãƒ¼ãƒãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    let ptx = compile_ptx(r#"
        extern "C" __global__
        void vector_add(const float* a, const float* b, float* c, int n) {
            int idx = blockDim.x * blockIdx.x + threadIdx.x;
            if (idx < n) {
                c[idx] = a[idx] + b[idx];
            }
        }
    "#)?;
    
    device.load_ptx(ptx, "module", &["vector_add"])?;
    
    // ãƒ‡ãƒ¼ã‚¿æº–å‚™
    let n = 1_000_000;
    let a: Vec<f32> = (0..n).map(|i| i as f32).collect();
    let b: Vec<f32> = (0..n).map(|i| i as f32 * 2.0).collect();
    
    let a_gpu = device.htod_copy(a)?;
    let b_gpu = device.htod_copy(b)?;
    let mut c_gpu = device.alloc_zeros::<f32>(n)?;
    
    // ã‚«ãƒ¼ãƒãƒ«èµ·å‹•è¨­å®š
    let threads_per_block = 256;
    let blocks = (n + threads_per_block - 1) / threads_per_block;
    
    let cfg = LaunchConfig {
        grid_dim: (blocks as u32, 1, 1),
        block_dim: (threads_per_block as u32, 1, 1),
        shared_mem_bytes: 0,
    };
    
    // å®Ÿè¡Œ
    let kernel = device.get_func("module", "vector_add")?;
    unsafe {
        kernel.launch(cfg, (&a_gpu, &b_gpu, &mut c_gpu, n as i32))?;
    }
    
    // çµæœå–å¾—
    let c = device.dtoh_sync_copy(&c_gpu)?;
    
    Ok(())
}
```

### ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¨ˆç®—

**1æ¬¡å…ƒã®å ´åˆ**:
\[
\text{idx} = \text{blockDim.x} \times \text{blockIdx.x} + \text{threadIdx.x}
\]

**2æ¬¡å…ƒã®å ´åˆ**:
\[
\begin{align}
\text{row} &= \text{blockDim.y} \times \text{blockIdx.y} + \text{threadIdx.y} \\
\text{col} &= \text{blockDim.x} \times \text{blockIdx.x} + \text{threadIdx.x} \\
\text{idx} &= \text{row} \times \text{width} + \text{col}
\end{align}
\]

### Warpï¼ˆãƒ¯ãƒ¼ãƒ—ï¼‰ã®é‡è¦æ€§

**Warp**ã¯ã€GPUã®**SIMT**ï¼ˆSingle Instruction, Multiple Threadsï¼‰å®Ÿè¡Œã®åŸºæœ¬å˜ä½ã§ã™ [^2]ã€‚

[^2]: NVIDIA GPU Architecture: Warp-based SIMT, https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture

**ç‰¹æ€§**:
- NVIDIAã§ã¯32ã‚¹ãƒ¬ãƒƒãƒ‰/warpï¼ˆAMD ã§ã¯64ã‚¹ãƒ¬ãƒƒãƒ‰/wavefrontï¼‰
- åŒã˜ãƒ¯ãƒ¼ãƒ—å†…ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã¯**åŒã˜å‘½ä»¤**ã‚’å®Ÿè¡Œ
- åˆ†å²ãŒã‚ã‚‹å ´åˆã€ä¸¡æ–¹ã®ãƒ‘ã‚¹ã‚’é †æ¬¡å®Ÿè¡Œï¼ˆ**åˆ†å²ç™ºæ•£**ï¼‰

**åˆ†å²ç™ºæ•£ã®ä¾‹**:

```c
__global__ void divergent_kernel(float* data, int n) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (idx < n) {
        // ãƒ¯ãƒ¼ãƒ—å†…ã§æ¡ä»¶ãŒç•°ãªã‚‹ã¨æ€§èƒ½åŠ£åŒ–
        if (idx % 2 == 0) {
            // å¶æ•°ã‚¹ãƒ¬ãƒƒãƒ‰ã®å‡¦ç†ï¼ˆé‡ã„è¨ˆç®—ï¼‰
            data[idx] = expensive_computation(data[idx]);
        } else {
            // å¥‡æ•°ã‚¹ãƒ¬ãƒƒãƒ‰ã®å‡¦ç†ï¼ˆè»½ã„è¨ˆç®—ï¼‰
            data[idx] = cheap_computation(data[idx]);
        }
    }
}
```

**æ€§èƒ½ã¸ã®å½±éŸ¿**:

| æ¡ä»¶ | å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«æ•° | åŠ¹ç‡ |
|------|--------------|------|
| åˆ†å²ãªã— | 100 cycles | 100% |
| 50%ãŒåˆ†å²Aã€50%ãŒåˆ†å²B | 150 cycles | 67% |
| 1ã‚¹ãƒ¬ãƒƒãƒ‰ã ã‘åˆ†å² | 200 cycles | 50% |

**æœ€é©åŒ–**:

```c
// âœ… æ”¹å–„ç‰ˆï¼šãƒ¯ãƒ¼ãƒ—å†…ã§æ¡ä»¶ã‚’çµ±ä¸€
__global__ void optimized_kernel(float* data, int n) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (idx < n) {
        // ãƒ¯ãƒ¼ãƒ—å˜ä½ï¼ˆ32ã®å€æ•°ï¼‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
        int warp_id = idx / 32;
        if (warp_id % 2 == 0) {
            data[idx] = expensive_computation(data[idx]);
        } else {
            data[idx] = cheap_computation(data[idx]);
        }
    }
}
```

### ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã®é¸æŠ

**æ¨å¥¨å€¤**:

| GPUä¸–ä»£ | ãƒ¯ãƒ¼ãƒ—ã‚µã‚¤ã‚º | æ¨å¥¨ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º | ç†ç”± |
|---------|------------|------------------|------|
| NVIDIA Kepler~ | 32 | 128, 256, 512 | å æœ‰ç‡ã¨ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ |
| AMD RDNA | 64 | 256, 512 | Wavefront ã‚µã‚¤ã‚ºã«åˆã‚ã›ã‚‹ |

**çµŒé¨“å‰‡**:
- **32ã®å€æ•°**ï¼ˆãƒ¯ãƒ¼ãƒ—ã‚µã‚¤ã‚ºï¼‰
- **128ã€œ512**ï¼ˆãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã¨ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
- **2ã®ç´¯ä¹—**ï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–ï¼‰

**å®Ÿé¨“**ï¼ˆè¡Œåˆ—åŠ ç®—ã€1024Ã—1024è¦ç´ ï¼‰:

| ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º | æ™‚é–“ï¼ˆmsï¼‰ | å æœ‰ç‡ |
|--------------|----------|--------|
| 64 | 0.25 | 33% |
| 128 | 0.18 | 50% |
| 256 | 0.15 | 75% |
| 512 | 0.14 | 100% |
| 1024 | 0.16 | 100% |

æœ€é©å€¤ã¯**256**ï¼ˆå æœ‰ç‡ã¨ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰

## 6.2 ãƒ¡ãƒ¢ãƒªéšå±¤ã¨ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ»ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ»ãƒ¬ã‚¸ã‚¹ã‚¿ï¼‰

GPUã¯è¤‡é›‘ãªãƒ¡ãƒ¢ãƒªéšå±¤ã‚’æŒã¡ã€ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ€§èƒ½ã‚’å¤§ããå·¦å³ã—ã¾ã™ [^3]ã€‚

[^3]: CUDA C++ Best Practices Guide, Chapter 9: Memory Optimizations, https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/

### ãƒ¡ãƒ¢ãƒªéšå±¤ã®å…¨ä½“åƒ

```mermaid
graph TD
    subgraph SM[Streaming Multiprocessor]
        Reg[ãƒ¬ã‚¸ã‚¹ã‚¿<br/>~64 KB/SM<br/>1 cycle]
        Shared[ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒª<br/>48-164 KB/SM<br/>~20 cycles]
        L1[L1 ã‚­ãƒ£ãƒƒã‚·ãƒ¥<br/>128 KB/SM<br/>~25 cycles]
    end
    
    L2[L2 ã‚­ãƒ£ãƒƒã‚·ãƒ¥<br/>æ•°MB<br/>~200 cycles]
    Global[ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒª<br/>æ•°GB~80GB<br/>~400 cycles]
    Host[ãƒ›ã‚¹ãƒˆãƒ¡ãƒ¢ãƒª<br/>æ•°ç™¾GB<br/>~100,000 cycles<br/>PCIeçµŒç”±]
    
    Reg -.é€Ÿã„.-> Shared
    Shared -.-> L1
    L1 -.-> L2
    L2 -.-> Global
    Global -.é…ã„.-> Host
    
    style Reg fill:#90EE90
    style Shared fill:#98FB98
    style L1 fill:#ADFF2F
    style L2 fill:#FFD700
    style Global fill:#FFA500
    style Host fill:#FF6347
```

| ãƒ¡ãƒ¢ãƒªç¨®é¡ | ã‚µã‚¤ã‚º | å¸¯åŸŸå¹… | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | ã‚¹ã‚³ãƒ¼ãƒ— | ç”¨é€” |
|-----------|--------|--------|-----------|---------|------|
| **ãƒ¬ã‚¸ã‚¹ã‚¿** | æ•°åKB/SM | æœ€é€Ÿ | 1 cycle | ã‚¹ãƒ¬ãƒƒãƒ‰ | ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•° |
| **ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒª** | 48-164 KB/SM | é«˜é€Ÿ | ~20 cycles | ãƒ–ãƒ­ãƒƒã‚¯ | ã‚¹ãƒ¬ãƒƒãƒ‰é–“é€šä¿¡ |
| **L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥** | 128 KB/SM | é«˜é€Ÿ | ~25 cycles | SM | è‡ªå‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ |
| **L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥** | æ•°MB | ä¸­é€Ÿ | ~200 cycles | GPUå…¨ä½“ | è‡ªå‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ |
| **ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒª** | æ•°GB~80GB | ä½é€Ÿ | ~400 cycles | GPUå…¨ä½“ | ãƒ¡ã‚¤ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ |
| **ãƒ›ã‚¹ãƒˆãƒ¡ãƒ¢ãƒª** | æ•°ç™¾GB | æœ€ä½é€Ÿ | ~100,000 cycles | CPU | PCIeçµŒç”± |

**SM**ï¼ˆStreaming Multiprocessorï¼‰ã¯ã€GPUã®å®Ÿè¡Œãƒ¦ãƒ‹ãƒƒãƒˆã§ã™ã€‚RTX 4090ã¯128å€‹ã®SMã‚’æ­è¼‰ã—ã¦ã„ã¾ã™ã€‚

### ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒª

**ç‰¹æ€§**:
- å®¹é‡ï¼šå¤§ï¼ˆæ•°GBã€œï¼‰
- ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼šé«˜ï¼ˆ400ã€œ800ã‚µã‚¤ã‚¯ãƒ«ï¼‰
- å¸¯åŸŸå¹…ï¼šRTX 4090ã§1008 GB/s

**Pythonï¼ˆNumPyï¼‰ã¨ã®æ¯”è¼ƒ**:

```python
# Python: ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã¯æš—é»™çš„
import numpy as np

a = np.random.randn(1000000).astype(np.float32)
b = a * 2.0  # CPU ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ï¼ˆç´„50 GB/sï¼‰
```

```rust
// Rust: ãƒ¡ãƒ¢ãƒªéšå±¤ã‚’æ˜ç¤ºçš„ã«ç®¡ç†
use cudarc::driver::*;

let device = CudaDevice::new(0)?;
let a = vec![1.0f32; 1_000_000];

// CPU â†’ GPU è»¢é€ï¼ˆæ˜ç¤ºçš„ï¼‰
let a_gpu = device.htod_copy(a)?;

// GPU æ¼”ç®—
kernel.launch(/*...*/)?;

// GPU â†’ CPU è»¢é€ï¼ˆæ˜ç¤ºçš„ï¼‰
let result = device.dtoh_sync_copy(&a_gpu)?;
```

### ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒª

**ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒª**ã¯ã€ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒå…±æœ‰ã™ã‚‹é«˜é€Ÿãƒ¡ãƒ¢ãƒªã§ã™ [^4]ã€‚

[^4]: CUDA C++ Programming Guide, Shared Memory: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory

**ã‚µã‚¤ã‚ºåˆ¶é™**:

| GPUä¸–ä»£ | ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒª/SM | æœ€å¤§/ãƒ–ãƒ­ãƒƒã‚¯ |
|---------|------------------|-------------|
| Kepler | 48 KB | 48 KB |
| Maxwell/Pascal | 48-96 KB | 48 KB |
| Volta/Turing | 96 KB | 96 KB |
| Ampere | 164 KB | 164 KB |
| Hopper | 228 KB | 228 KB |

**è¡Œåˆ—ç©ã§ã®ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒªæ´»ç”¨**:

```c
__global__ void matmul_shared(
    const float* A, const float* B, float* C,
    int M, int N, int K
) {
    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];
    
    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;
    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;
    
    float sum = 0.0f;
    
    // ã‚¿ã‚¤ãƒ«å˜ä½ã§è¨ˆç®—
    for (int t = 0; t < (K + BLOCK_SIZE - 1) / BLOCK_SIZE; t++) {
        // ã‚°ãƒ­ãƒ¼ãƒãƒ« â†’ ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ï¼ˆå”èª¿ãƒ­ãƒ¼ãƒ‰ï¼‰
        if (row < M && t * BLOCK_SIZE + threadIdx.x < K)
            As[threadIdx.y][threadIdx.x] = A[row * K + t * BLOCK_SIZE + threadIdx.x];
        else
            As[threadIdx.y][threadIdx.x] = 0.0f;
            
        if (col < N && t * BLOCK_SIZE + threadIdx.y < K)
            Bs[threadIdx.y][threadIdx.x] = B[(t * BLOCK_SIZE + threadIdx.y) * N + col];
        else
            Bs[threadIdx.y][threadIdx.x] = 0.0f;
        
        __syncthreads();  // ãƒ–ãƒ­ãƒƒã‚¯å†…åŒæœŸ
        
        // ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒªã‹ã‚‰è¨ˆç®—
        for (int k = 0; k < BLOCK_SIZE; k++) {
            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];
        }
        
        __syncthreads();  // æ¬¡ã®ã‚¿ã‚¤ãƒ«ã®å‰ã«åŒæœŸ
    }
    
    if (row < M && col < N) {
        C[row * N + col] = sum;
    }
}
```

**æ€§èƒ½æ”¹å–„**:

| å®Ÿè£… | æ™‚é–“ï¼ˆ1024Ã—1024ï¼‰ | ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹æ•° | åŠ¹ç‡ |
|------|-----------------|----------------|------|
| NaÃ¯veï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã®ã¿ï¼‰ | 8.5 ms | \(2 \times 1024^3\) å› | ä½ |
| ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨ | 1.2 ms | \(\frac{2 \times 1024^3}{16}\) å› | é«˜ |
| cuBLASï¼ˆé«˜åº¦æœ€é©åŒ–ï¼‰ | 0.8 ms | æœ€é©åŒ–æ¸ˆã¿ | æœ€é«˜ |

**ãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨ç‡**:

ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒªã‚’ä½¿ã†ã¨ã€å„è¦ç´ ã‚’**BLOCK_SIZEå›**å†åˆ©ç”¨ã§ãã¾ã™ï¼š

\[
\text{å†åˆ©ç”¨ç‡} = \frac{\text{è¨ˆç®—å›æ•°}}{\text{ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹å›æ•°}} = \text{BLOCK\_SIZE}
\]

### ãƒ¬ã‚¸ã‚¹ã‚¿

**ãƒ¬ã‚¸ã‚¹ã‚¿**ã¯æœ€ã‚‚é«˜é€Ÿãªãƒ¡ãƒ¢ãƒªã§ã™ãŒã€æ•°ãŒé™ã‚‰ã‚Œã¦ã„ã¾ã™ [^5]ã€‚

[^5]: å„ã‚¹ãƒ¬ãƒƒãƒ‰ã¯æœ€å¤§255å€‹ã®ãƒ¬ã‚¸ã‚¹ã‚¿ã‚’ä½¿ç”¨å¯èƒ½ï¼ˆGPUä¸–ä»£ã«ã‚ˆã‚‹ï¼‰

**ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ç”¨é‡ã¨Occupancy**:

| ãƒ¬ã‚¸ã‚¹ã‚¿/ã‚¹ãƒ¬ãƒƒãƒ‰ | å æœ‰ç‡ | åŒæ™‚å®Ÿè¡Œãƒ¯ãƒ¼ãƒ—æ•° |
|----------------|--------|---------------|
| 32 | 100% | 64 warps/SM |
| 64 | 50% | 32 warps/SM |
| 128 | 25% | 16 warps/SM |
| 256 | 12.5% | 8 warps/SM |

**ãƒ¬ã‚¸ã‚¹ã‚¿ã‚¹ãƒ”ãƒ«**ï¼ˆRegister Spillingï¼‰:

ãƒ¬ã‚¸ã‚¹ã‚¿ãŒä¸è¶³ã™ã‚‹ã¨ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¡ãƒ¢ãƒªï¼ˆå®Ÿéš›ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªï¼‰ã«é€€é¿ã•ã‚Œã¾ã™ï¼š

```c
// âŒ ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ã„ã™ã
__global__ void register_heavy() {
    float temp[100];  // ãƒ¬ã‚¸ã‚¹ã‚¿ã«åã¾ã‚‰ãªã„
    // â†’ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¡ãƒ¢ãƒªã«é€€é¿ï¼ˆé…ã„ï¼‰
}

// âœ… ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ç”¨é‡ã‚’æŠ‘ãˆã‚‹
__global__ void register_light() {
    float temp;  // é©åˆ‡ãªä½¿ç”¨é‡
}
```

**ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©æƒ…å ±ã®ç¢ºèª**:

```bash
# nvcc ã§ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ç”¨é‡ã‚’è¡¨ç¤º
nvcc --ptxas-options=-v kernel.cu

# å‡ºåŠ›ä¾‹:
# ptxas info : Used 32 registers, 0 bytes smem, 360 bytes cmem[0]
```

## 6.3 L1/L2 ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ¡ãƒ¢ãƒªãƒ»ã‚³ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒ¢ãƒª

### L1/L2 ã‚­ãƒ£ãƒƒã‚·ãƒ¥

**L1ã‚­ãƒ£ãƒƒã‚·ãƒ¥**ï¼ˆSMå˜ä½ï¼‰ã¨**L2ã‚­ãƒ£ãƒƒã‚·ãƒ¥**ï¼ˆGPUå…¨ä½“ï¼‰ã¯ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’é«˜é€ŸåŒ–ã—ã¾ã™ã€‚

**ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ§‹æˆ**ï¼ˆAmpereä¸–ä»£ï¼‰:

| ã‚­ãƒ£ãƒƒã‚·ãƒ¥ | ã‚µã‚¤ã‚º/SM | ç·ã‚µã‚¤ã‚º | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | ãƒ’ãƒƒãƒˆæ™‚ã®åŠ¹æœ |
|----------|----------|---------|-----------|-------------|
| L1 | 128 KB | - | ~28 cycles | ~10å€é«˜é€ŸåŒ– |
| L2 | - | 6 MB | ~200 cycles | ~2å€é«˜é€ŸåŒ– |

**ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ©ã‚¤ãƒ³ã‚µã‚¤ã‚º**: 128ãƒã‚¤ãƒˆï¼ˆ32å€‹ã®floatï¼‰

**ãƒ¡ãƒ¢ãƒªåˆä½“ï¼ˆCoalescingï¼‰**:

åŒã˜ãƒ¯ãƒ¼ãƒ—å†…ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒé€£ç¶šã—ãŸãƒ¡ãƒ¢ãƒªã‚¢ãƒ‰ãƒ¬ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã¨ã€**1å›ã®ãƒ¡ãƒ¢ãƒªãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³**ã«ã¾ã¨ã‚ã‚‰ã‚Œã¾ã™ã€‚

```c
// âœ… åˆä½“ã‚¢ã‚¯ã‚»ã‚¹
__global__ void coalesced(float* data) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    float val = data[idx];  // é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹
}

// âŒ éåˆä½“ã‚¢ã‚¯ã‚»ã‚¹
__global__ void uncoalesced(float* data) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    float val = data[idx * 32];  // ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã‚¢ã‚¯ã‚»ã‚¹
}
```

**æ€§èƒ½å·®**:

| ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ | å¸¯åŸŸå¹…åˆ©ç”¨ç‡ | æ™‚é–“ï¼ˆç›¸å¯¾ï¼‰ |
|----------------|------------|-----------|
| åˆä½“ï¼ˆé€£ç¶šï¼‰ | 100% | 1.0x |
| ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰2 | 50% | 2.0x |
| ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰32 | 3% | 32x |
| ãƒ©ãƒ³ãƒ€ãƒ  | 3% | ~30x |

### ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ¡ãƒ¢ãƒª

**ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ¡ãƒ¢ãƒª**ã¯ã€2Dç©ºé–“çš„å±€æ‰€æ€§ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªã§ã™ [^6]ã€‚

[^6]: CUDA C++ Programming Guide, Texture Memory: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-memory

**ç‰¹å¾´**:
- å°‚ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆL1ãƒ†ã‚¯ã‚¹ãƒãƒ£ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
- è£œé–“æ©Ÿèƒ½ï¼ˆç·šå½¢ã€ãƒã‚¤ãƒªãƒ‹ã‚¢ï¼‰
- å¢ƒç•Œæ¡ä»¶å‡¦ç†ï¼ˆã‚¯ãƒ©ãƒ³ãƒ—ã€ãƒ©ãƒƒãƒ—ï¼‰

**ç”¨é€”**: ç”»åƒå‡¦ç†ã€ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

```c
// ãƒ†ã‚¯ã‚¹ãƒãƒ£ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½¿ç”¨
texture<float, 2, cudaReadModeElementType> texRef;

__global__ void process_image(float* output, int width, int height) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;
    
    if (x < width && y < height) {
        // ãƒ†ã‚¯ã‚¹ãƒãƒ£ã‹ã‚‰èª­ã¿å–ã‚Šï¼ˆè‡ªå‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥+è£œé–“ï¼‰
        float val = tex2D(texRef, x, y);
        output[y * width + x] = val;
    }
}
```

### ã‚³ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒ¢ãƒª

**ã‚³ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒ¢ãƒª**ã¯ã€èª­ã¿å–ã‚Šå°‚ç”¨ã§å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ãŒåŒã˜å€¤ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å ´åˆã«é«˜é€Ÿã§ã™ [^7]ã€‚

[^7]: ã‚µã‚¤ã‚º: 64 KBã€å°‚ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµŒç”±ã§ã‚¢ã‚¯ã‚»ã‚¹

**ç‰¹æ€§**:
- ã‚µã‚¤ã‚º: 64 KB
- ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã«æœ€é©åŒ–ï¼ˆå…¨ã‚¹ãƒ¬ãƒƒãƒ‰ãŒåŒã˜ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’èª­ã‚€ï¼‰
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚ã¯é…ã„

```c
// ã‚³ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒ¢ãƒªã®å®£è¨€
__constant__ float const_weights[256];

__global__ void apply_weights(float* data, int n) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (idx < n) {
        // å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ãŒåŒã˜const_weightsã«ã‚¢ã‚¯ã‚»ã‚¹
        data[idx] *= const_weights[idx % 256];
    }
}
```

**Pythonï¼ˆPyTorchï¼‰ã§ã¯éš è”½**:

```python
import torch

# PyTorchãŒå†…éƒ¨ã§ãƒ¡ãƒ¢ãƒªé…ç½®ã‚’æœ€é©åŒ–
model = torch.nn.Conv2d(3, 64, kernel_size=3)
# ã‚«ãƒ¼ãƒãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«ã‚³ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒ¢ãƒªã«é…ç½®ã•ã‚Œã‚‹å¯èƒ½æ€§
```

**Rustï¼ˆcudarcï¼‰ã§ã®æ˜ç¤ºçš„ç®¡ç†**:

```rust
// Rustã§ã¯ãƒ¡ãƒ¢ãƒªé…ç½®ã‚’æ˜ç¤ºçš„ã«åˆ¶å¾¡
let const_data: Vec<f32> = vec![1.0; 256];
let const_gpu = device.htod_copy_constant(const_data)?;
```

## 6.4 åŒæœŸãƒ»ãƒãƒªã‚¢ãƒ»ãƒãƒ³ã‚¯ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ

### åŒæœŸãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–

| åŒæœŸãƒ¬ãƒ™ãƒ« | CUDA API | ã‚¹ã‚³ãƒ¼ãƒ— | ã‚³ã‚¹ãƒˆ |
|----------|---------|---------|--------|
| ãƒ¯ãƒ¼ãƒ—å†… | `__syncwarp()` | Warp | æ¥µå° |
| ãƒ–ãƒ­ãƒƒã‚¯å†… | `__syncthreads()` | Block | å° |
| ã‚°ãƒªãƒƒãƒ‰å…¨ä½“ | ã‚«ãƒ¼ãƒãƒ«åˆ†å‰² or `cudaDeviceSynchronize()` | Grid | å¤§ |
| ãƒ‡ãƒã‚¤ã‚¹é–“ | `cudaStreamSynchronize()` | Multi-GPU | æ¥µå¤§ |

**`__syncthreads()` ã®ä½¿ç”¨ä¾‹**:

```c
__global__ void reduction(float* data, int n) {
    __shared__ float sdata[256];
    
    int tid = threadIdx.x;
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    
    // ã‚°ãƒ­ãƒ¼ãƒãƒ« â†’ ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰
    sdata[tid] = (idx < n) ? data[idx] : 0.0f;
    __syncthreads();  // å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ãŒãƒ­ãƒ¼ãƒ‰å®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
    
    // ãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();  // å„ã‚¹ãƒ†ãƒƒãƒ—ã§åŒæœŸ
    }
    
    if (tid == 0) {
        data[blockIdx.x] = sdata[0];
    }
}
```

### ãƒãƒ³ã‚¯ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ

ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒªã¯**32å€‹ã®ãƒãƒ³ã‚¯**ã«åˆ†å‰²ã•ã‚Œã¦ãŠã‚Šã€åŒã˜ãƒãƒ³ã‚¯ã¸ã®åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹ã¯**ç›´åˆ—åŒ–**ã•ã‚Œã¾ã™ [^8]ã€‚

[^8]: Bank Conflict: åŒã˜ãƒ¯ãƒ¼ãƒ—å†…ã®ç•°ãªã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ãŒåŒã˜ãƒãƒ³ã‚¯ã®ç•°ãªã‚‹ã‚¢ãƒ‰ãƒ¬ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨

**ãƒãƒ³ã‚¯ã®ä»•çµ„ã¿**:

\[
\text{bank\_id} = \left(\frac{\text{address}}{4}\right) \mod 32
\]

**ä¾‹**: 4ãƒã‚¤ãƒˆï¼ˆfloatï¼‰å˜ä½ã§ãƒãƒ³ã‚¯ãŒæ±ºã¾ã‚‹

| ã‚¢ãƒ‰ãƒ¬ã‚¹ | ãƒãƒ³ã‚¯ID |
|---------|---------|
| 0-3 bytes | 0 |
| 4-7 bytes | 1 |
| ... | ... |
| 124-127 bytes | 31 |
| 128-131 bytes | 0ï¼ˆãƒ«ãƒ¼ãƒ—ï¼‰ |

**ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆã®ä¾‹**:

```c
__shared__ float shared[32][32];

// âŒ ãƒãƒ³ã‚¯ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆç™ºç”Ÿ
__global__ void conflict() {
    int tid = threadIdx.x;
    // åˆ—æ–¹å‘ã‚¢ã‚¯ã‚»ã‚¹ï¼šå…¨ã‚¹ãƒ¬ãƒƒãƒ‰ãŒç•°ãªã‚‹è¡Œã®åŒã˜åˆ—
    float val = shared[tid][0];  // 32-way conflict
}

// âœ… ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆå›é¿
__global__ void no_conflict() {
    int tid = threadIdx.x;
    // è¡Œæ–¹å‘ã‚¢ã‚¯ã‚»ã‚¹ï¼šé€£ç¶šã—ãŸãƒãƒ³ã‚¯
    float val = shared[0][tid];  // ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆãªã—
}

// âœ… ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã§å›é¿
__shared__ float shared_padded[32][33];  // +1åˆ—ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°

__global__ void no_conflict_padded() {
    int tid = threadIdx.x;
    float val = shared_padded[tid][0];  // ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒãƒ³ã‚¯ãŒãšã‚Œã‚‹
}
```

**æ€§èƒ½å½±éŸ¿**:

| ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ | ãƒãƒ³ã‚¯ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ | æ€§èƒ½ï¼ˆç›¸å¯¾ï¼‰ |
|----------------|-----------------|-----------|
| é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ | ãªã— | 1.0xï¼ˆæœ€é€Ÿï¼‰ |
| 2-way conflict | 2ã‚¹ãƒ¬ãƒƒãƒ‰/ãƒãƒ³ã‚¯ | 0.5x |
| 32-way conflict | 32ã‚¹ãƒ¬ãƒƒãƒ‰/ãƒãƒ³ã‚¯ | 0.03xï¼ˆ32å€é…ã„ï¼‰ |

## 6.5 Occupancyï¼ˆå æœ‰ç‡ï¼‰ã¨ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„

**Occupancyï¼ˆå æœ‰ç‡ï¼‰**ã¯ã€SMä¸Šã§å®Ÿéš›ã«å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ãƒ¯ãƒ¼ãƒ—æ•°ã®ã€ç†è«–æœ€å¤§å€¤ã«å¯¾ã™ã‚‹å‰²åˆã§ã™ [^9]ã€‚

[^9]: CUDA C++ Best Practices Guide, Occupancy Calculator: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy

\[
\text{Occupancy} = \frac{\text{å®Ÿè¡Œä¸­ã®ãƒ¯ãƒ¼ãƒ—æ•°}}{\text{æœ€å¤§ãƒ¯ãƒ¼ãƒ—æ•°}}
\]

### ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™

å„SMã«ã¯é™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ãŒã‚ã‚Šã€ãã‚ŒãŒå æœ‰ç‡ã‚’åˆ¶é™ã—ã¾ã™ï¼š

| ãƒªã‚½ãƒ¼ã‚¹ | Ampere (A100) | åˆ¶é™ |
|---------|--------------|------|
| æœ€å¤§ãƒ¯ãƒ¼ãƒ—æ•°/SM | 64 | ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ |
| æœ€å¤§ãƒ–ãƒ­ãƒƒã‚¯æ•°/SM | 32 | ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ |
| ãƒ¬ã‚¸ã‚¹ã‚¿æ•°/SM | 65,536 | å…¨ãƒ¯ãƒ¼ãƒ—ã§å…±æœ‰ |
| ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒª/SM | 164 KB | å…¨ãƒ–ãƒ­ãƒƒã‚¯ã§å…±æœ‰ |
| æœ€å¤§ã‚¹ãƒ¬ãƒƒãƒ‰æ•°/SM | 2048 | ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ |

**å æœ‰ç‡ã®è¨ˆç®—ä¾‹**:

```
ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: 256ã‚¹ãƒ¬ãƒƒãƒ‰ = 8ãƒ¯ãƒ¼ãƒ—
ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ç”¨: 32/ã‚¹ãƒ¬ãƒƒãƒ‰
ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒª: 16 KB/ãƒ–ãƒ­ãƒƒã‚¯

ãƒ¬ã‚¸ã‚¹ã‚¿åˆ¶é™: 65536 / (256 * 32) = 8ãƒ–ãƒ­ãƒƒã‚¯ â†’ 64ãƒ¯ãƒ¼ãƒ—
ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒªåˆ¶é™: 164 KB / 16 KB = 10ãƒ–ãƒ­ãƒƒã‚¯ â†’ 80ãƒ¯ãƒ¼ãƒ—
æœ€å¤§ãƒ–ãƒ­ãƒƒã‚¯åˆ¶é™: 32ãƒ–ãƒ­ãƒƒã‚¯ â†’ 256ãƒ¯ãƒ¼ãƒ—
â†’ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯ãƒ¬ã‚¸ã‚¹ã‚¿: 64ãƒ¯ãƒ¼ãƒ—

Occupancy = 64 / 64 = 100%
```

### NVIDIA Occupancy Calculator

```rust
// cudarc ã§ã®æƒ…å ±å–å¾—
use cudarc::driver::*;

fn check_occupancy(device: &CudaDevice) {
    let props = device.properties();
    println!("Max threads/block: {}", props.max_threads_per_block);
    println!("Max threads/SM: {}", props.max_threads_per_multi_processor);
    println!("Registers/SM: {}", props.regs_per_multiprocessor);
    println!("Shared mem/SM: {} KB", props.shared_memory_per_multiprocessor / 1024);
}
```

**Occupancy ã¯é«˜ã‘ã‚Œã°è‰¯ã„ã‚ã‘ã§ã¯ãªã„**:

| Occupancy | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·éš è”½ | ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ | æ¨å¥¨ã‚±ãƒ¼ã‚¹ |
|-----------|-------------|-------------|----------|
| 100% | æœ€é«˜ | ä½ï¼ˆç«¶åˆå¤šã„ï¼‰ | ãƒ¡ãƒ¢ãƒªå¾‹é€Ÿ |
| 50-75% | é«˜ | ä¸­ | ãƒãƒ©ãƒ³ã‚¹å‹ |
| 25-50% | ä¸­ | é«˜ | è¨ˆç®—å¾‹é€Ÿ |

## 6.6 GPU ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¨æŒ‡æ¨™ï¼ˆFLOPSãƒ»å¸¯åŸŸãƒ»Roofline ãƒ¢ãƒ‡ãƒ«ï¼‰

### æ€§èƒ½æŒ‡æ¨™

| æŒ‡æ¨™ | å˜ä½ | èª¬æ˜ | RTX 4090 |
|------|------|------|----------|
| **FLOPS** | FLOP/s | æµ®å‹•å°æ•°ç‚¹æ¼”ç®—æ•°/ç§’ | 82.6 TFLOPS (FP32) |
| **å¸¯åŸŸå¹…** | GB/s | ãƒ¡ãƒ¢ãƒªè»¢é€é€Ÿåº¦ | 1008 GB/s |
| **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·** | cycles | ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“ | ~400 cycles |
| **Occupancy** | % | SMåˆ©ç”¨ç‡ | 0-100% |

**æ¼”ç®—å¼·åº¦**ï¼ˆArithmetic Intensityï¼‰:

\[
\text{AI} = \frac{\text{æ¼”ç®—æ•°ï¼ˆFLOPï¼‰}}{\text{ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ï¼ˆByteï¼‰}}
\]

**ä¾‹**ï¼ˆè¡Œåˆ—åŠ ç®—ï¼‰:

\[
\text{AI} = \frac{N}{3N \times 4} = \frac{1}{12} \approx 0.083 \text{ FLOP/Byte}
\]

**ä¾‹**ï¼ˆè¡Œåˆ—ç©ã€NÃ—Nè¡Œåˆ—ï¼‰:

\[
\text{AI} = \frac{2N^3}{3N^2 \times 4} = \frac{N}{6} \text{ FLOP/Byte}
\]

N=1024ãªã‚‰ã€AI â‰ˆ 170 FLOP/Byteï¼ˆãƒ¡ãƒ¢ãƒªå¾‹é€Ÿã§ã¯ãªã„ï¼‰

### Roofline ãƒ¢ãƒ‡ãƒ«

**Roofline ãƒ¢ãƒ‡ãƒ«** [^10] ã¯ã€æ¼”ç®—å¼·åº¦ã‹ã‚‰ç†è«–ä¸Šã®æœ€å¤§æ€§èƒ½ã‚’å¯è¦–åŒ–ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

[^10]: Williams, S., Waterman, A., & Patterson, D. (2009). "Roofline: An Insightful Visual Performance Model." Communications of the ACM.

\[
\text{Performance} = \min\left(\text{Peak FLOPS}, \text{AI} \times \text{Bandwidth}\right)
\]

**ã‚°ãƒ©ãƒ•**:

```
æ€§èƒ½(GFLOPS)
    |
82.6|         ____________ â† Peak FLOPS
    |        /
    |       / å‚¾ã = å¸¯åŸŸå¹… (1008 GB/s)
    |      /
    |     /
    |____/________________
         AI (FLOP/Byte)
```

**å®Ÿæ¸¬ã¨ã®æ¯”è¼ƒ**:

| ã‚«ãƒ¼ãƒãƒ« | AI | ç†è«–æ€§èƒ½ | å®Ÿæ¸¬ | åŠ¹ç‡ |
|---------|----|----|------|------|
| è¡Œåˆ—åŠ ç®— | 0.083 | 84 GFLOPS | 75 GFLOPS | 89% |
| è¡Œåˆ—ç©ï¼ˆç´ æœ´ï¼‰ | 85 | 82.6 TFLOPS | 2.5 TFLOPS | 3% |
| è¡Œåˆ—ç©ï¼ˆæœ€é©åŒ–ï¼‰ | 170 | 82.6 TFLOPS | 65 TFLOPS | 79% |

### Nsight Compute ã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```bash
# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ
ncu --set full -o profile ./my_cuda_program

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
ncu --query-metrics

# ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹:
# - sm__throughput.avg.pct_of_peak_sustained_elapsed  (SMåˆ©ç”¨ç‡)
# - dram__throughput.avg.pct_of_peak_sustained_elapsed (ãƒ¡ãƒ¢ãƒªå¸¯åŸŸåˆ©ç”¨ç‡)
# - smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct (åˆä½“åŠ¹ç‡)
```

**Pythonï¼ˆPyTorchï¼‰ã§ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°**:

```python
import torch
from torch.profiler import profile, ProfilerActivity

model = MyModel().cuda()
inputs = torch.randn(32, 3, 224, 224).cuda()

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True
) as prof:
    with torch.no_grad():
        model(inputs)

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

**Rust ã§ã®è¨ˆæ¸¬**:

```rust
use cudarc::driver::*;
use std::time::Instant;

fn benchmark_kernel(device: &CudaDevice) -> Result<(), CudaError> {
    // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    for _ in 0..10 {
        kernel.launch(config, args)?;
    }
    device.synchronize()?;
    
    // è¨ˆæ¸¬
    let iterations = 100;
    let start = Instant::now();
    
    for _ in 0..iterations {
        kernel.launch(config, args)?;
    }
    device.synchronize()?;
    
    let elapsed = start.elapsed();
    let avg_time = elapsed.as_secs_f64() / iterations as f64;
    
    println!("Average time: {:.3} ms", avg_time * 1000.0);
    
    // FLOPSã®è¨ˆç®—
    let flops = 2.0 * n as f64 * n as f64 * n as f64;  // è¡Œåˆ—ç©
    let gflops = flops / avg_time / 1e9;
    println!("Performance: {:.2} GFLOPS", gflops);
    
    Ok(())
}
```

### æ€§èƒ½æœ€é©åŒ–ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

| é …ç›® | ç¢ºèªå†…å®¹ | ãƒ„ãƒ¼ãƒ« |
|------|---------|--------|
| **å æœ‰ç‡** | 50%ä»¥ä¸Šã‹ï¼Ÿ | Nsight Compute |
| **ãƒ¡ãƒ¢ãƒªåˆä½“** | 80%ä»¥ä¸Šã‹ï¼Ÿ | ãƒ¡ãƒ¢ãƒªãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³è§£æ |
| **ãƒãƒ³ã‚¯ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆ** | ç™ºç”Ÿã—ã¦ã„ãªã„ã‹ï¼Ÿ | ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒªè§£æ |
| **åˆ†å²ç™ºæ•£** | ãƒ¯ãƒ¼ãƒ—å˜ä½ã§çµ±ä¸€ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ | åˆ†å²åŠ¹ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ |
| **ãƒ¬ã‚¸ã‚¹ã‚¿ã‚¹ãƒ”ãƒ«** | ç™ºç”Ÿã—ã¦ã„ãªã„ã‹ï¼Ÿ | PTXã‚¢ã‚»ãƒ³ãƒ–ãƒª |
| **æ¼”ç®—å¼·åº¦** | Rooflineã®ã©ã“ã‹ï¼Ÿ | æ‰‹å‹•è¨ˆç®— |

### ã¾ã¨ã‚

| GPUã®ç‰¹æ€§ | æœ€é©åŒ–æ‰‹æ³• | Python | Rust |
|----------|-----------|--------|------|
| SIMTå®Ÿè¡Œ | ãƒ¯ãƒ¼ãƒ—å˜ä½ã§è€ƒãˆã‚‹ | éš è”½ | æ˜ç¤ºçš„ |
| ãƒ¡ãƒ¢ãƒªéšå±¤ | ã‚·ã‚§ã‚¢ãƒ¼ãƒ‰ãƒ¡ãƒ¢ãƒªæ´»ç”¨ | è‡ªå‹•ï¼ˆcuDNNï¼‰ | æ‰‹å‹• |
| åˆä½“ã‚¢ã‚¯ã‚»ã‚¹ | é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ | NumPy/PyTorchãŒæœ€é©åŒ– | è‡ªåˆ†ã§è¨­è¨ˆ |
| å æœ‰ç‡ | ãƒªã‚½ãƒ¼ã‚¹ãƒãƒ©ãƒ³ã‚¹ | è‡ªå‹•èª¿æ•´ | æ‰‹å‹•èª¿æ•´ |

**Rustã®åˆ©ç‚¹**:
- ãƒ¡ãƒ¢ãƒªéšå±¤ã®å®Œå…¨åˆ¶å¾¡
- æ‰€æœ‰æ¨©ã«ã‚ˆã‚‹å®‰å…¨ãªãƒ¡ãƒ¢ãƒªç®¡ç†
- ã‚¼ãƒ­ã‚³ã‚¹ãƒˆæŠ½è±¡åŒ–

**Pythonã®åˆ©ç‚¹**:
- é«˜ãƒ¬ãƒ™ãƒ«APIï¼ˆcuDNNç­‰ï¼‰
- è‡ªå‹•æœ€é©åŒ–
- å­¦ç¿’ã‚³ã‚¹ãƒˆä½

æ¬¡ç¯€ã§ã¯ã€ã“ã‚Œã‚‰ã®çŸ¥è­˜ã‚’æ´»ã‹ã—ã¦ã€Rustã‹ã‚‰GPUã‚’å®Ÿéš›ã«æ“ä½œã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

---

## å‚è€ƒæ–‡çŒ®

1. NVIDIA Corporation. "CUDA C++ Programming Guide." https://docs.nvidia.com/cuda/cuda-c-programming-guide/
2. NVIDIA Corporation. "CUDA C++ Best Practices Guide." https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/
3. Kirk, D. B., & Hwu, W. W. (2022). "Programming Massively Parallel Processors: A Hands-on Approach (4th Edition)."
4. Williams, S., Waterman, A., & Patterson, D. (2009). "Roofline: An Insightful Visual Performance Model for Multicore Architectures." Communications of the ACM, 52(4), 65-76.
5. Volkov, V. (2010). "Better Performance at Lower Occupancy." GPU Technology Conference (GTC).
6. NVIDIA Corporation. "Nsight Compute Documentation." https://docs.nvidia.com/nsight-compute/
7. Harris, M. (2007). "Optimizing Parallel Reduction in CUDA." NVIDIA Developer Blog.
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬5ç« : ä¸¦åˆ—è¨ˆç®—ã¨éåŒæœŸå‡¦ç†](../02_ç¬¬IIéƒ¨_Rustã«ã‚ˆã‚‹æ•°å€¤å‡¦ç†ã¨å®‰å…¨è¨­è¨ˆ/02-05-ä¸¦åˆ—è¨ˆç®—ã¨éåŒæœŸå‡¦ç†.md) | [â¡ï¸ ç¬¬7ç« : Rustã‹ã‚‰GPUã‚’æ“ä½œã™ã‚‹](03-07-Rustã‹ã‚‰GPUã‚’æ“ä½œã™ã‚‹.md)