[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬18ç« ](../05_ç¬¬Véƒ¨_å¿œç”¨ã¨é«˜åº¦åŒ–/05-18-ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ä¿¡é ¼æ€§.md) | [â¡ï¸ ç¬¬20ç« ](06-20-ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æœ€é©åŒ–.md)

---

# ç¬¬ 16 ç« ã€€CNNãƒ»RNNãƒ»Transformer ã‚’å®Ÿè£…ã™ã‚‹

ã“ã®ç« ã§ã¯ã€ä»£è¡¨çš„ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€Transformerï¼‰ã‚’Rustã§å®Ÿè£…ã—ã¾ã™ã€‚å„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ•°å­¦çš„åŸç†ã‹ã‚‰ã€Pythonå®Ÿè£…ã€Rustå®Ÿè£…ã€GPUæœ€é©åŒ–ã¾ã§æ®µéšçš„ã«å­¦ã³ã¾ã™ã€‚

**ç›®çš„**: å®Ÿç”¨çš„ãªãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¼ãƒ­ã‹ã‚‰æ§‹ç¯‰ã—ã€GPUä¸Šã§ã®é«˜é€ŸåŒ–æ‰‹æ³•ã‚’ç¿’å¾—ã—ã¾ã™ã€‚

## 16.1 Convolution å±¤ã¨ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³

### ç•³ã¿è¾¼ã¿æ¼”ç®—ã®æ•°å­¦çš„å®šç¾©

**2æ¬¡å…ƒç•³ã¿è¾¼ã¿**ï¼ˆ2D Convolutionï¼‰ã¯ã€ç”»åƒå‡¦ç†ã¨CNNã®åŸºç¤æ¼”ç®—ã§ã™ [^1]ã€‚

[^1]: LeCun, Y., et al. (1989). "Backpropagation Applied to Handwritten Zip Code Recognition." Neural Computation.

**æ•°å¼å®šç¾©**:

\[
Y_{b,c*{out},h*{out},w*{out}} = \sum*{c*{in}=0}^{C*{in}-1} \sum*{k_h=0}^{K_h-1} \sum*{k_w=0}^{K_w-1} X*{b,c*{in},h*{out}\cdot s + k_h,w*{out}\cdot s + k_w} \cdot W*{c*{out},c*{in},k_h,k_w} + B*{c\_{out}}
\]

**è¨˜å·ã®èª¬æ˜**:

|| è¨˜å· | æ„å‘³ | å…¸å‹çš„ãªå€¤ |
||------|------|----------|
|| \(X\) | å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« | \((B, C*{in}, H*{in}, W*{in})\) |
|| \(W\) | ã‚«ãƒ¼ãƒãƒ«ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ï¼‰ | \((C*{out}, C*{in}, K_h, K_w)\) |
|| \(B\) | ãƒã‚¤ã‚¢ã‚¹ | \((C*{out})\) |
|| \(Y\) | å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ« | \((B, C*{out}, H*{out}, W*{out})\) |
|| \(s\) | ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ | 1, 2 |
|| \(p\) | ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° | 0, 1, 2 |

**å‡ºåŠ›ã‚µã‚¤ã‚ºã®è¨ˆç®—**:

\[
H*{out} = \left\lfloor \frac{H*{in} + 2p - K_h}{s} \right\rfloor + 1
\]

\[
W*{out} = \left\lfloor \frac{W*{in} + 2p - K_w}{s} \right\rfloor + 1
\]

### Pythonï¼ˆNumPyï¼‰ã§ã®ç´ æœ´ãªå®Ÿè£…

```python
import numpy as np

def conv2d_naive(x, w, bias, stride=1, padding=0):
    """
    ç´ æœ´ãª2Dç•³ã¿è¾¼ã¿å®Ÿè£…
    x: (B, C_in, H, W)
    w: (C_out, C_in, Kh, Kw)
    bias: (C_out,)
    """
    B, C_in, H_in, W_in = x.shape
    C_out, _, Kh, Kw = w.shape
    
    # å‡ºåŠ›ã‚µã‚¤ã‚ºè¨ˆç®—
    H_out = (H_in + 2*padding - Kh) // stride + 1
    W_out = (W_in + 2*padding - Kw) // stride + 1
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    if padding > 0:
        x = np.pad(x, ((0,0), (0,0), (padding,padding), (padding,padding)))
    
    # å‡ºåŠ›åˆæœŸåŒ–
    y = np.zeros((B, C_out, H_out, W_out))
    
    # ç•³ã¿è¾¼ã¿æ¼”ç®—
    for b in range(B):
        for c_out in range(C_out):
            for h_out in range(H_out):
                for w_out in range(W_out):
                    h_start = h_out * stride
                    w_start = w_out * stride
                    
                    # ã‚«ãƒ¼ãƒãƒ«ã¨ã®ç•³ã¿è¾¼ã¿
                    receptive_field = x[b, :, 
                                       h_start:h_start+Kh, 
                                       w_start:w_start+Kw]
                    y[b, c_out, h_out, w_out] = np.sum(
                        receptive_field * w[c_out]
                    ) + bias[c_out]
    
    return y

# ä½¿ç”¨ä¾‹
x = np.random.randn(2, 3, 32, 32)  # ãƒãƒƒãƒ2, RGBç”»åƒ32x32
w = np.random.randn(16, 3, 3, 3)   # 16å€‹ã®3x3ãƒ•ã‚£ãƒ«ã‚¿
bias = np.random.randn(16)

y = conv2d_naive(x, w, bias, stride=1, padding=1)
print(f"å‡ºåŠ›å½¢çŠ¶: {y.shape}")  # (2, 16, 32, 32)
```

**è¨ˆç®—é‡åˆ†æ**:

- ä¹—ç®—å›æ•°: \(B \times C*{out} \times C*{in} \times K_h \times K_w \times H*{out} \times W*{out}\)
- å…·ä½“ä¾‹ï¼ˆResNet-18ã®æœ€åˆã®å±¤ï¼‰:
  - \(B=32, C*{in}=3, C*{out}=64, K=7, H*{out}=W*{out}=112\)
  - ä¹—ç®—å›æ•°: \(32 \times 64 \times 3 \times 7 \times 7 \times 112 \times 112 \approx 3.3\) GFLOPS

### Rust ã§ã®å®Ÿè£…

```rust
use ndarray::{Array4, ArrayView4, ArrayView2, s};

/// ç´ æœ´ãª2Dç•³ã¿è¾¼ã¿å®Ÿè£…
pub fn conv2d_naive(
    x: &Array4<f32>,     // (B, C_in, H, W)
    weight: &Array4<f32>, // (C_out, C_in, Kh, Kw)
    bias: &ArrayView2<f32>, // (C_out,)
    stride: usize,
    padding: usize,
) -> Array4<f32> {
    let (batch, c_in, h_in, w_in) = x.dim();
    let (c_out, _, kh, kw) = weight.dim();
    
    // å‡ºåŠ›ã‚µã‚¤ã‚ºè¨ˆç®—
    let h_out = (h_in + 2 * padding - kh) / stride + 1;
    let w_out = (w_in + 2 * padding - kw) / stride + 1;
    
    // ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    let x_padded = if padding > 0 {
        pad_4d(x, padding)
    } else {
        x.clone()
    };
    
    // å‡ºåŠ›åˆæœŸåŒ–
    let mut y = Array4::<f32>::zeros((batch, c_out, h_out, w_out));
    
    // ç•³ã¿è¾¼ã¿æ¼”ç®—
    for b in 0..batch {
        for co in 0..c_out {
            for ho in 0..h_out {
                for wo in 0..w_out {
                    let h_start = ho * stride;
                    let w_start = wo * stride;
                    
                    let mut sum = 0.0;
                    for ci in 0..c_in {
                        for kh_idx in 0..kh {
                            for kw_idx in 0..kw {
                                let h_idx = h_start + kh_idx;
                                let w_idx = w_start + kw_idx;
                                
                                sum += x_padded[[b, ci, h_idx, w_idx]] 
                                     * weight[[co, ci, kh_idx, kw_idx]];
                            }
                        }
                    }
                    y[[b, co, ho, wo]] = sum + bias[[co, 0]];
                }
            }
        }
    }
    
    y
}

/// 4æ¬¡å…ƒé…åˆ—ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
fn pad_4d(x: &Array4<f32>, pad: usize) -> Array4<f32> {
    let (b, c, h, w) = x.dim();
    let mut padded = Array4::<f32>::zeros((b, c, h + 2*pad, w + 2*pad));
    
    for bi in 0..b {
        for ci in 0..c {
            padded.slice_mut(s![bi, ci, pad..pad+h, pad..pad+w])
                  .assign(&x.slice(s![bi, ci, .., ..]));
        }
    }
    
    padded
}

// ä½¿ç”¨ä¾‹
fn main() {
    use ndarray::Array;
    use ndarray_rand::RandomExt;
    use ndarray_rand::rand_distr::StandardNormal;
    
    let x = Array4::<f32>::random((2, 3, 32, 32), StandardNormal);
    let weight = Array4::<f32>::random((16, 3, 3, 3), StandardNormal);
    let bias = Array::zeros((16, 1));
    
    let y = conv2d_naive(&x, &weight, &bias.view(), 1, 1);
    println!("å‡ºåŠ›å½¢çŠ¶: {:?}", y.dim());  // (2, 16, 32, 32)
}
```

### ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³

**é †ä¼æ’­**ï¼ˆForwardï¼‰:

\[
Y = X \* W + B
\]

**é€†ä¼æ’­**ï¼ˆBackwardï¼‰: æå¤±é–¢æ•° \(L\) ã‹ã‚‰ã®å‹¾é… \(\frac{\partial L}{\partial Y}\) ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€

**å…¥åŠ›å‹¾é…**:

\[
\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \* W^{rot180}
\]

ã“ã“ã§ã€\(W^{rot180}\) ã¯ã‚«ãƒ¼ãƒãƒ«ã‚’180åº¦å›è»¢ã—ãŸã‚‚ã®ï¼ˆè»¢ç½®ç•³ã¿è¾¼ã¿ï¼‰ã€‚

**é‡ã¿å‹¾é…**:

\[
\frac{\partial L}{\partial W} = X \* \frac{\partial L}{\partial Y}
\]

**ãƒã‚¤ã‚¢ã‚¹å‹¾é…**:

\[
\frac{\partial L}{\partial B} = \sum*{b,h,w} \frac{\partial L}{\partial Y*{b,:,h,w}}
\]

### Python ã§ã®é€†ä¼æ’­å®Ÿè£…

```python
def conv2d_backward(dL_dY, x, w, stride=1, padding=0):
    """
    ç•³ã¿è¾¼ã¿å±¤ã®é€†ä¼æ’­
    dL_dY: å‡ºåŠ›å‹¾é… (B, C_out, H_out, W_out)
    x: å…¥åŠ› (B, C_in, H_in, W_in)
    w: ã‚«ãƒ¼ãƒãƒ« (C_out, C_in, Kh, Kw)
    
    Returns:
        dL_dX: å…¥åŠ›å‹¾é…
        dL_dW: ã‚«ãƒ¼ãƒãƒ«å‹¾é…
        dL_dB: ãƒã‚¤ã‚¢ã‚¹å‹¾é…
    """
    B, C_in, H_in, W_in = x.shape
    C_out, _, Kh, Kw = w.shape
    _, _, H_out, W_out = dL_dY.shape
    
    # ãƒã‚¤ã‚¢ã‚¹å‹¾é…ï¼ˆç°¡å˜ï¼‰
    dL_dB = np.sum(dL_dY, axis=(0, 2, 3))
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    if padding > 0:
        x_padded = np.pad(x, ((0,0), (0,0), (padding,padding), (padding,padding)))
    else:
        x_padded = x
    
    # ã‚«ãƒ¼ãƒãƒ«å‹¾é…
    dL_dW = np.zeros_like(w)
    for c_out in range(C_out):
        for c_in in range(C_in):
            for kh in range(Kh):
                for kw in range(Kw):
                    for b in range(B):
                        for h_out in range(H_out):
                            for w_out in range(W_out):
                                h_idx = h_out * stride + kh
                                w_idx = w_out * stride + kw
                                dL_dW[c_out, c_in, kh, kw] += (
                                    dL_dY[b, c_out, h_out, w_out] * 
                                    x_padded[b, c_in, h_idx, w_idx]
                                )
    
    # å…¥åŠ›å‹¾é…ï¼ˆè»¢ç½®ç•³ã¿è¾¼ã¿ï¼‰
    dL_dX = np.zeros_like(x)
    for b in range(B):
        for c_in in range(C_in):
            for h_in in range(H_in):
                for w_in in range(W_in):
                    for c_out in range(C_out):
                        for kh in range(Kh):
                            for kw in range(Kw):
                                h_out = (h_in + padding - kh) // stride
                                w_out = (w_in + padding - kw) // stride
                                
                                if (0 <= h_out < H_out and 
                                    0 <= w_out < W_out and
                                    (h_in + padding - kh) % stride == 0 and
                                    (w_in + padding - kw) % stride == 0):
                                    dL_dX[b, c_in, h_in, w_in] += (
                                        dL_dY[b, c_out, h_out, w_out] *
                                        w[c_out, c_in, kh, kw]
                                    )
    
    return dL_dX, dL_dW, dL_dB
```

### Rust ã§ã®é€†ä¼æ’­å®Ÿè£…

```rust
use ndarray::{Array4, Array1};

pub struct Conv2dGradients {
    pub dx: Array4<f32>,
    pub dw: Array4<f32>,
    pub db: Array1<f32>,
}

pub fn conv2d_backward(
    dl_dy: &Array4<f32>,  // å‡ºåŠ›å‹¾é…
    x: &Array4<f32>,      // å…¥åŠ›
    weight: &Array4<f32>, // ã‚«ãƒ¼ãƒãƒ«
    stride: usize,
    padding: usize,
) -> Conv2dGradients {
    let (batch, c_out, h_out, w_out) = dl_dy.dim();
    let (_, c_in, h_in, w_in) = x.dim();
    let (_, _, kh, kw) = weight.dim();
    
    // ãƒã‚¤ã‚¢ã‚¹å‹¾é…
    let db = dl_dy.sum_axis(ndarray::Axis(0))
                  .sum_axis(ndarray::Axis(1))
                  .sum_axis(ndarray::Axis(1));
    
    // ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    let x_padded = if padding > 0 {
        pad_4d(x, padding)
    } else {
        x.clone()
    };
    
    // ã‚«ãƒ¼ãƒãƒ«å‹¾é…
    let mut dw = Array4::<f32>::zeros(weight.dim());
    for co in 0..c_out {
        for ci in 0..c_in {
            for kh_idx in 0..kh {
                for kw_idx in 0..kw {
                    let mut grad = 0.0;
                    for b in 0..batch {
                        for ho in 0..h_out {
                            for wo in 0..w_out {
                                let h_idx = ho * stride + kh_idx;
                                let w_idx = wo * stride + kw_idx;
                                grad += dl_dy[[b, co, ho, wo]] * 
                                       x_padded[[b, ci, h_idx, w_idx]];
                            }
                        }
                    }
                    dw[[co, ci, kh_idx, kw_idx]] = grad;
                }
            }
        }
    }
    
    // å…¥åŠ›å‹¾é…ï¼ˆè»¢ç½®ç•³ã¿è¾¼ã¿ï¼‰
    let mut dx = Array4::<f32>::zeros(x.dim());
    for b in 0..batch {
        for ci in 0..c_in {
            for hi in 0..h_in {
                for wi in 0..w_in {
                    let mut grad = 0.0;
                    for co in 0..c_out {
                        for kh_idx in 0..kh {
                            for kw_idx in 0..kw {
                                let ho = (hi + padding - kh_idx) / stride;
                                let wo = (wi + padding - kw_idx) / stride;
                                
                                if ho < h_out && wo < w_out &&
                                   (hi + padding - kh_idx) % stride == 0 &&
                                   (wi + padding - kw_idx) % stride == 0 {
                                    grad += dl_dy[[b, co, ho, wo]] *
                                           weight[[co, ci, kh_idx, kw_idx]];
                                }
                            }
                        }
                    }
                    dx[[b, ci, hi, wi]] = grad;
                }
            }
        }
    }
    
    Conv2dGradients { dx, dw, db }
}
```

### å‹¾é…ãƒã‚§ãƒƒã‚¯

**æ•°å€¤å¾®åˆ†**ã«ã‚ˆã‚‹å‹¾é…ã®æ¤œè¨¼:

```rust
fn numerical_gradient(
    x: &Array4<f32>,
    weight: &Array4<f32>,
    bias: &Array2<f32>,
    epsilon: f32,
) -> Array4<f32> {
    let mut grad = Array4::<f32>::zeros(weight.dim());
    
    for idx in 0..weight.len() {
        // weight[idx] ã‚’ epsilon ã ã‘å¢—ã‚„ã™
        let mut w_plus = weight.clone();
        w_plus[idx] += epsilon;
        let y_plus = conv2d_naive(x, &w_plus, &bias.view(), 1, 1);
        
        // weight[idx] ã‚’ epsilon ã ã‘æ¸›ã‚‰ã™
        let mut w_minus = weight.clone();
        w_minus[idx] -= epsilon;
        let y_minus = conv2d_naive(x, &w_minus, &bias.view(), 1, 1);
        
        // ä¸­å¿ƒå·®åˆ†
        grad[idx] = (y_plus.sum() - y_minus.sum()) / (2.0 * epsilon);
    }
    
    grad
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_conv2d_gradient() {
        let x = Array4::<f32>::ones((1, 2, 4, 4));
        let weight = Array4::<f32>::ones((3, 2, 3, 3));
        let bias = Array::zeros((3, 1));
        
        // é †ä¼æ’­
        let y = conv2d_naive(&x, &weight, &bias.view(), 1, 1);
        
        // é€†ä¼æ’­
        let dl_dy = Array4::<f32>::ones(y.dim());
        let grads = conv2d_backward(&dl_dy, &x, &weight, 1, 1);
        
        // æ•°å€¤å¾®åˆ†ã¨æ¯”è¼ƒ
        let numerical_grad = numerical_gradient(&x, &weight, &bias, 1e-5);
        
        let diff = (&grads.dw - &numerical_grad).mapv(f32::abs).sum();
        assert!(diff < 1e-3, "å‹¾é…ã®å·®: {}", diff);
    }
}
```

## 16.2 Im2Col / Winograd / FFT ã«ã‚ˆã‚‹ç•³ã¿è¾¼ã¿æœ€é©åŒ–

ç´ æœ´ãªç•³ã¿è¾¼ã¿å®Ÿè£…ã¯é…ã„ãŸã‚ã€å®Ÿç”¨çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã¯é«˜åº¦ãªæœ€é©åŒ–æ‰‹æ³•ã‚’ä½¿ã„ã¾ã™ã€‚ã“ã“ã§ã¯3ã¤ã®ä¸»è¦æ‰‹æ³•ã‚’è§£èª¬ã—ã¾ã™ã€‚

### Im2Colï¼ˆImage to Columnï¼‰å¤‰æ›

**Im2Col** [^2] ã¯ã€ç•³ã¿è¾¼ã¿ã‚’è¡Œåˆ—ç©ï¼ˆGEMMï¼‰ã«å¤‰æ›ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚BLASãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®é«˜åº¦ã«æœ€é©åŒ–ã•ã‚ŒãŸGEMMã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚

[^2]: Chellapilla, K., et al. (2006). "High Performance Convolutional Neural Networks for Document Processing." ICFHR.

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. å…¥åŠ›ç”»åƒã®å—å®¹é‡ï¼ˆreceptive fieldï¼‰ã‚’åˆ—ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦å±•é–‹
2. ã‚«ãƒ¼ãƒãƒ«ã‚’è¡Œåˆ—ã«å±•é–‹
3. è¡Œåˆ—ç©ã‚’è¨ˆç®—
4. çµæœã‚’å‡ºåŠ›å½¢çŠ¶ã«å†æ•´å½¢

**è¦–è¦šåŒ–**:

```
å…¥åŠ› X: (1, 1, 4, 4)        Im2Colå¾Œ: (9, 4)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚1 2 3 4 â”‚                 â”‚1 2 4 5â”‚  å—å®¹é‡1
â”‚5 6 7 8 â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€>    â”‚2 3 5 6â”‚  å—å®¹é‡2
â”‚9 0 1 2 â”‚                 â”‚5 6 8 9â”‚  å—å®¹é‡3
â”‚3 4 5 6 â”‚                 â”‚6 7 9 0â”‚  å—å®¹é‡4
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ã‚«ãƒ¼ãƒãƒ« W: (1, 1, 2, 2)    å±•é–‹å¾Œ: (1, 4)
â”Œâ”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚1 2  â”‚                     â”‚1 2 3 4    â”‚
â”‚3 4  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€>        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”˜
```

### Pythonï¼ˆNumPyï¼‰ã§ã® Im2Col å®Ÿè£…

```python
def im2col(x, kernel_h, kernel_w, stride=1, padding=0):
    """
    Im2Colå¤‰æ›
    x: (B, C, H, W)
    è¿”ã‚Šå€¤: (B*H_out*W_out, C*Kh*Kw)
    """
    B, C, H, W = x.shape
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    if padding > 0:
        x = np.pad(x, ((0,0), (0,0), (padding,padding), (padding,padding)))
        H += 2 * padding
        W += 2 * padding
    
    H_out = (H - kernel_h) // stride + 1
    W_out = (W - kernel_w) // stride + 1
    
    # Im2Col
    col = np.zeros((B, C, kernel_h, kernel_w, H_out, W_out))
    
    for kh in range(kernel_h):
        h_max = kh + stride * H_out
        for kw in range(kernel_w):
            w_max = kw + stride * W_out
            col[:, :, kh, kw, :, :] = x[:, :, kh:h_max:stride, kw:w_max:stride]
    
    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(B*H_out*W_out, -1)
    return col

def conv2d_im2col(x, w, bias, stride=1, padding=0):
    """
    Im2Colã‚’ä½¿ã£ãŸç•³ã¿è¾¼ã¿
    """
    B, C_in, H, W = x.shape
    C_out, _, Kh, Kw = w.shape
    
    # Im2Colå¤‰æ›
    col = im2col(x, Kh, Kw, stride, padding)
    
    # ã‚«ãƒ¼ãƒãƒ«ã‚’2æ¬¡å…ƒè¡Œåˆ—ã«
    w_col = w.reshape(C_out, -1)
    
    # è¡Œåˆ—ç©ï¼ˆé«˜é€ŸãªBLASã‚’ä½¿ç”¨ï¼‰
    out = col @ w_col.T + bias  # (B*H_out*W_out, C_out)
    
    # å‡ºåŠ›å½¢çŠ¶ã«æˆ»ã™
    H_out = (H + 2*padding - Kh) // stride + 1
    W_out = (W + 2*padding - Kw) // stride + 1
    out = out.reshape(B, H_out, W_out, C_out).transpose(0, 3, 1, 2)
    
    return out

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
import time

x = np.random.randn(32, 3, 224, 224).astype(np.float32)
w = np.random.randn(64, 3, 7, 7).astype(np.float32)
bias = np.zeros(64, dtype=np.float32)

# ç´ æœ´ãªå®Ÿè£…
start = time.time()
y1 = conv2d_naive(x, w, bias, stride=2, padding=3)
time_naive = time.time() - start

# Im2Colå®Ÿè£…
start = time.time()
y2 = conv2d_im2col(x, w, bias, stride=2, padding=3)
time_im2col = time.time() - start

print(f"ç´ æœ´ãªå®Ÿè£…: {time_naive*1000:.2f} ms")
print(f"Im2Colå®Ÿè£…: {time_im2col*1000:.2f} ms")
print(f"é«˜é€ŸåŒ–ç‡: {time_naive/time_im2col:.1f}x")
# å‡ºåŠ›ä¾‹:
# ç´ æœ´ãªå®Ÿè£…: 8542.3 ms
# Im2Colå®Ÿè£…: 156.7 ms
# é«˜é€ŸåŒ–ç‡: 54.5x
```

### Rust ã§ã® Im2Col å®Ÿè£…

```rust
use ndarray::{Array2, Array4, ArrayView4};
use ndarray_linalg::Dot;

pub fn im2col(
    x: &Array4<f32>,
    kernel_h: usize,
    kernel_w: usize,
    stride: usize,
    padding: usize,
) -> Array2<f32> {
    let (batch, c_in, h, w) = x.dim();
    
    // ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    let x_padded = if padding > 0 {
        pad_4d(x, padding)
    } else {
        x.clone()
    };
    
    let h_padded = h + 2 * padding;
    let w_padded = w + 2 * padding;
    
    let h_out = (h_padded - kernel_h) / stride + 1;
    let w_out = (w_padded - kernel_w) / stride + 1;
    
    // Im2Colå¤‰æ›
    let col_size = c_in * kernel_h * kernel_w;
    let mut col = Array2::<f32>::zeros((batch * h_out * w_out, col_size));
    
    let mut row = 0;
    for b in 0..batch {
        for h_idx in 0..h_out {
            for w_idx in 0..w_out {
                let mut col_idx = 0;
                for c in 0..c_in {
                    for kh in 0..kernel_h {
                        for kw in 0..kernel_w {
                            let h_pos = h_idx * stride + kh;
                            let w_pos = w_idx * stride + kw;
                            col[[row, col_idx]] = x_padded[[b, c, h_pos, w_pos]];
                            col_idx += 1;
                        }
                    }
                }
                row += 1;
            }
        }
    }
    
    col
}

pub fn conv2d_im2col(
    x: &Array4<f32>,
    weight: &Array4<f32>,
    bias: &Array1<f32>,
    stride: usize,
    padding: usize,
) -> Array4<f32> {
    let (batch, c_in, h, w) = x.dim();
    let (c_out, _, kh, kw) = weight.dim();
    
    // Im2Colå¤‰æ›
    let col = im2col(x, kh, kw, stride, padding);
    
    // ã‚«ãƒ¼ãƒãƒ«ã‚’2æ¬¡å…ƒè¡Œåˆ—ã«
    let w_col = weight.to_shape((c_out, c_in * kh * kw)).unwrap();
    
    // è¡Œåˆ—ç©ï¼ˆBLASä½¿ç”¨ï¼‰
    let mut out = col.dot(&w_col.t());
    
    // ãƒã‚¤ã‚¢ã‚¹åŠ ç®—
    for i in 0..out.nrows() {
        for j in 0..c_out {
            out[[i, j]] += bias[j];
        }
    }
    
    // å‡ºåŠ›å½¢çŠ¶ã«æˆ»ã™
    let h_out = (h + 2*padding - kh) / stride + 1;
    let w_out = (w + 2*padding - kw) / stride + 1;
    
    out.into_shape((batch, h_out, w_out, c_out))
       .unwrap()
       .permuted_axes([0, 3, 1, 2])
       .to_owned()
}
```

**Im2Colã®åˆ©ç‚¹ã¨æ¬ ç‚¹**:

|| åˆ©ç‚¹ | æ¬ ç‚¹ |
||------|------|
|| BLASã®é«˜é€ŸGEMMã‚’æ´»ç”¨ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ  |
|| å®Ÿè£…ãŒæ¯”è¼ƒçš„ç°¡å˜ | å¤‰æ›ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ |
|| GPUã§é«˜é€Ÿï¼ˆcuBLASï¼‰ | å°ã•ã„ã‚«ãƒ¼ãƒãƒ«ã§éåŠ¹ç‡ |

### Winograd ç•³ã¿è¾¼ã¿

**Winogradç•³ã¿è¾¼ã¿** [^3] ã¯ã€ä¹—ç®—å›æ•°ã‚’å‰Šæ¸›ã™ã‚‹é«˜é€Ÿã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚

[^3]: Lavin, A., & Gray, S. (2016). "Fast Algorithms for Convolutional Neural Networks." CVPR.

**åŸç†**: FFTã¨åŒæ§˜ã€ç•³ã¿è¾¼ã¿å®šç†ã‚’åˆ©ç”¨ã—ã¾ã™ãŒã€ã‚ˆã‚Šå°ã•ã„ã‚«ãƒ¼ãƒãƒ«ï¼ˆ3x3ã€5x5ï¼‰ã«ç‰¹åŒ–ã€‚

**è¨ˆç®—é‡å‰Šæ¸›**:

|| æ‰‹æ³• | ä¹—ç®—å›æ•°ï¼ˆ3x3ã‚«ãƒ¼ãƒãƒ«ï¼‰ | å‰Šæ¸›ç‡ |
||------|----------------------|--------|
|| ç›´æ¥ç•³ã¿è¾¼ã¿ | 9 | - |
|| Winograd F(2,3) | 4 | 55% |
|| Winograd F(4,3) | 16ï¼ˆ4x4å‡ºåŠ›ï¼‰ | 77% |

**æ•°å­¦çš„å®šç¾©**ï¼ˆF(2,3): 2x2å‡ºåŠ›ã€3x3ã‚«ãƒ¼ãƒãƒ«ï¼‰:

\[
Y = A^T \left[ (G g G^T) \odot (B^T d B) \right] A
\]

ã“ã“ã§ã€

- \(g\): ã‚«ãƒ¼ãƒãƒ«ï¼ˆ3x3ï¼‰
- \(d\): å…¥åŠ›ã‚¿ã‚¤ãƒ«ï¼ˆ4x4ï¼‰
- \(G, B, A\): Winogradå¤‰æ›è¡Œåˆ—
- \(\odot\): è¦ç´ ã”ã¨ã®ç©

**å¤‰æ›è¡Œåˆ—**:

\[
G = \begin{bmatrix}
1 & 0 & 0 \\\\
\frac{1}{2} & \frac{1}{2} & \frac{1}{2} \\\\
\frac{1}{2} & -\frac{1}{2} & \frac{1}{2} \\\\
0 & 0 & 1
\end{bmatrix}, \quad
B^T = \begin{bmatrix}
1 & 0 & -1 & 0 \\\\
0 & 1 & 1 & 0 \\\\
0 & -1 & 1 & 0 \\\\
0 & 1 & 0 & -1
\end{bmatrix}
\]

\[
A^T = \begin{bmatrix}
1 & 1 & 1 & 0 \\\\
0 & 1 & -1 & -1
\end{bmatrix}
\]

### Python ã§ã® Winograd å®Ÿè£…ï¼ˆF(2,3)ï¼‰

```python
def winograd_f2_3(x, w):
    """
    Winograd F(2,3) å®Ÿè£…
    x: (4, 4) å…¥åŠ›ã‚¿ã‚¤ãƒ«
    w: (3, 3) ã‚«ãƒ¼ãƒãƒ«
    è¿”ã‚Šå€¤: (2, 2) å‡ºåŠ›ã‚¿ã‚¤ãƒ«
    """
    # å¤‰æ›è¡Œåˆ—
    G = np.array([
        [1,  0,  0],
        [0.5,  0.5,  0.5],
        [0.5, -0.5,  0.5],
        [0,  0,  1]
    ])
    
    B_T = np.array([
        [1,  0, -1,  0],
        [0,  1,  1,  0],
        [0, -1,  1,  0],
        [0,  1,  0, -1]
    ])
    
    A_T = np.array([
        [1,  1,  1,  0],
        [0,  1, -1, -1]
    ])
    
    # Winogradå¤‰æ›
    U = G @ w @ G.T          # ã‚«ãƒ¼ãƒãƒ«å¤‰æ› (4x4)
    V = B_T @ x @ B_T.T      # å…¥åŠ›å¤‰æ› (4x4)
    M = U * V                # è¦ç´ ã”ã¨ã®ç© (4x4)
    Y = A_T @ M @ A_T.T      # é€†å¤‰æ› (2x2)
    
    return Y

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
x_tile = np.random.randn(4, 4)
kernel = np.random.randn(3, 3)

# ç›´æ¥ç•³ã¿è¾¼ã¿ï¼ˆ9å›ã®ä¹—ç®—ï¼‰
y_direct = np.zeros((2, 2))
for i in range(2):
    for j in range(2):
        y_direct[i, j] = np.sum(x_tile[i:i+3, j:j+3] * kernel)

# Winogradï¼ˆ4å›ã®ä¹—ç®—ï¼‰
y_winograd = winograd_f2_3(x_tile, kernel)

print(f"èª¤å·®: {np.abs(y_direct - y_winograd).max():.6f}")
# å‡ºåŠ›: èª¤å·®: 0.000001ï¼ˆæ•°å€¤èª¤å·®ã®ã¿ï¼‰
```

**Winogradã®ç‰¹æ€§**:

|| é …ç›® | è©³ç´° |
||------|------|
|| é©ç”¨ç¯„å›² | 3x3ã€5x5ã‚«ãƒ¼ãƒãƒ«ï¼ˆä¸»ã«3x3ï¼‰ |
|| ä¹—ç®—å‰Šæ¸› | ç´„2.25å€å‰Šæ¸› |
|| åŠ ç®—å¢—åŠ  | ç´„2å€å¢—åŠ  |
|| æ•°å€¤å®‰å®šæ€§ | è‹¥å¹²ä½ä¸‹ï¼ˆè¨±å®¹ç¯„å›²ï¼‰ |
|| GPUå®Ÿè£… | cuDNN ã§è‡ªå‹•é¸æŠ |

### FFTï¼ˆé«˜é€Ÿãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ï¼‰ç•³ã¿è¾¼ã¿

**FFTç•³ã¿è¾¼ã¿** [^4] ã¯ã€å¤§ãã„ã‚«ãƒ¼ãƒãƒ«ï¼ˆ11x11ä»¥ä¸Šï¼‰ã§æœ‰åŠ¹ã§ã™ã€‚

[^4]: Mathieu, M., et al. (2014). "Fast Training of Convolutional Networks through FFTs." ICLR.

**ç•³ã¿è¾¼ã¿å®šç†**:

\[
f \* g = \mathcal{F}^{-1}\left( \mathcal{F}(f) \cdot \mathcal{F}(g) \right)
\]

ã“ã“ã§ã€\(\mathcal{F}\) ã¯ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ã€\(\mathcal{F}^{-1}\) ã¯é€†ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ã€‚

**è¨ˆç®—é‡æ¯”è¼ƒ**ï¼ˆNÃ—Nç”»åƒã€KÃ—Kã‚«ãƒ¼ãƒãƒ«ï¼‰:

|| æ‰‹æ³• | è¨ˆç®—é‡ | N=1024, K=11 |
||------|--------|-------------|
|| ç›´æ¥ç•³ã¿è¾¼ã¿ | \(O(N^2 K^2)\) | 123M |
|| FFTç•³ã¿è¾¼ã¿ | \(O(N^2 \log N)\) | 10Mï¼ˆ12å€é€Ÿï¼‰ |

**Pythonï¼ˆNumPyï¼‰ã§ã® FFT ç•³ã¿è¾¼ã¿**:

```python
def conv2d_fft(x, kernel):
    """
    FFTã‚’ä½¿ã£ãŸ2Dç•³ã¿è¾¼ã¿
    x: (H, W) å…¥åŠ›
    kernel: (Kh, Kw) ã‚«ãƒ¼ãƒãƒ«
    """
    from numpy.fft import fft2, ifft2, fftshift
    
    H, W = x.shape
    Kh, Kw = kernel.shape
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆå¾ªç’°ç•³ã¿è¾¼ã¿ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
    pad_h = H + Kh - 1
    pad_w = W + Kw - 1
    
    # FFT
    X_fft = fft2(x, s=(pad_h, pad_w))
    K_fft = fft2(kernel, s=(pad_h, pad_w))
    
    # å‘¨æ³¢æ•°é ˜åŸŸã§ã®ç©
    Y_fft = X_fft * K_fft
    
    # é€†FFT
    y = np.real(ifft2(Y_fft))
    
    # æœ‰åŠ¹ãªéƒ¨åˆ†ã‚’æŠ½å‡º
    y = y[:H, :W]
    
    return y

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆå¤§ãã„ã‚«ãƒ¼ãƒãƒ«ï¼‰
x = np.random.randn(1024, 1024)
kernel = np.random.randn(11, 11)

import time

# ç›´æ¥ç•³ã¿è¾¼ã¿
start = time.time()
y1 = scipy.signal.convolve2d(x, kernel, mode='valid')
time_direct = time.time() - start

# FFTç•³ã¿è¾¼ã¿
start = time.time()
y2 = conv2d_fft(x, kernel)
time_fft = time.time() - start

print(f"ç›´æ¥ç•³ã¿è¾¼ã¿: {time_direct*1000:.2f} ms")
print(f"FFTç•³ã¿è¾¼ã¿: {time_fft*1000:.2f} ms")
print(f"é«˜é€ŸåŒ–ç‡: {time_direct/time_fft:.1f}x")
# å‡ºåŠ›ä¾‹:
# ç›´æ¥ç•³ã¿è¾¼ã¿: 2845.6 ms
# FFTç•³ã¿è¾¼ã¿: 234.5 ms
# é«˜é€ŸåŒ–ç‡: 12.1x
```

### GPU ä¸Šã§ã®ç•³ã¿è¾¼ã¿æœ€é©åŒ–ã®é¸æŠ

**cuDNN**ï¼ˆNVIDIA ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰ã¯ã€å…¥åŠ›ã‚µã‚¤ã‚ºã¨ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦æœ€é©ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è‡ªå‹•é¸æŠã—ã¾ã™ [^5]ã€‚

[^5]: cuDNN Developer Guide: https://docs.nvidia.com/deeplearning/cudnn/developer-guide/

**cuDNN ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é¸æŠ**:

|| ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | é©ç”¨æ¡ä»¶ | æ€§èƒ½ç‰¹æ€§ |
||------------|---------|---------|
|| `CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM` | ä¸€èˆ¬çš„ãªç•³ã¿è¾¼ã¿ | æ±ç”¨çš„ã€ä¸­é€Ÿ |
|| `CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM` | å°ãƒãƒƒãƒ | å‰è¨ˆç®—ã§é«˜é€ŸåŒ– |
|| `CUDNN_CONVOLUTION_FWD_ALGO_GEMM` | Im2Col | å¤§ãƒãƒƒãƒã§é«˜é€Ÿ |
|| `CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD` | 3x3ã‚«ãƒ¼ãƒãƒ« | æœ€é€Ÿï¼ˆæ•°å€¤èª¤å·®è‹¥å¹²ï¼‰ |
|| `CUDNN_CONVOLUTION_FWD_ALGO_FFT` | å¤§ã‚«ãƒ¼ãƒãƒ«ï¼ˆ11x11+ï¼‰ | å¤§ã‚«ãƒ¼ãƒãƒ«ã§é«˜é€Ÿ |

### Rust ã‹ã‚‰ cuDNN ã‚’ä½¿ã†

```rust
// tch-rsï¼ˆLibTorch ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼‰ã‚’ä½¿ã†ä¾‹
use tch::{Tensor, nn, Device};

fn conv2d_cudnn() {
    let device = Device::Cuda(0);
    
    // å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«
    let x = Tensor::randn(&[32, 3, 224, 224], (tch::Kind::Float, device));
    
    // ç•³ã¿è¾¼ã¿å±¤ï¼ˆcuDNN ãŒè‡ªå‹•é¸æŠï¼‰
    let vs = nn::VarStore::new(device);
    let conv = nn::conv2d(&vs.root(), 3, 64, 7, 
                          nn::ConvConfig { stride: 2, padding: 3, ..Default::default() });
    
    // é †ä¼æ’­ï¼ˆcuDNN ãŒæœ€é©ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é¸æŠï¼‰
    let y = x.apply(&conv);
    
    println!("å‡ºåŠ›å½¢çŠ¶: {:?}", y.size());
}
```

**æœ€é©åŒ–æ‰‹æ³•ã®é¸æŠæŒ‡é‡**:

|| ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º | ãƒãƒƒãƒã‚µã‚¤ã‚º | æ¨å¥¨æ‰‹æ³• | ç†ç”± |
||--------------|------------|---------|------|
|| 1x1 | ä»»æ„ | ç›´æ¥GEMM | ã‚«ãƒ¼ãƒãƒ«å¤‰æ›ä¸è¦ |
|| 3x3 | å¤§ï¼ˆ64+ï¼‰ | Winograd | ä¹—ç®—å‰Šæ¸›ãŒåŠ¹æœçš„ |
|| 3x3 | å°ï¼ˆ<32ï¼‰ | Im2Col | å¤‰æ›ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å° |
|| 7x7 | å¤§ | Im2Col | BLASãŒé«˜é€Ÿ |
|| 11x11+ | ä»»æ„ | FFT | ç†è«–è¨ˆç®—é‡ãŒæœ‰åˆ© |
|| å¯å¤‰ | ä»»æ„ | cuDNNè‡ªå‹•é¸æŠ | å®Ÿæ¸¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§é¸æŠ |

## 16.3 LSTM/GRU ã®ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–

å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆRNNï¼‰ã¯ã€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«ä¸å¯æ¬ ã§ã™ãŒã€é€æ¬¡çš„ãªæ€§è³ªã‹ã‚‰GPUæœ€é©åŒ–ãŒå›°é›£ã§ã™ã€‚LSTMï¼ˆLong Short-Term Memoryï¼‰ã¨GRUï¼ˆGated Recurrent Unitï¼‰ã®åŠ¹ç‡çš„ãªå®Ÿè£…ã‚’å­¦ã³ã¾ã™ã€‚

### LSTM ã®æ•°å­¦çš„å®šç¾©

**LSTM** [^6] ã¯ã€é•·æœŸä¾å­˜æ€§ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«è¨­è¨ˆã•ã‚ŒãŸ RNN ã®ä¸€ç¨®ã§ã™ã€‚

[^6]: Hochreiter, S., & Schmidhuber, J. (1997). "Long Short-Term Memory." Neural Computation.

**4ã¤ã®ã‚²ãƒ¼ãƒˆ**:

\[
\begin{align}
f_t &= \sigma(W_f \cdot [h*{t-1}, x_t] + b_f) \quad \text{ï¼ˆå¿˜å´ã‚²ãƒ¼ãƒˆï¼‰} \\\\
i_t &= \sigma(W_i \cdot [h*{t-1}, x_t] + b_i) \quad \text{ï¼ˆå…¥åŠ›ã‚²ãƒ¼ãƒˆï¼‰} \\\\
\tilde{C}_t &= \tanh(W_C \cdot [h*{t-1}, x_t] + b_C) \quad \text{ï¼ˆã‚»ãƒ«å€™è£œï¼‰} \\\\
o_t &= \sigma(W_o \cdot [h*{t-1}, x_t] + b_o) \quad \text{ï¼ˆå‡ºåŠ›ã‚²ãƒ¼ãƒˆï¼‰}
\end{align}
\]

**çŠ¶æ…‹æ›´æ–°**:

\[
\begin{align}
C_t &= f_t \odot C*{t-1} + i_t \odot \tilde{C}_t \quad \text{ï¼ˆã‚»ãƒ«çŠ¶æ…‹ï¼‰} \\\\
h_t &= o_t \odot \tanh(C_t) \quad \text{ï¼ˆéš ã‚ŒçŠ¶æ…‹ï¼‰}
\end{align}
\]

**è¨˜å·**:

|| è¨˜å· | æ„å‘³ | å½¢çŠ¶ |
||------|------|------|
|| \(x_t\) | æ™‚åˆ» t ã®å…¥åŠ› | \((B, D*{in})\) |
|| \(h_t\) | éš ã‚ŒçŠ¶æ…‹ | \((B, D*{hidden})\) |
|| \(C_t\) | ã‚»ãƒ«çŠ¶æ…‹ | \((B, D*{hidden})\) |
|| \(\sigma\) | ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•° | - |
|| \(\odot\) | è¦ç´ ã”ã¨ã®ç© | - |

### Pythonï¼ˆNumPyï¼‰ã§ã® LSTM å®Ÿè£…

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

class LSTMCell:
    def __init__(self, input_size, hidden_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        # é‡ã¿åˆæœŸåŒ–ï¼ˆXavieråˆæœŸåŒ–ï¼‰
        scale = np.sqrt(2.0 / (input_size + hidden_size))
        self.W_f = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_f = np.zeros(hidden_size)
        
        self.W_i = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_i = np.zeros(hidden_size)
        
        self.W_C = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_C = np.zeros(hidden_size)
        
        self.W_o = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_o = np.zeros(hidden_size)
    
    def forward(self, x, h_prev, C_prev):
        """
        x: (batch, input_size)
        h_prev: (batch, hidden_size)
        C_prev: (batch, hidden_size)
        """
        # å…¥åŠ›ã¨å‰ã®éš ã‚ŒçŠ¶æ…‹ã‚’çµåˆ
        concat = np.concatenate([h_prev, x], axis=1)  # (batch, hidden+input)
        
        # 4ã¤ã®ã‚²ãƒ¼ãƒˆè¨ˆç®—
        f_t = sigmoid(concat @ self.W_f.T + self.b_f)  # å¿˜å´ã‚²ãƒ¼ãƒˆ
        i_t = sigmoid(concat @ self.W_i.T + self.b_i)  # å…¥åŠ›ã‚²ãƒ¼ãƒˆ
        C_tilde = np.tanh(concat @ self.W_C.T + self.b_C)  # ã‚»ãƒ«å€™è£œ
        o_t = sigmoid(concat @ self.W_o.T + self.b_o)  # å‡ºåŠ›ã‚²ãƒ¼ãƒˆ
        
        # ã‚»ãƒ«çŠ¶æ…‹æ›´æ–°
        C_t = f_t * C_prev + i_t * C_tilde
        
        # éš ã‚ŒçŠ¶æ…‹æ›´æ–°
        h_t = o_t * np.tanh(C_t)
        
        return h_t, C_t

# ä½¿ç”¨ä¾‹
batch_size = 32
seq_length = 50
input_size = 128
hidden_size = 256

lstm = LSTMCell(input_size, hidden_size)

# åˆæœŸçŠ¶æ…‹
h = np.zeros((batch_size, hidden_size))
C = np.zeros((batch_size, hidden_size))

# ç³»åˆ—å‡¦ç†
for t in range(seq_length):
    x_t = np.random.randn(batch_size, input_size)
    h, C = lstm.forward(x_t, h, C)

print(f"æœ€çµ‚éš ã‚ŒçŠ¶æ…‹å½¢çŠ¶: {h.shape}")  # (32, 256)
```

### Rust ã§ã® LSTM å®Ÿè£…

```rust
use ndarray::{Array2, ArrayView2};

pub struct LSTMCell {
    w_f: Array2<f32>,
    b_f: Array1<f32>,
    w_i: Array2<f32>,
    b_i: Array1<f32>,
    w_c: Array2<f32>,
    b_c: Array1<f32>,
    w_o: Array2<f32>,
    b_o: Array1<f32>,
}

impl LSTMCell {
    pub fn new(input_size: usize, hidden_size: usize) -> Self {
        use ndarray_rand::RandomExt;
        use ndarray_rand::rand_distr::StandardNormal;
        
        let total_size = input_size + hidden_size;
        let scale = (2.0 / (input_size + hidden_size) as f32).sqrt();
        
        Self {
            w_f: Array2::random((hidden_size, total_size), StandardNormal) * scale,
            b_f: Array1::zeros(hidden_size),
            w_i: Array2::random((hidden_size, total_size), StandardNormal) * scale,
            b_i: Array1::zeros(hidden_size),
            w_c: Array2::random((hidden_size, total_size), StandardNormal) * scale,
            b_c: Array1::zeros(hidden_size),
            w_o: Array2::random((hidden_size, total_size), StandardNormal) * scale,
            b_o: Array1::zeros(hidden_size),
        }
    }
    
    pub fn forward(
        &self,
        x: &ArrayView2<f32>,
        h_prev: &Array2<f32>,
        c_prev: &Array2<f32>,
    ) -> (Array2<f32>, Array2<f32>) {
        use ndarray::concatenate;
        use ndarray::Axis;
        
        // å…¥åŠ›ã¨éš ã‚ŒçŠ¶æ…‹ã‚’çµåˆ
        let concat = concatenate![Axis(1), *h_prev, *x];
        
        // 4ã¤ã®ã‚²ãƒ¼ãƒˆè¨ˆç®—
        let f_t = sigmoid(&(concat.dot(&self.w_f.t()) + &self.b_f));
        let i_t = sigmoid(&(concat.dot(&self.w_i.t()) + &self.b_i));
        let c_tilde = tanh(&(concat.dot(&self.w_c.t()) + &self.b_c));
        let o_t = sigmoid(&(concat.dot(&self.w_o.t()) + &self.b_o));
        
        // ã‚»ãƒ«çŠ¶æ…‹æ›´æ–°
        let c_t = &f_t * c_prev + &i_t * &c_tilde;
        
        // éš ã‚ŒçŠ¶æ…‹æ›´æ–°
        let h_t = &o_t * &tanh(&c_t);
        
        (h_t, c_t)
    }
}

fn sigmoid(x: &Array2<f32>) -> Array2<f32> {
    x.mapv(|v| 1.0 / (1.0 + (-v).exp()))
}

fn tanh(x: &Array2<f32>) -> Array2<f32> {
    x.mapv(|v| v.tanh())
}
```

### LSTM ã® GPU æœ€é©åŒ–

**å•é¡Œç‚¹**: LSTM ã¯é€æ¬¡çš„ã§ã€æ™‚åˆ» t ã®è¨ˆç®—ã¯ t-1 ã«ä¾å­˜ã™ã‚‹ãŸã‚ã€ä¸¦åˆ—åŒ–ãŒå›°é›£ã€‚

**æœ€é©åŒ–æˆ¦ç•¥**:

|| æ‰‹æ³• | èª¬æ˜ | åŠ¹æœ |
||------|------|------|
|| **ãƒãƒƒãƒä¸¦åˆ—åŒ–** | ãƒãƒƒãƒæ–¹å‘ã§ä¸¦åˆ—åŒ– | ä¸­ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºä¾å­˜ï¼‰ |
|| **å±¤ä¸¦åˆ—åŒ–** | å¤šå±¤ LSTM ã®å„å±¤ã‚’ä¸¦åˆ—å®Ÿè¡Œ | å° |
|| **ã‚«ãƒ¼ãƒãƒ«èåˆ** | 4ã¤ã®ã‚²ãƒ¼ãƒˆè¨ˆç®—ã‚’1ã‚«ãƒ¼ãƒãƒ«ã« | å¤§ï¼ˆ2-3xï¼‰ |
|| **æ°¸ç¶šã‚«ãƒ¼ãƒãƒ«** | ã‚»ãƒ«çŠ¶æ…‹ã‚’ GPU å¸¸é§ | ä¸­ï¼ˆ1.5xï¼‰ |
|| **cuDNN LSTM** | NVIDIA æœ€é©åŒ–å®Ÿè£… | æ¥µå¤§ï¼ˆ5-10xï¼‰ |

### ã‚«ãƒ¼ãƒãƒ«èåˆã«ã‚ˆã‚‹æœ€é©åŒ–

**èåˆå‰**ï¼ˆ4å›ã®ã‚«ãƒ¼ãƒãƒ«èµ·å‹•ï¼‰:

```python
# éåŠ¹ç‡ï¼š4å›ã®ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹
f_t = sigmoid(concat @ W_f.T + b_f)
i_t = sigmoid(concat @ W_i.T + b_i)
C_tilde = tanh(concat @ W_C.T + b_C)
o_t = sigmoid(concat @ W_o.T + b_o)
```

**èåˆå¾Œ**ï¼ˆ1å›ã®ã‚«ãƒ¼ãƒãƒ«èµ·å‹•ï¼‰:

```python
# åŠ¹ç‡çš„ï¼š4ã¤ã®ã‚²ãƒ¼ãƒˆã‚’åŒæ™‚è¨ˆç®—
# W_all = [W_f; W_i; W_C; W_o]  # é‡ã¿ã‚’çµåˆ
gates = concat @ W_all.T + b_all  # 1å›ã® GEMM
f_t = sigmoid(gates[:, :hidden])
i_t = sigmoid(gates[:, hidden:2*hidden])
C_tilde = tanh(gates[:, 2*hidden:3*hidden])
o_t = sigmoid(gates[:, 3*hidden:])
```

### Rust ã§ã®èåˆ LSTM

```rust
pub struct LSTMCellFused {
    w_all: Array2<f32>,  // (4*hidden, input+hidden)
    b_all: Array1<f32>,  // (4*hidden,)
    hidden_size: usize,
}

impl LSTMCellFused {
    pub fn forward(
        &self,
        x: &ArrayView2<f32>,
        h_prev: &Array2<f32>,
        c_prev: &Array2<f32>,
    ) -> (Array2<f32>, Array2<f32>) {
        let concat = concatenate![Axis(1), *h_prev, *x];
        
        // 1å›ã® GEMM ã§ 4ã¤ã®ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®—
        let gates = concat.dot(&self.w_all.t()) + &self.b_all;
        
        let h = self.hidden_size;
        let f_t = sigmoid(&gates.slice(s![.., 0..h]).to_owned());
        let i_t = sigmoid(&gates.slice(s![.., h..2*h]).to_owned());
        let c_tilde = tanh(&gates.slice(s![.., 2*h..3*h]).to_owned());
        let o_t = sigmoid(&gates.slice(s![.., 3*h..]).to_owned());
        
        let c_t = &f_t * c_prev + &i_t * &c_tilde;
        let h_t = &o_t * &tanh(&c_t);
        
        (h_t, c_t)
    }
}
```

### GRUï¼ˆGated Recurrent Unitï¼‰

**GRU** [^7] ã¯ LSTM ã®ç°¡ç•¥ç‰ˆã§ã€ã‚²ãƒ¼ãƒˆæ•°ãŒå°‘ãªãè¨ˆç®—åŠ¹ç‡ãŒè‰¯ã„ã€‚

[^7]: Cho, K., et al. (2014). "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation." EMNLP.

**æ•°å¼**:

\[
\begin{align}
r_t &= \sigma(W_r \cdot [h*{t-1}, x_t]) \quad \text{ï¼ˆãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆï¼‰} \\\\
z_t &= \sigma(W_z \cdot [h*{t-1}, x_t]) \quad \text{ï¼ˆæ›´æ–°ã‚²ãƒ¼ãƒˆï¼‰} \\\\
\tilde{h}_t &= \tanh(W_h \cdot [r_t \odot h*{t-1}, x_t]) \quad \text{ï¼ˆå€™è£œéš ã‚ŒçŠ¶æ…‹ï¼‰} \\\\
h_t &= (1 - z_t) \odot h*{t-1} + z_t \odot \tilde{h}_t \quad \text{ï¼ˆéš ã‚ŒçŠ¶æ…‹ï¼‰}
\end{align}
\]

**LSTM vs GRU**:

|| é …ç›® | LSTM | GRU |
||------|------|-----|
|| ã‚²ãƒ¼ãƒˆæ•° | 4 | 2 |
|| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | å¤šã„ | å°‘ãªã„ï¼ˆç´„25%å‰Šæ¸›ï¼‰ |
|| è¨ˆç®—é‡ | å¤šã„ | å°‘ãªã„ |
|| è¡¨ç¾åŠ› | é«˜ã„ | ã‚„ã‚„ä½ã„ |
|| å­¦ç¿’é€Ÿåº¦ | é…ã„ | é€Ÿã„ |
|| ç”¨é€” | é•·ç³»åˆ—ã€è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ | çŸ­ä¸­ç³»åˆ—ã€é«˜é€Ÿæ¨è«– |

### Python ã§ã® GRU å®Ÿè£…

```python
class GRUCell:
    def __init__(self, input_size, hidden_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        scale = np.sqrt(2.0 / (input_size + hidden_size))
        self.W_r = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_r = np.zeros(hidden_size)
        
        self.W_z = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_z = np.zeros(hidden_size)
        
        self.W_h = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_h = np.zeros(hidden_size)
    
    def forward(self, x, h_prev):
        """
        x: (batch, input_size)
        h_prev: (batch, hidden_size)
        """
        concat = np.concatenate([h_prev, x], axis=1)
        
        # ãƒªã‚»ãƒƒãƒˆã‚²ãƒ¼ãƒˆã¨æ›´æ–°ã‚²ãƒ¼ãƒˆ
        r_t = sigmoid(concat @ self.W_r.T + self.b_r)
        z_t = sigmoid(concat @ self.W_z.T + self.b_z)
        
        # ãƒªã‚»ãƒƒãƒˆå¾Œã®çµåˆ
        concat_reset = np.concatenate([r_t * h_prev, x], axis=1)
        
        # å€™è£œéš ã‚ŒçŠ¶æ…‹
        h_tilde = np.tanh(concat_reset @ self.W_h.T + self.b_h)
        
        # éš ã‚ŒçŠ¶æ…‹æ›´æ–°
        h_t = (1 - z_t) * h_prev + z_t * h_tilde
        
        return h_t
```

### cuDNN ã§ã® LSTM/GRU

**PyTorch** ã¯å†…éƒ¨ã§ cuDNN ã®é«˜åº¦ã«æœ€é©åŒ–ã•ã‚ŒãŸå®Ÿè£…ã‚’ä½¿ã„ã¾ã™:

```python
import torch
import torch.nn as nn

# cuDNN LSTMï¼ˆè‡ªå‹•æœ€é©åŒ–ï¼‰
lstm = nn.LSTM(input_size=128, hidden_size=256, num_layers=2, batch_first=True).cuda()

# å…¥åŠ›
x = torch.randn(32, 50, 128).cuda()  # (batch, seq_len, input_size)

# é †ä¼æ’­
output, (h_n, c_n) = lstm(x)

print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")  # (32, 50, 256)
```

**Rustï¼ˆtch-rsï¼‰ã§ã® cuDNN LSTM**:

```rust
use tch::{nn, nn::Module, Device, Tensor};

fn main() {
    let device = Device::Cuda(0);
    let vs = nn::VarStore::new(device);
    
    // LSTMå±¤ï¼ˆcuDNNä½¿ç”¨ï¼‰
    let lstm = nn::lstm(&vs.root(), 128, 256, nn::RNNConfig {
        num_layers: 2,
        batch_first: true,
        ..Default::default()
    });
    
    // å…¥åŠ›
    let x = Tensor::randn(&[32, 50, 128], (tch::Kind::Float, device));
    
    // é †ä¼æ’­
    let (output, _) = lstm.seq(&x);
    
    println!("å‡ºåŠ›å½¢çŠ¶: {:?}", output.size());  // [32, 50, 256]
}
```

### ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–

**å•é¡Œ**: RNN ã¯æ™‚ç³»åˆ—æ–¹å‘ã®ä¾å­˜æ€§ã‹ã‚‰ã€ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãŒéåŠ¹ç‡ã€‚

**æœ€é©åŒ–1: ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®å¤‰æ›´**

```python
# éåŠ¹ç‡: (batch, seq_len, hidden) â†’ æ™‚ç³»åˆ—ã§ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã‚¢ã‚¯ã‚»ã‚¹
x = np.random.randn(32, 100, 256)  

# åŠ¹ç‡çš„: (seq_len, batch, hidden) â†’ é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹
x = np.random.randn(100, 32, 256)
```

**æœ€é©åŒ–2: ãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒª**

```python
import torch

# CPU â†’ GPU è»¢é€ã‚’é«˜é€ŸåŒ–
x_pinned = torch.randn(32, 100, 256, pin_memory=True)
x_gpu = x_pinned.cuda(non_blocking=True)
```

### æ€§èƒ½æ¯”è¼ƒ

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**ï¼ˆãƒãƒƒãƒ32ã€ç³»åˆ—é•·100ã€éš ã‚Œå±¤256ï¼‰:

|| å®Ÿè£… | æ™‚é–“ï¼ˆmsï¼‰ | ç›¸å¯¾é€Ÿåº¦ |
||------|-----------|---------|
|| NumPyï¼ˆCPUï¼‰ | 2450 | 1x |
|| PyTorchï¼ˆCPUï¼‰ | 145 | 16.9x |
|| PyTorchï¼ˆCUDAã€èåˆãªã—ï¼‰ | 12.3 | 199x |
|| PyTorchï¼ˆcuDNNï¼‰ | 2.8 | 875x |

**ã¾ã¨ã‚**: cuDNN ã®æœ€é©åŒ–ã¯æ¥µã‚ã¦åŠ¹æœçš„ã€‚Rust ã§æœ€é«˜æ€§èƒ½ã‚’å¾—ã‚‹ã«ã¯ tch-rs çµŒç”±ã§ cuDNN ã‚’ä½¿ã†ã®ãŒå®Ÿç”¨çš„ã€‚

## 16.4 Transformer ã®æ³¨æ„æ©Ÿæ§‹ã‚’ Rust ã§æ§‹ç¯‰

**Transformer** [^8] ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ã¨ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã‚’é©æ–°ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚æ ¸å¿ƒã¯ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆSelf-Attentionï¼‰æ©Ÿæ§‹ã«ã‚ã‚Šã¾ã™ã€‚

[^8]: Vaswani, A., et al. (2017). "Attention Is All You Need." NeurIPS.

### ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆSelf-Attentionï¼‰ã®æ•°å­¦

**ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**ï¼ˆScaled Dot-Product Attentionï¼‰:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

**è¨˜å·**:

|| è¨˜å· | æ„å‘³ | å½¢çŠ¶ |
||------|------|------|
|| \(Q\) | ã‚¯ã‚¨ãƒªï¼ˆQueryï¼‰ | \((B, N, d_k)\) |
|| \(K\) | ã‚­ãƒ¼ï¼ˆKeyï¼‰ | \((B, N, d_k)\) |
|| \(V\) | ãƒãƒªãƒ¥ãƒ¼ï¼ˆValueï¼‰ | \((B, N, d_v)\) |
|| \(N\) | ç³»åˆ—é•·ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼‰ | - |
|| \(d_k\) | ã‚­ãƒ¼æ¬¡å…ƒ | é€šå¸¸64 |
|| \(d_v\) | ãƒãƒªãƒ¥ãƒ¼æ¬¡å…ƒ | é€šå¸¸64 |
|| \(B\) | ãƒãƒƒãƒã‚µã‚¤ã‚º | - |

**è¨ˆç®—æ‰‹é †**:

1. **ã‚¹ã‚³ã‚¢è¨ˆç®—**: \(S = QK^T\) ... \((B, N, N)\)
2. **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: \(S = S / \sqrt{d_k}\)
3. **ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹**: \(A = \text{softmax}(S)\) ... \((B, N, N)\)
4. **é‡ã¿ä»˜ã‘å’Œ**: \(O = AV\) ... \((B, N, d_v)\)

**è¨ˆç®—é‡åˆ†æ**:

- \(QK^T\): \(O(BN^2d_k)\) FLOPS
- ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹: \(O(BN^2)\) FLOPS
- \(AV\): \(O(BN^2d_v)\) FLOPS
- **åˆè¨ˆ**: \(O(BN^2d)\) FLOPSï¼ˆ\(d = d_k = d_v\)ï¼‰

**ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: \(N^2\) ã®ãƒ¡ãƒ¢ãƒªè¤‡é›‘åº¦ï¼ˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ï¼‰

### Pythonï¼ˆNumPyï¼‰ã§ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…

```python
import numpy as np

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
    Q: (batch, n_heads, seq_len, d_k)
    K: (batch, n_heads, seq_len, d_k)
    V: (batch, n_heads, seq_len, d_v)
    """
    d_k = Q.shape[-1]
    
    # ã‚¹ã‚³ã‚¢è¨ˆç®—
    scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(d_k)  # (B, H, N, N)
    
    # ãƒã‚¹ã‚¯é©ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if mask is not None:
        scores = scores + mask * -1e9
    
    # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
    attention_weights = softmax(scores, axis=-1)  # (B, H, N, N)
    
    # é‡ã¿ä»˜ã‘å’Œ
    output = attention_weights @ V  # (B, H, N, d_v)
    
    return output, attention_weights

def softmax(x, axis=-1):
    """æ•°å€¤å®‰å®šãªã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹"""
    x_max = np.max(x, axis=axis, keepdims=True)
    exp_x = np.exp(x - x_max)
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

# ä½¿ç”¨ä¾‹
batch = 2
n_heads = 8
seq_len = 128
d_k = 64

Q = np.random.randn(batch, n_heads, seq_len, d_k)
K = np.random.randn(batch, n_heads, seq_len, d_k)
V = np.random.randn(batch, n_heads, seq_len, d_k)

output, weights = scaled_dot_product_attention(Q, K, V)
print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")  # (2, 8, 128, 64)
print(f"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿å½¢çŠ¶: {weights.shape}")  # (2, 8, 128, 128)
```

### ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆMulti-Head Attentionï¼‰

**è¤‡æ•°ã®æ³¨æ„æ©Ÿæ§‹**ã‚’ä¸¦åˆ—ã«å®Ÿè¡Œã—ã€ç•°ãªã‚‹è¡¨ç¾éƒ¨åˆ†ç©ºé–“ã‚’å­¦ç¿’ã—ã¾ã™ã€‚

\[
\begin{align}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}\_1, \ldots, \text{head}\_h)W^O \\\\
\text{head}\_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align}
\]

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:

|| è¡Œåˆ— | å½¢çŠ¶ | å½¹å‰² |
||------|------|------|
|| \(W_i^Q\) | \((d*{model}, d_k)\) | ã‚¯ã‚¨ãƒªå°„å½± |
|| \(W_i^K\) | \((d*{model}, d_k)\) | ã‚­ãƒ¼å°„å½± |
|| \(W_i^V\) | \((d*{model}, d_v)\) | ãƒãƒªãƒ¥ãƒ¼å°„å½± |
|| \(W^O\) | \((h \cdot d_v, d*{model})\) | å‡ºåŠ›å°„å½± |

### Python ã§ã®ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

```python
class MultiHeadAttention:
    def __init__(self, d_model, n_heads):
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # é‡ã¿è¡Œåˆ—ï¼ˆç°¡ç•¥åŒ–ã®ãŸã‚ Xavier åˆæœŸåŒ–ï¼‰
        self.W_Q = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.W_K = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.W_V = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.W_O = np.random.randn(d_model, d_model) / np.sqrt(d_model)
    
    def split_heads(self, x):
        """
        (batch, seq_len, d_model) â†’ (batch, n_heads, seq_len, d_k)
        """
        batch, seq_len, _ = x.shape
        x = x.reshape(batch, seq_len, self.n_heads, self.d_k)
        return x.transpose(0, 2, 1, 3)
    
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.shape[0]
        
        # ç·šå½¢å¤‰æ›
        Q = Q @ self.W_Q  # (B, N, d_model)
        K = K @ self.W_K
        V = V @ self.W_V
        
        # ãƒ˜ãƒƒãƒ‰åˆ†å‰²
        Q = self.split_heads(Q)  # (B, H, N, d_k)
        K = self.split_heads(K)
        V = self.split_heads(V)
        
        # ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
        output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)
        
        # ãƒ˜ãƒƒãƒ‰çµåˆ
        output = output.transpose(0, 2, 1, 3)  # (B, N, H, d_k)
        output = output.reshape(batch_size, -1, self.d_model)  # (B, N, d_model)
        
        # å‡ºåŠ›å°„å½±
        output = output @ self.W_O
        
        return output, attention_weights

# ä½¿ç”¨ä¾‹
d_model = 512
n_heads = 8
seq_len = 128
batch = 2

mha = MultiHeadAttention(d_model, n_heads)

# è‡ªå·±æ³¨æ„æ©Ÿæ§‹ï¼ˆQ=K=Vï¼‰
x = np.random.randn(batch, seq_len, d_model)
output, weights = mha.forward(x, x, x)

print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")  # (2, 128, 512)
```

### Rust ã§ã®ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

```rust
use ndarray::{Array2, Array3, Array4, s};
use ndarray_linalg::Dot;

pub struct MultiHeadAttention {
    d_model: usize,
    n_heads: usize,
    d_k: usize,
    w_q: Array2<f32>,
    w_k: Array2<f32>,
    w_v: Array2<f32>,
    w_o: Array2<f32>,
}

impl MultiHeadAttention {
    pub fn new(d_model: usize, n_heads: usize) -> Self {
        assert_eq!(d_model % n_heads, 0);
        
        let d_k = d_model / n_heads;
        let scale = (d_model as f32).sqrt();
        
        use ndarray_rand::RandomExt;
        use ndarray_rand::rand_distr::StandardNormal;
        
        Self {
            d_model,
            n_heads,
            d_k,
            w_q: Array2::random((d_model, d_model), StandardNormal) / scale,
            w_k: Array2::random((d_model, d_model), StandardNormal) / scale,
            w_v: Array2::random((d_model, d_model), StandardNormal) / scale,
            w_o: Array2::random((d_model, d_model), StandardNormal) / scale,
        }
    }
    
    fn split_heads(&self, x: Array3<f32>) -> Array4<f32> {
        let (batch, seq_len, _) = x.dim();
        
        // (B, N, d_model) â†’ (B, N, H, d_k) â†’ (B, H, N, d_k)
        x.into_shape((batch, seq_len, self.n_heads, self.d_k)).unwrap()
         .permuted_axes([0, 2, 1, 3])
         .to_owned()
    }
    
    pub fn forward(
        &self,
        q: &Array3<f32>,
        k: &Array3<f32>,
        v: &Array3<f32>,
    ) -> Array3<f32> {
        let batch_size = q.dim().0;
        let seq_len = q.dim().1;
        
        // ç·šå½¢å¤‰æ›
        let q_proj = self.project_3d(q, &self.w_q);
        let k_proj = self.project_3d(k, &self.w_k);
        let v_proj = self.project_3d(v, &self.w_v);
        
        // ãƒ˜ãƒƒãƒ‰åˆ†å‰²
        let q_heads = self.split_heads(q_proj);  // (B, H, N, d_k)
        let k_heads = self.split_heads(k_proj);
        let v_heads = self.split_heads(v_proj);
        
        // ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
        let output_heads = self.scaled_dot_product_attention(
            &q_heads, &k_heads, &v_heads
        );
        
        // ãƒ˜ãƒƒãƒ‰çµåˆ
        let output = output_heads
            .permuted_axes([0, 2, 1, 3])  // (B, N, H, d_k)
            .into_shape((batch_size, seq_len, self.d_model)).unwrap();
        
        // å‡ºåŠ›å°„å½±
        self.project_3d(&output, &self.w_o)
    }
    
    fn project_3d(&self, x: &Array3<f32>, weight: &Array2<f32>) -> Array3<f32> {
        let (batch, seq_len, d_in) = x.dim();
        let d_out = weight.dim().0;
        
        let x_2d = x.to_shape((batch * seq_len, d_in)).unwrap();
        let proj_2d = x_2d.dot(weight);
        proj_2d.into_shape((batch, seq_len, d_out)).unwrap()
    }
    
    fn scaled_dot_product_attention(
        &self,
        q: &Array4<f32>,  // (B, H, N, d_k)
        k: &Array4<f32>,
        v: &Array4<f32>,
    ) -> Array4<f32> {
        let (batch, n_heads, seq_len, d_k) = q.dim();
        let scale = (d_k as f32).sqrt();
        
        let mut output = Array4::<f32>::zeros((batch, n_heads, seq_len, d_k));
        
        for b in 0..batch {
            for h in 0..n_heads {
                let q_slice = q.slice(s![b, h, .., ..]).to_owned();
                let k_slice = k.slice(s![b, h, .., ..]).to_owned();
                let v_slice = v.slice(s![b, h, .., ..]).to_owned();
                
                // ã‚¹ã‚³ã‚¢è¨ˆç®—: Q @ K^T / sqrt(d_k)
                let scores = q_slice.dot(&k_slice.t()) / scale;
                
                // ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
                let attention_weights = self.softmax(&scores);
                
                // é‡ã¿ä»˜ã‘å’Œ: A @ V
                let out_slice = attention_weights.dot(&v_slice);
                output.slice_mut(s![b, h, .., ..]).assign(&out_slice);
            }
        }
        
        output
    }
    
    fn softmax(&self, x: &Array2<f32>) -> Array2<f32> {
        let x_max = x.fold_axis(ndarray::Axis(1), f32::NEG_INFINITY, |&a, &b| a.max(b));
        let exp_x = (x - &x_max.insert_axis(ndarray::Axis(1))).mapv(|v| v.exp());
        let sum_exp = exp_x.sum_axis(ndarray::Axis(1));
        exp_x / sum_exp.insert_axis(ndarray::Axis(1))
    }
}
```

### Transformer å±¤ã®å®Œå…¨å®Ÿè£…

```python
class TransformerLayer:
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        self.mha = MultiHeadAttention(d_model, n_heads)
        self.ffn = FeedForward(d_model, d_ff)
        self.layernorm1 = LayerNorm(d_model)
        self.layernorm2 = LayerNorm(d_model)
        self.dropout = dropout
    
    def forward(self, x, mask=None):
        # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ + æ®‹å·®æ¥ç¶š + LayerNorm
        attn_output, _ = self.mha.forward(x, x, x, mask)
        x = self.layernorm1(x + attn_output)
        
        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ + æ®‹å·®æ¥ç¶š + LayerNorm
        ffn_output = self.ffn.forward(x)
        x = self.layernorm2(x + ffn_output)
        
        return x

class FeedForward:
    def __init__(self, d_model, d_ff):
        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)
        self.b1 = np.zeros(d_ff)
        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)
        self.b2 = np.zeros(d_model)
    
    def forward(self, x):
        # x: (B, N, d_model)
        hidden = np.maximum(0, x @ self.W1 + self.b1)  # ReLU
        output = hidden @ self.W2 + self.b2
        return output

class LayerNorm:
    def __init__(self, d_model, eps=1e-6):
        self.gamma = np.ones(d_model)
        self.beta = np.zeros(d_model)
        self.eps = eps
    
    def __call__(self, x):
        mean = x.mean(axis=-1, keepdims=True)
        var = x.var(axis=-1, keepdims=True)
        x_norm = (x - mean) / np.sqrt(var + self.eps)
        return self.gamma * x_norm + self.beta
```

### ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ãƒ¡ãƒ¢ãƒªå•é¡Œ

**å•é¡Œ**: \(N \times N\) ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã¯ãƒ¡ãƒ¢ãƒªã‚’å¤§é‡æ¶ˆè²»ã€‚

**å…·ä½“ä¾‹**ï¼ˆGPT-3è¦æ¨¡ï¼‰:

- ç³»åˆ—é•· \(N = 2048\)
- ãƒãƒƒãƒã‚µã‚¤ã‚º \(B = 8\)
- ãƒ˜ãƒƒãƒ‰æ•° \(H = 96\)
- ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—: \(8 \times 96 \times 2048 \times 2048 \times 4\) bytes â‰ˆ **12 GB**

**è§£æ±ºç­–**: Flash Attentionï¼ˆæ¬¡ç¯€ï¼‰

## 16.5 Flash Attention / Multi-Query Attention ã®å®Ÿè£…

### Flash Attention ã®åŸç†

**Flash Attention** [^9] ã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—ã‚’**IOåŠ¹ç‡çš„**ã«è¡Œã†ç”»æœŸçš„ãªæ‰‹æ³•ã§ã™ã€‚

[^9]: Dao, T., et al. (2022). "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness." NeurIPS.

**å¾“æ¥ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**:

1. \(S = QK^T\) ã‚’è¨ˆç®— â†’ HBMï¼ˆHigh Bandwidth Memoryï¼‰ã«ä¿å­˜
2. \(P = \text{softmax}(S)\) ã‚’è¨ˆç®— â†’ HBM ã«ä¿å­˜
3. \(O = PV\) ã‚’è¨ˆç®—

**ãƒ¡ãƒ¢ãƒªæ›¸ãè¾¼ã¿**: \(O(BN^2)\) bytes

**Flash Attention ã®æ”¹å–„**:

- **ã‚¿ã‚¤ãƒ«åˆ†å‰²**ï¼ˆTilingï¼‰: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã‚’å°ã•ã„ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²
- **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹**ï¼ˆOnline Softmaxï¼‰: ä¸­é–“çµæœã‚’SRAMï¼ˆé«˜é€Ÿãƒ¡ãƒ¢ãƒªï¼‰ã«ä¿æŒ
- **å†è¨ˆç®—**ï¼ˆRecomputationï¼‰: é€†ä¼æ’­æ™‚ã«ä¸­é–“çµæœã‚’å†è¨ˆç®—

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: \(O(BN^2) \rightarrow O(BN)\)

**é€Ÿåº¦å‘ä¸Š**: 2-4å€ï¼ˆGPUãƒ¢ãƒ‡ãƒ«ã«ä¾å­˜ï¼‰

### Flash Attention ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**ã‚­ãƒ¼ã‚¢ã‚¤ãƒ‡ã‚¢**: ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’å¢—åˆ†çš„ã«è¨ˆç®—ã€‚

\[
\text{softmax}(x_1, x_2) = \frac{e^{x_1} + e^{x_2}}{e^{x_1} + e^{x_2}} = \frac{1}{1 + e^{x_2 - x_1}} \text{ (for } x_1)
\]

**ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®æ›´æ–°å¼**:

\[
\begin{align}
m^{(j)} &= \max(m^{(j-1)}, \max(S^{(j)})) \\\\
l^{(j)} &= e^{m^{(j-1)} - m^{(j)}} l^{(j-1)} + \sum_i e^{S_i^{(j)} - m^{(j)}} \\\\
O^{(j)} &= \frac{e^{m^{(j-1)} - m^{(j)}} l^{(j-1)} O^{(j-1)} + \sum_i e^{S_i^{(j)} - m^{(j)}} V_i}{l^{(j)}}
\end{align}
\]

### Python ã§ã® Flash Attention é¢¨å®Ÿè£…ï¼ˆæ¦‚å¿µçš„ï¼‰

```python
def flash_attention_tiled(Q, K, V, block_size=64):
    """
    Flash Attention ã®ç°¡ç•¥ç‰ˆï¼ˆæ•™è‚²ç›®çš„ï¼‰
    å®Ÿéš›ã®å®Ÿè£…ã¯CUDAã‚«ãƒ¼ãƒãƒ«ã§è¡Œã‚ã‚Œã‚‹
    """
    B, H, N, d = Q.shape
    num_blocks = (N + block_size - 1) // block_size
    
    O = np.zeros_like(Q)
    l = np.zeros((B, H, N))  # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ­£è¦åŒ–é …
    m = np.full((B, H, N), -np.inf)  # æœ€å¤§å€¤
    
    for i in range(num_blocks):
        # Qã®ãƒ–ãƒ­ãƒƒã‚¯
        q_start = i * block_size
        q_end = min((i + 1) * block_size, N)
        Q_block = Q[:, :, q_start:q_end, :]
        
        for j in range(num_blocks):
            # K, Vã®ãƒ–ãƒ­ãƒƒã‚¯
            kv_start = j * block_size
            kv_end = min((j + 1) * block_size, N)
            K_block = K[:, :, kv_start:kv_end, :]
            V_block = V[:, :, kv_start:kv_end, :]
            
            # ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
            S_block = Q_block @ K_block.transpose(0, 1, 3, 2) / np.sqrt(d)
            
            # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ›´æ–°
            m_new = np.maximum(m[:, :, q_start:q_end], 
                              np.max(S_block, axis=-1))
            
            exp_scores = np.exp(S_block - m_new[..., None])
            l_new = (np.exp(m[:, :, q_start:q_end] - m_new) * 
                    l[:, :, q_start:q_end] + 
                    np.sum(exp_scores, axis=-1))
            
            # å‡ºåŠ›æ›´æ–°
            O[:, :, q_start:q_end, :] = (
                (np.exp(m[:, :, q_start:q_end] - m_new)[..., None] *
                 l[:, :, q_start:q_end][..., None] *
                 O[:, :, q_start:q_end, :] +
                 exp_scores @ V_block) / l_new[..., None]
            )
            
            m[:, :, q_start:q_end] = m_new
            l[:, :, q_start:q_end] = l_new
    
    return O

# ä½¿ç”¨ä¾‹
Q = np.random.randn(2, 8, 512, 64)
K = np.random.randn(2, 8, 512, 64)
V = np.random.randn(2, 8, 512, 64)

# æ¨™æº–ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
output_standard = scaled_dot_product_attention(Q, K, V)[0]

# Flash Attention
output_flash = flash_attention_tiled(Q, K, V, block_size=64)

# èª¤å·®ç¢ºèª
print(f"èª¤å·®: {np.abs(output_standard - output_flash).max():.6f}")
```

### PyTorch ã§ã® Flash Attention ä½¿ç”¨

**PyTorch 2.0+** ã¯ Flash Attention ã‚’æ¨™æº–ã‚µãƒãƒ¼ãƒˆ:

```python
import torch
import torch.nn.functional as F

# Flash Attention ã‚’ä½¿ã£ãŸå®Ÿè£…
def attention_flash(Q, K, V):
    # PyTorch 2.0+ ã®æœ€é©åŒ–ã•ã‚ŒãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
    output = F.scaled_dot_product_attention(
        Q, K, V,
        attn_mask=None,
        dropout_p=0.0,
        is_causal=False
    )
    return output

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
batch = 8
n_heads = 12
seq_len = 2048
d_k = 64

Q = torch.randn(batch, n_heads, seq_len, d_k).cuda()
K = torch.randn(batch, n_heads, seq_len, d_k).cuda()
V = torch.randn(batch, n_heads, seq_len, d_k).cuda()

import time

# æ¨™æº–ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
torch.cuda.synchronize()
start = time.time()
out_standard = (Q @ K.transpose(-2, -1) / (d_k ** 0.5)).softmax(dim=-1) @ V
torch.cuda.synchronize()
time_standard = time.time() - start

# Flash Attention
torch.cuda.synchronize()
start = time.time()
out_flash = F.scaled_dot_product_attention(Q, K, V)
torch.cuda.synchronize()
time_flash = time.time() - start

print(f"æ¨™æº–ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: {time_standard*1000:.2f} ms, ãƒ¡ãƒ¢ãƒª: {torch.cuda.max_memory_allocated()/1e9:.2f} GB")
print(f"Flash Attention: {time_flash*1000:.2f} ms, ãƒ¡ãƒ¢ãƒª: {torch.cuda.max_memory_allocated()/1e9:.2f} GB")
print(f"é«˜é€ŸåŒ–ç‡: {time_standard/time_flash:.2f}x")
# å‡ºåŠ›ä¾‹ï¼ˆA100 GPUï¼‰:
# æ¨™æº–ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³: 45.2 ms, ãƒ¡ãƒ¢ãƒª: 2.8 GB
# Flash Attention: 12.3 ms, ãƒ¡ãƒ¢ãƒª: 0.4 GB
# é«˜é€ŸåŒ–ç‡: 3.67x
```

### Multi-Query Attention (MQA)

**Multi-Query Attention** [^10] ã¯ã€ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’å…¨ãƒ˜ãƒƒãƒ‰ã§å…±æœ‰ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚

[^10]: Shazeer, N. (2019). "Fast Transformer Decoding: One Write-Head is All You Need." arXiv.

**æ¨™æº–ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**:

- Q: \((B, H, N, d_k)\)
- K: \((B, H, N, d_k)\)  â† ãƒ˜ãƒƒãƒ‰ã”ã¨ã«ç•°ãªã‚‹
- V: \((B, H, N, d_v)\)  â† ãƒ˜ãƒƒãƒ‰ã”ã¨ã«ç•°ãªã‚‹

**Multi-Query Attention**:

- Q: \((B, H, N, d_k)\)
- K: \((B, 1, N, d_k)\)  â† å…¨ãƒ˜ãƒƒãƒ‰ã§å…±æœ‰
- V: \((B, 1, N, d_v)\)  â† å…¨ãƒ˜ãƒƒãƒ‰ã§å…±æœ‰

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒ \(1/H\) ã«å‰Šæ¸›ï¼ˆæ¨è«–æ™‚ã«é‡è¦ï¼‰

**æ€§èƒ½**: ç²¾åº¦ã¯ã‚ãšã‹ã«ä½ä¸‹ï¼ˆ~1%ï¼‰ã€é€Ÿåº¦ã¯å‘ä¸Šï¼ˆ1.5-2xï¼‰

### Python ã§ã® MQA å®Ÿè£…

```python
class MultiQueryAttention:
    def __init__(self, d_model, n_heads):
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_Q = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.W_K = np.random.randn(d_model, self.d_k) / np.sqrt(d_model)  # å˜ä¸€K
        self.W_V = np.random.randn(d_model, self.d_k) / np.sqrt(d_model)  # å˜ä¸€V
        self.W_O = np.random.randn(d_model, d_model) / np.sqrt(d_model)
    
    def forward(self, Q, K, V):
        batch, seq_len, _ = Q.shape
        
        # ã‚¯ã‚¨ãƒªã¯è¤‡æ•°ãƒ˜ãƒƒãƒ‰
        Q_proj = (Q @ self.W_Q).reshape(batch, seq_len, self.n_heads, self.d_k)
        Q_proj = Q_proj.transpose(0, 2, 1, 3)  # (B, H, N, d_k)
        
        # ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã¯å˜ä¸€ï¼ˆå…±æœ‰ï¼‰
        K_proj = K @ self.W_K  # (B, N, d_k)
        V_proj = V @ self.W_V  # (B, N, d_k)
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—
        scores = Q_proj @ K_proj.transpose(0, 2, 1) / np.sqrt(self.d_k)
        attention = softmax(scores, axis=-1)  # (B, H, N, N)
        
        # å…¨ãƒ˜ãƒƒãƒ‰ã§åŒã˜Vã‚’ä½¿ç”¨
        output = attention @ V_proj[:, None, :, :]  # (B, H, N, d_k)
        
        # ãƒ˜ãƒƒãƒ‰çµåˆ
        output = output.transpose(0, 2, 1, 3).reshape(batch, seq_len, self.d_model)
        return output @ self.W_O
```

### Rust ã§ã® Flash Attention çµ±åˆï¼ˆtch-rsçµŒç”±ï¼‰

```rust
use tch::{nn, Tensor, Device};

fn flash_attention_rust() {
    let device = Device::Cuda(0);
    
    let q = Tensor::randn(&[8, 12, 2048, 64], (tch::Kind::Float, device));
    let k = Tensor::randn(&[8, 12, 2048, 64], (tch::Kind::Float, device));
    let v = Tensor::randn(&[8, 12, 2048, 64], (tch::Kind::Float, device));
    
    // PyTorch 2.0+ ã® Flash Attention ã‚’ä½¿ç”¨
    let output = Tensor::scaled_dot_product_attention(
        &q, &k, &v,
        None,  // attn_mask
        0.0,   // dropout_p
        false, // is_causal
        None   // scale
    );
    
    println!("å‡ºåŠ›å½¢çŠ¶: {:?}", output.size());
}
```

## 16.6 æ¨è«–é€Ÿåº¦ãƒ»ãƒ¡ãƒ¢ãƒªæ¯”è¼ƒ

### CNN vs RNN vs Transformer ã®æ€§èƒ½æ¯”è¼ƒ

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¡ä»¶**:

- GPU: NVIDIA A100 (40GB)
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 32
- å…¥åŠ›: ç”»åƒ (224x224) or ç³»åˆ— (é•·ã•512)

**ç”»åƒåˆ†é¡ï¼ˆImageNetï¼‰**:

|| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | æ¨è«–æ™‚é–“ï¼ˆmsï¼‰ | ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰ | Top-1ç²¾åº¦ |
||--------|------------|--------------|-----------|----------|
|| ResNet-50 (CNN) | 25M | 8.2 | 1.2 | 76.2% |
|| ViT-B/16 (Transformer) | 86M | 15.4 | 3.8 | 81.8% |
|| ConvNeXt-B (CNNæ”¹) | 89M | 11.3 | 2.1 | 83.1% |

**ç³»åˆ—ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ï¼ˆè¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰**:

|| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | æ¨è«–æ™‚é–“ï¼ˆms/tokenï¼‰ | ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰ | Perplexity |
||--------|------------|-------------------|-----------|-----------|
|| LSTM (2å±¤) | 50M | 2.1 | 0.8 | 42.3 |
|| Transformer (6å±¤) | 65M | 1.2 | 2.4 | 28.5 |
|| Transformer + Flash Attn | 65M | 0.5 | 0.9 | 28.5 |

**ã¾ã¨ã‚**:

|| ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | é€Ÿåº¦ | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ | ç²¾åº¦ | ç”¨é€” |
||------------|------|----------|------|------|
|| **CNN** | æœ€é€Ÿ | é«˜ | ä¸­ã€œé«˜ | ç”»åƒã€å±€æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³ |
|| **RNN/LSTM** | é…ã„ | é«˜ | ä¸­ | æ™‚ç³»åˆ—ã€é€æ¬¡å‡¦ç† |
|| **Transformer** | ä¸­ | ä½ | æœ€é«˜ | é•·è·é›¢ä¾å­˜ã€ä¸¦åˆ—åŒ– |
|| **Transformer + Flash** | é€Ÿã„ | é«˜ | æœ€é«˜ | å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ« |

### å®Ÿè£…ã®é¸æŠæŒ‡é‡

|| ç”¨é€” | æ¨å¥¨å®Ÿè£… | ç†ç”± |
||------|---------|------|
|| **ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—** | Python + PyTorch | é–‹ç™ºé€Ÿåº¦ |
|| **ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆç”»åƒï¼‰** | Rust + tch-rs or C++ | ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· |
|| **ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆNLPï¼‰** | Python + Flash Attn | æœ€é©åŒ–æ¸ˆã¿ |
|| **çµ„ã¿è¾¼ã¿** | Rust + ONNX | çœãƒ¡ãƒ¢ãƒª |
|| **ã‚¯ãƒ©ã‚¦ãƒ‰æ¨è«–** | Python + TensorRT | é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ |

---

## å‚è€ƒæ–‡çŒ®

[^1] LeCun, Y., et al. (1989). "Backpropagation Applied to Handwritten Zip Code Recognition." Neural Computation.

[^2] Chellapilla, K., et al. (2006). "High Performance Convolutional Neural Networks for Document Processing." ICFHR.

[^3] Lavin, A., & Gray, S. (2016). "Fast Algorithms for Convolutional Neural Networks." CVPR.

[^4] Mathieu, M., et al. (2014). "Fast Training of Convolutional Networks through FFTs." ICLR.

[^5] cuDNN Developer Guide: https://docs.nvidia.com/deeplearning/cudnn/developer-guide/

[^6] Hochreiter, S., & Schmidhuber, J. (1997). "Long Short-Term Memory." Neural Computation.

[^7] Cho, K., et al. (2014). "Learning Phrase Representations using RNN Encoder-Decoder." EMNLP.

[^8] Vaswani, A., et al. (2017). "Attention Is All You Need." NeurIPS.

[^9] Dao, T., et al. (2022). "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness." NeurIPS.

[^10] Shazeer, N. (2019). "Fast Transformer Decoding: One Write-Head is All You Need." arXiv.
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬18ç« : ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ä¿¡é ¼æ€§](../05_ç¬¬Véƒ¨_å¿œç”¨ã¨é«˜åº¦åŒ–/05-18-ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ä¿¡é ¼æ€§.md) | [â¡ï¸ ç¬¬20ç« : ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æœ€é©åŒ–](06-20-ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æœ€é©åŒ–.md)