[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬20ç« ](06-20-ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æœ€é©åŒ–.md) | [â¡ï¸ ç¬¬22ç« ](../07_ç¬¬VIIéƒ¨_å±•æœ›ã¨è¨­è¨ˆæŒ‡é‡/07-22-Rustã§ã®MLã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®é€²åŒ–.md)

---

# ç¬¬ 18 ç« ã€€å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

ã“ã®ç« ã§ã¯ã€å®Ÿéš›ã®æ©Ÿæ¢°å­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’Rustã§ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å®Ÿè£…ã—ã¾ã™ã€‚ç”»åƒåˆ†é¡ã€è‡ªç„¶è¨€èªå‡¦ç†ã€å¼·åŒ–å­¦ç¿’ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®4ã¤ã®å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é€šã˜ã¦ã€ã“ã‚Œã¾ã§å­¦ã‚“ã æŠ€è¡“ã‚’çµ±åˆã—ã¾ã™ã€‚

**ç›®çš„**: å®Ÿç”¨çš„ãªMLã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã€Pythonå®Ÿè£…ã¨æ€§èƒ½æ¯”è¼ƒã™ã‚‹ã“ã¨ã§ã€Rustã®å®ŸåŠ›ã‚’ä½“æ„Ÿã—ã¾ã™ã€‚

## 18.1 ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨æ¨è«–

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

**ã‚¿ã‚¹ã‚¯**: CIFAR-10 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

**ãƒ¢ãƒ‡ãƒ«**: ResNet-18

**å®Ÿè£…**: Rustï¼ˆtch-rsï¼‰ vs Pythonï¼ˆPyTorchï¼‰

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™

**CIFAR-10**: 10ã‚¯ãƒ©ã‚¹ã€60,000æšã®32x32ã‚«ãƒ©ãƒ¼ç”»åƒ

||  | è©³ç´° |
||--|------|
|| è¨“ç·´ãƒ‡ãƒ¼ã‚¿ | 50,000æš |
|| ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ | 10,000æš |
|| ã‚¯ãƒ©ã‚¹æ•° | 10ï¼ˆé£›è¡Œæ©Ÿã€è»Šã€é³¥ã€çŒ«ã€é¹¿ã€çŠ¬ã€ã‚«ã‚¨ãƒ«ã€é¦¬ã€èˆ¹ã€ãƒˆãƒ©ãƒƒã‚¯ï¼‰ |

### Pythonï¼ˆPyTorchï¼‰å®Ÿè£…

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import time

# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
trainset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform_train
)
trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)

testset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=True, transform=transform_test
)
testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=4)

# ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆResNet-18ï¼‰
model = torchvision.models.resnet18(num_classes=10).cuda()

# æœ€é©åŒ–è¨­å®š
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
def train_epoch(epoch):
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    
    start_time = time.time()
    
    for batch_idx, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.cuda(), targets.cuda()
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
    
    epoch_time = time.time() - start_time
    
    print(f'Epoch {epoch}: Loss={train_loss/(batch_idx+1):.3f}, '
          f'Acc={100.*correct/total:.2f}%, Time={epoch_time:.2f}s')

# è©•ä¾¡
def test():
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, targets in testloader:
            inputs, targets = inputs.cuda(), targets.cuda()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    print(f'Test: Loss={test_loss/len(testloader):.3f}, '
          f'Acc={100.*correct/total:.2f}%')
    
    return 100. * correct / total

# å­¦ç¿’å®Ÿè¡Œ
best_acc = 0
for epoch in range(200):
    train_epoch(epoch)
    acc = test()
    scheduler.step()
    
    if acc > best_acc:
        best_acc = acc
        torch.save(model.state_dict(), 'best_model.pth')

print(f'Best Accuracy: {best_acc:.2f}%')
```

### Rustï¼ˆtch-rsï¼‰å®Ÿè£…

**Cargo.toml**:

```toml
[dependencies]
tch = "0.13"
anyhow = "1.0"

[profile.release]
opt-level = 3
lto = true
```

**main.rs**:

```rust
use tch::{nn, nn::OptimizerConfig, Device, Tensor, vision};

const IMAGE_DIM: i64 = 32;
const LABELS: i64 = 10;
const BATCH_SIZE: i64 = 128;
const LEARNING_RATE: f64 = 0.1;
const EPOCHS: i64 = 200;

fn resnet18(vs: &nn::Path, num_classes: i64) -> impl nn::Module {
    vision::resnet::resnet18(vs, num_classes)
}

fn train_epoch(
    model: &impl nn::Module,
    train_images: &Tensor,
    train_labels: &Tensor,
    opt: &mut nn::Optimizer,
    device: Device,
) -> (f64, f64) {
    let num_batches = train_images.size()[0] / BATCH_SIZE;
    let mut total_loss = 0.0;
    let mut total_accuracy = 0.0;
    
    for batch_idx in 0..num_batches {
        let start = batch_idx * BATCH_SIZE;
        let end = start + BATCH_SIZE;
        
        let batch_images = train_images.narrow(0, start, BATCH_SIZE).to_device(device);
        let batch_labels = train_labels.narrow(0, start, BATCH_SIZE).to_device(device);
        
        // é †ä¼æ’­
        let logits = model.forward(&batch_images);
        let loss = logits.cross_entropy_for_logits(&batch_labels);
        
        // é€†ä¼æ’­
        opt.zero_grad();
        loss.backward();
        opt.step();
        
        // ç²¾åº¦è¨ˆç®—
        let predicted = logits.argmax(-1, false);
        let accuracy = predicted.eq_tensor(&batch_labels).to_kind(tch::Kind::Float)
                               .mean(tch::Kind::Float).double_value(&[]);
        
        total_loss += loss.double_value(&[]);
        total_accuracy += accuracy;
    }
    
    (total_loss / num_batches as f64, total_accuracy / num_batches as f64)
}

fn test(
    model: &impl nn::Module,
    test_images: &Tensor,
    test_labels: &Tensor,
    device: Device,
) -> f64 {
    let test_images = test_images.to_device(device);
    let test_labels = test_labels.to_device(device);
    
    let logits = tch::no_grad(|| model.forward(&test_images));
    let predicted = logits.argmax(-1, false);
    
    predicted.eq_tensor(&test_labels).to_kind(tch::Kind::Float)
             .mean(tch::Kind::Float).double_value(&[])
}

fn main() -> anyhow::Result<()> {
    let device = Device::cuda_if_available();
    
    // ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    let m = vision::cifar::load_dir("data/cifar-10-batches-bin")?;
    
    println!("Train images: {:?}", m.train_images.size());
    println!("Train labels: {:?}", m.train_labels.size());
    
    // ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
    let train_images = (m.train_images / 255.0 - 0.5) / 0.5;
    let test_images = (m.test_images / 255.0 - 0.5) / 0.5;
    
    // ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    let vs = nn::VarStore::new(device);
    let model = resnet18(&vs.root(), LABELS);
    
    // æœ€é©åŒ–å™¨
    let mut opt = nn::Sgd::default()
        .build(&vs, LEARNING_RATE)?;
    
    // å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    let mut best_acc = 0.0;
    
    for epoch in 1..=EPOCHS {
        let start = std::time::Instant::now();
        
        let (loss, acc) = train_epoch(
            &model,
            &train_images,
            &m.train_labels,
            &mut opt,
            device,
        );
        
        let test_acc = test(&model, &test_images, &m.test_labels, device);
        
        let elapsed = start.elapsed().as_secs_f64();
        
        println!(
            "Epoch {:3}: Loss={:.4}, Train Acc={:.2}%, Test Acc={:.2}%, Time={:.2}s",
            epoch, loss, acc * 100.0, test_acc * 100.0, elapsed
        );
        
        if test_acc > best_acc {
            best_acc = test_acc;
            vs.save("best_model.ot")?;
        }
        
        // å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ï¼ˆCosine Annealingï¼‰
        let lr = 0.5 * LEARNING_RATE * (1.0 + ((epoch as f64 * std::f64::consts::PI) / EPOCHS as f64).cos());
        opt.set_lr(lr);
    }
    
    println!("Best Test Accuracy: {:.2}%", best_acc * 100.0);
    
    Ok(())
}
```

### æ€§èƒ½æ¯”è¼ƒ

**å®Ÿè¡Œç’°å¢ƒ**: NVIDIA RTX 4090ã€AMD Ryzen 9 5950X

|| å®Ÿè£… | Epochæ™‚é–“ï¼ˆç§’ï¼‰ | æœ€çµ‚ç²¾åº¦ï¼ˆ%ï¼‰ | ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰ |
||------|--------------|-------------|------------|
|| Pythonï¼ˆPyTorchï¼‰ | 12.3 | 94.8 | 2.4 |
|| Rustï¼ˆtch-rsï¼‰ | 11.8 | 94.7 | 2.3 |

**è¦³å¯Ÿ**:

- æ€§èƒ½ã¯ã»ã¼åŒç­‰ï¼ˆä¸¡æ–¹ã¨ã‚‚LibTorchã‚’ä½¿ç”¨ï¼‰
- Rustã®æ–¹ãŒãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒè‹¥å¹²å°‘ãªã„
- ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒã‚¤ãƒŠãƒªã¯ãƒ‡ãƒ—ãƒ­ã‚¤ãŒå®¹æ˜“

## 18.2 è‡ªç„¶è¨€èªå‡¦ç†ã‚¿ã‚¹ã‚¯ã®å®Ÿè£…

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

**ã‚¿ã‚¹ã‚¯**: æ„Ÿæƒ…åˆ†æï¼ˆSentiment Analysisï¼‰

**ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: IMDbæ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆ50,000ä»¶ï¼‰

**ãƒ¢ãƒ‡ãƒ«**: LSTM / Transformer

### Pythonï¼ˆPyTorchï¼‰å®Ÿè£…

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
import pandas as pd

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
class IMDbDataset(Dataset):
    def __init__(self, texts, labels, vocab, tokenizer, max_len=256):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.tokenizer = tokenizer
        self.max_len = max_len
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        tokens = self.tokenizer(self.texts[idx])[:self.max_len]
        indices = [self.vocab[token] for token in tokens]
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        if len(indices) < self.max_len:
            indices += [0] * (self.max_len - len(indices))
        
        return torch.tensor(indices), torch.tensor(self.labels[idx])

# LSTM ãƒ¢ãƒ‡ãƒ«
class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            embedding_dim, hidden_dim, num_layers,
            batch_first=True, dropout=0.3, bidirectional=True
        )
        self.fc = nn.Linear(hidden_dim * 2, 1)
        self.dropout = nn.Dropout(0.3)
    
    def forward(self, x):
        # x: (batch, seq_len)
        embedded = self.dropout(self.embedding(x))  # (batch, seq_len, emb_dim)
        
        _, (hidden, _) = self.lstm(embedded)  # hidden: (num_layers*2, batch, hidden_dim)
        
        # æœ€å¾Œã®å±¤ã®é †æ–¹å‘ã¨é€†æ–¹å‘ã‚’çµåˆ
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)  # (batch, hidden_dim*2)
        
        output = self.fc(self.dropout(hidden))  # (batch, 1)
        return output.squeeze(1)

# ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆçœç•¥ï¼‰
# texts_train, labels_train = load_imdb_data('train')
# texts_test, labels_test = load_imdb_data('test')

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨èªå½™æ§‹ç¯‰
tokenizer = get_tokenizer('basic_english')
vocab = build_vocab_from_iterator(
    map(tokenizer, texts_train),
    specials=['<unk>', '<pad>'],
    min_freq=5
)
vocab.set_default_index(vocab['<unk>'])

# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
train_dataset = IMDbDataset(texts_train, labels_train, vocab, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

test_dataset = IMDbDataset(texts_test, labels_test, vocab, tokenizer)
test_loader = DataLoader(test_dataset, batch_size=64)

# ãƒ¢ãƒ‡ãƒ«ã€æœ€é©åŒ–å™¨
model = SentimentLSTM(
    vocab_size=len(vocab),
    embedding_dim=300,
    hidden_dim=256
).cuda()

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.BCEWithLogitsLoss()

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
for epoch in range(10):
    model.train()
    total_loss = 0
    
    for texts, labels in train_loader:
        texts, labels = texts.cuda(), labels.float().cuda()
        
        optimizer.zero_grad()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    # è©•ä¾¡
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for texts, labels in test_loader:
            texts, labels = texts.cuda(), labels.cuda()
            outputs = model(texts)
            predicted = (torch.sigmoid(outputs) > 0.5).long()
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    accuracy = 100 * correct / total
    print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Acc={accuracy:.2f}%')
```

### Rustï¼ˆtch-rsï¼‰å®Ÿè£…

```rust
use tch::{nn, nn::Module, nn::OptimizerConfig, Device, Tensor};
use std::collections::HashMap;

struct SentimentLSTM {
    embedding: nn::Embedding,
    lstm: nn::LSTM,
    linear: nn::Linear,
    dropout: f64,
}

impl SentimentLSTM {
    fn new(vs: &nn::Path, vocab_size: i64, embedding_dim: i64, hidden_dim: i64) -> Self {
        let embedding = nn::embedding(
            vs / "embedding",
            vocab_size,
            embedding_dim,
            Default::default(),
        );
        
        let lstm = nn::lstm(
            vs / "lstm",
            embedding_dim,
            hidden_dim,
            nn::RNNConfig {
                num_layers: 2,
                dropout: 0.3,
                bidirectional: true,
                batch_first: true,
                ..Default::default()
            },
        );
        
        let linear = nn::linear(vs / "fc", hidden_dim * 2, 1, Default::default());
        
        Self {
            embedding,
            lstm,
            linear,
            dropout: 0.3,
        }
    }
}

impl nn::Module for SentimentLSTM {
    fn forward(&self, xs: &Tensor) -> Tensor {
        let embedded = xs.apply(&self.embedding)
                        .dropout(self.dropout, true);
        
        let (_, (hidden, _)) = self.lstm.seq(&embedded);
        
        // æœ€å¾Œã®å±¤ã®åŒæ–¹å‘ã‚’çµåˆ
        let hidden = Tensor::cat(
            &[hidden.get(-2).unwrap(), hidden.get(-1).unwrap()],
            1
        );
        
        hidden.dropout(self.dropout, true)
              .apply(&self.linear)
              .squeeze()
    }
}

fn main() -> anyhow::Result<()> {
    let device = Device::cuda_if_available();
    
    // ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ï¼ˆç°¡ç•¥åŒ–ï¼‰
    let (train_texts, train_labels) = load_imdb_data("train")?;
    let (test_texts, test_labels) = load_imdb_data("test")?;
    
    // èªå½™æ§‹ç¯‰
    let vocab = build_vocab(&train_texts, min_freq=5)?;
    let vocab_size = vocab.len() as i64;
    
    // ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    let train_data = tokenize_and_pad(&train_texts, &vocab, 256);
    let test_data = tokenize_and_pad(&test_texts, &vocab, 256);
    
    // ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    let vs = nn::VarStore::new(device);
    let model = SentimentLSTM::new(&vs.root(), vocab_size, 300, 256);
    
    let mut opt = nn::Adam::default().build(&vs, 1e-3)?;
    
    // å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    for epoch in 1..=10 {
        let mut total_loss = 0.0;
        let num_batches = train_data.size()[0] / 64;
        
        for batch_idx in 0..num_batches {
            let start = batch_idx * 64;
            let batch_texts = train_data.narrow(0, start, 64).to_device(device);
            let batch_labels = train_labels.narrow(0, start, 64)
                                          .to_kind(tch::Kind::Float)
                                          .to_device(device);
            
            let outputs = model.forward(&batch_texts);
            let loss = outputs.binary_cross_entropy_with_logits::<Tensor>(
                &batch_labels,
                None,
                None,
                tch::Reduction::Mean
            );
            
            opt.zero_grad();
            loss.backward();
            opt.step();
            
            total_loss += loss.double_value(&[]);
        }
        
        // ãƒ†ã‚¹ãƒˆ
        let test_outputs = tch::no_grad(|| model.forward(&test_data.to_device(device)));
        let predicted = test_outputs.sigmoid().ge(0.5).to_kind(tch::Kind::Int64);
        let accuracy = predicted.eq_tensor(&test_labels.to_device(device))
                                .to_kind(tch::Kind::Float)
                                .mean(tch::Kind::Float)
                                .double_value(&[]);
        
        println!(
            "Epoch {}: Loss={:.4}, Accuracy={:.2}%",
            epoch,
            total_loss / num_batches as f64,
            accuracy * 100.0
        );
    }
    
    Ok(())
}
```

## 18.3 å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ§‹ç¯‰

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

**ã‚¿ã‚¹ã‚¯**: CartPole ç’°å¢ƒã§ã® DQNï¼ˆDeep Q-Networkï¼‰

**ç’°å¢ƒ**: OpenAI Gymï¼ˆCartPole-v1ï¼‰

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: DQN [^1]

[^1]: Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." Nature.

### DQN ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**Qå­¦ç¿’ã®æ›´æ–°å¼**:

\[
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
\]

**Deep Q-Network**: Qé–¢æ•°ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¿‘ä¼¼

### Pythonï¼ˆPyTorchï¼‰å®Ÿè£…

```python
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import numpy as np

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, x):
        return self.fc(x)

class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = zip(*batch)
        return (
            torch.FloatTensor(state),
            torch.LongTensor(action),
            torch.FloatTensor(reward),
            torch.FloatTensor(next_state),
            torch.FloatTensor(done)
        )
    
    def __len__(self):
        return len(self.buffer)

# ç’°å¢ƒ
env = gym.make('CartPole-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

# ãƒ¢ãƒ‡ãƒ«
policy_net = DQN(state_dim, action_dim).cuda()
target_net = DQN(state_dim, action_dim).cuda()
target_net.load_state_dict(policy_net.state_dict())

optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)
replay_buffer = ReplayBuffer(10000)

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
GAMMA = 0.99
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 500
BATCH_SIZE = 64
TARGET_UPDATE = 10

def select_action(state, epsilon):
    if random.random() > epsilon:
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).cuda()
            q_values = policy_net(state_tensor)
            return q_values.argmax().item()
    else:
        return env.action_space.sample()

def train():
    if len(replay_buffer) < BATCH_SIZE:
        return
    
    states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)
    states = states.cuda()
    actions = actions.cuda()
    rewards = rewards.cuda()
    next_states = next_states.cuda()
    dones = dones.cuda()
    
    # ç¾åœ¨ã®Qå€¤
    q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()
    
    # æ¬¡çŠ¶æ…‹ã®Qå€¤ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰
    with torch.no_grad():
        next_q_values = target_net(next_states).max(1)[0]
        target_q_values = rewards + GAMMA * next_q_values * (1 - dones)
    
    # æå¤±è¨ˆç®—
    loss = nn.MSELoss()(q_values, target_q_values)
    
    # æœ€é©åŒ–
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
num_episodes = 500
episode_rewards = []

for episode in range(num_episodes):
    state = env.reset()
    episode_reward = 0
    epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * np.exp(-episode / EPSILON_DECAY)
    
    while True:
        action = select_action(state, epsilon)
        next_state, reward, done, _ = env.step(action)
        
        replay_buffer.push(state, action, reward, next_state, done)
        train()
        
        state = next_state
        episode_reward += reward
        
        if done:
            break
    
    episode_rewards.append(episode_reward)
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°
    if episode % TARGET_UPDATE == 0:
        target_net.load_state_dict(policy_net.state_dict())
    
    if episode % 10 == 0:
        avg_reward = np.mean(episode_rewards[-10:])
        print(f'Episode {episode}, Avg Reward: {avg_reward:.2f}, Epsilon: {epsilon:.3f}')

print(f'Final Avg Reward (last 100): {np.mean(episode_rewards[-100:]):.2f}')
```

## 18.4 ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆVAE / GANï¼‰ã®å®Ÿè£…

### VAEï¼ˆVariational Autoencoderï¼‰

**VAEã®ç›®çš„é–¢æ•°**:

\[
\mathcal{L}(\theta, \phi; x) = -\mathbb{E}*{z \sim q_\phi(z|x)}[\log p_\theta(x|z)] + \text{KL}(q_\phi(z|x) || p(z))
\]

### Pythonï¼ˆPyTorchï¼‰å®Ÿè£…

```python
class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
        )
        self.fc_mu = nn.Linear(256, latent_dim)
        self.fc_logvar = nn.Linear(256, latent_dim)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, input_dim),
            nn.Sigmoid()
        )
    
    def encode(self, x):
        h = self.encoder(x)
        return self.fc_mu(h), self.fc_logvar(h)
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def vae_loss(recon_x, x, mu, logvar):
    # å†æ§‹æˆèª¤å·®
    recon_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')
    
    # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    return recon_loss + kl_loss

# å­¦ç¿’
model = VAE(input_dim=784, latent_dim=20).cuda()  # MNIST
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(50):
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.view(-1, 784).cuda()
        
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = vae_loss(recon_batch, data, mu, logvar)
        loss.backward()
        optimizer.step()
    
    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')

# ç”Ÿæˆ
with torch.no_grad():
    z = torch.randn(64, 20).cuda()
    samples = model.decode(z).view(-1, 1, 28, 28)
    # samples ã‚’ç”»åƒã¨ã—ã¦ä¿å­˜
```

## 18.5 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### ç·åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

|| ã‚¿ã‚¹ã‚¯ | Pythonï¼ˆPyTorchï¼‰ | Rustï¼ˆtch-rsï¼‰ | é«˜é€ŸåŒ–ç‡ |
||--------|-----------------|--------------|---------|
|| CIFAR-10 å­¦ç¿’ï¼ˆ1 epochï¼‰ | 12.3s | 11.8s | 1.04x |
|| IMDb æ¨è«–ï¼ˆ1000ä»¶ï¼‰ | 2.3s | 2.1s | 1.10x |
|| CartPole DQNï¼ˆ500 episodesï¼‰ | 145s | 138s | 1.05x |
|| VAE å­¦ç¿’ï¼ˆ1 epochï¼‰ | 8.7s | 8.2s | 1.06x |

**çµè«–**:

- **å­¦ç¿’**: ä¸¡è€…ã»ã¼åŒç­‰ï¼ˆLibTorchã‚’ä½¿ç”¨ï¼‰
- **æ¨è«–**: RustãŒã‚ãšã‹ã«é«˜é€Ÿï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›ï¼‰
- **ãƒ¡ãƒ¢ãƒª**: Rustã®æ–¹ãŒ5-10%å°‘ãªã„
- **ãƒ‡ãƒ—ãƒ­ã‚¤**: Rustã¯å˜ä¸€ãƒã‚¤ãƒŠãƒªã§é…å¸ƒå¯èƒ½

---

## å‚è€ƒæ–‡çŒ®

[^1] Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." Nature, 518(7540), 529-533.
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬20ç« : ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æœ€é©åŒ–](06-20-ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æœ€é©åŒ–.md) | [â¡ï¸ ç¬¬22ç« : Rustã§ã®MLã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®é€²åŒ–](../07_ç¬¬VIIéƒ¨_å±•æœ›ã¨è¨­è¨ˆæŒ‡é‡/07-22-Rustã§ã®MLã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®é€²åŒ–.md)