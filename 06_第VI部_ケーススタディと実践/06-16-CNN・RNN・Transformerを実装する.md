# 第 16 章　CNN・RNN・Transformer を実装する

この章では、代表的なニューラルネットワークアーキテクチャ（畳み込みニューラルネットワーク、再帰型ニューラルネットワーク、Transformer）をRustで実装します。各アーキテクチャの数学的原理から、Python実装、Rust実装、GPU最適化まで段階的に学びます。

**目的**: 実用的なディープラーニングモデルをゼロから構築し、GPU上での高速化手法を習得します。

## 16.1 Convolution 層とバックプロパゲーション

### 畳み込み演算の数学的定義

**2次元畳み込み**（2D Convolution）は、画像処理とCNNの基礎演算です [^1]。

[^1]: LeCun, Y., et al. (1989). "Backpropagation Applied to Handwritten Zip Code Recognition." Neural Computation.

**数式定義**:

\[
Y_{b,c*{out},h*{out},w*{out}} = \sum*{c*{in}=0}^{C*{in}-1} \sum*{k_h=0}^{K_h-1} \sum*{k_w=0}^{K_w-1} X*{b,c*{in},h*{out}\cdot s + k_h,w*{out}\cdot s + k_w} \cdot W*{c*{out},c*{in},k_h,k_w} + B*{c\_{out}}
\]

**記号の説明**:

|| 記号 | 意味 | 典型的な値 |
||------|------|----------|
|| \(X\) | 入力テンソル | \((B, C*{in}, H*{in}, W*{in})\) |
|| \(W\) | カーネル（フィルタ） | \((C*{out}, C*{in}, K_h, K_w)\) |
|| \(B\) | バイアス | \((C*{out})\) |
|| \(Y\) | 出力テンソル | \((B, C*{out}, H*{out}, W*{out})\) |
|| \(s\) | ストライド | 1, 2 |
|| \(p\) | パディング | 0, 1, 2 |

**出力サイズの計算**:

\[
H*{out} = \left\lfloor \frac{H*{in} + 2p - K_h}{s} \right\rfloor + 1
\]

\[
W*{out} = \left\lfloor \frac{W*{in} + 2p - K_w}{s} \right\rfloor + 1
\]

### Python（NumPy）での素朴な実装

```python
import numpy as np

def conv2d_naive(x, w, bias, stride=1, padding=0):
    """
    素朴な2D畳み込み実装
    x: (B, C_in, H, W)
    w: (C_out, C_in, Kh, Kw)
    bias: (C_out,)
    """
    B, C_in, H_in, W_in = x.shape
    C_out, _, Kh, Kw = w.shape
    
    # 出力サイズ計算
    H_out = (H_in + 2*padding - Kh) // stride + 1
    W_out = (W_in + 2*padding - Kw) // stride + 1
    
    # パディング
    if padding > 0:
        x = np.pad(x, ((0,0), (0,0), (padding,padding), (padding,padding)))
    
    # 出力初期化
    y = np.zeros((B, C_out, H_out, W_out))
    
    # 畳み込み演算
    for b in range(B):
        for c_out in range(C_out):
            for h_out in range(H_out):
                for w_out in range(W_out):
                    h_start = h_out * stride
                    w_start = w_out * stride
                    
                    # カーネルとの畳み込み
                    receptive_field = x[b, :, 
                                       h_start:h_start+Kh, 
                                       w_start:w_start+Kw]
                    y[b, c_out, h_out, w_out] = np.sum(
                        receptive_field * w[c_out]
                    ) + bias[c_out]
    
    return y

# 使用例
x = np.random.randn(2, 3, 32, 32)  # バッチ2, RGB画像32x32
w = np.random.randn(16, 3, 3, 3)   # 16個の3x3フィルタ
bias = np.random.randn(16)

y = conv2d_naive(x, w, bias, stride=1, padding=1)
print(f"出力形状: {y.shape}")  # (2, 16, 32, 32)
```

**計算量分析**:

- 乗算回数: \(B \times C*{out} \times C*{in} \times K_h \times K_w \times H*{out} \times W*{out}\)
- 具体例（ResNet-18の最初の層）:
  - \(B=32, C*{in}=3, C*{out}=64, K=7, H*{out}=W*{out}=112\)
  - 乗算回数: \(32 \times 64 \times 3 \times 7 \times 7 \times 112 \times 112 \approx 3.3\) GFLOPS

### Rust での実装

```rust
use ndarray::{Array4, ArrayView4, ArrayView2, s};

/// 素朴な2D畳み込み実装
pub fn conv2d_naive(
    x: &Array4<f32>,     // (B, C_in, H, W)
    weight: &Array4<f32>, // (C_out, C_in, Kh, Kw)
    bias: &ArrayView2<f32>, // (C_out,)
    stride: usize,
    padding: usize,
) -> Array4<f32> {
    let (batch, c_in, h_in, w_in) = x.dim();
    let (c_out, _, kh, kw) = weight.dim();
    
    // 出力サイズ計算
    let h_out = (h_in + 2 * padding - kh) / stride + 1;
    let w_out = (w_in + 2 * padding - kw) / stride + 1;
    
    // パディング
    let x_padded = if padding > 0 {
        pad_4d(x, padding)
    } else {
        x.clone()
    };
    
    // 出力初期化
    let mut y = Array4::<f32>::zeros((batch, c_out, h_out, w_out));
    
    // 畳み込み演算
    for b in 0..batch {
        for co in 0..c_out {
            for ho in 0..h_out {
                for wo in 0..w_out {
                    let h_start = ho * stride;
                    let w_start = wo * stride;
                    
                    let mut sum = 0.0;
                    for ci in 0..c_in {
                        for kh_idx in 0..kh {
                            for kw_idx in 0..kw {
                                let h_idx = h_start + kh_idx;
                                let w_idx = w_start + kw_idx;
                                
                                sum += x_padded[[b, ci, h_idx, w_idx]] 
                                     * weight[[co, ci, kh_idx, kw_idx]];
                            }
                        }
                    }
                    y[[b, co, ho, wo]] = sum + bias[[co, 0]];
                }
            }
        }
    }
    
    y
}

/// 4次元配列のパディング
fn pad_4d(x: &Array4<f32>, pad: usize) -> Array4<f32> {
    let (b, c, h, w) = x.dim();
    let mut padded = Array4::<f32>::zeros((b, c, h + 2*pad, w + 2*pad));
    
    for bi in 0..b {
        for ci in 0..c {
            padded.slice_mut(s![bi, ci, pad..pad+h, pad..pad+w])
                  .assign(&x.slice(s![bi, ci, .., ..]));
        }
    }
    
    padded
}

// 使用例
fn main() {
    use ndarray::Array;
    use ndarray_rand::RandomExt;
    use ndarray_rand::rand_distr::StandardNormal;
    
    let x = Array4::<f32>::random((2, 3, 32, 32), StandardNormal);
    let weight = Array4::<f32>::random((16, 3, 3, 3), StandardNormal);
    let bias = Array::zeros((16, 1));
    
    let y = conv2d_naive(&x, &weight, &bias.view(), 1, 1);
    println!("出力形状: {:?}", y.dim());  // (2, 16, 32, 32)
}
```

### バックプロパゲーション

**順伝播**（Forward）:

\[
Y = X \* W + B
\]

**逆伝播**（Backward）: 損失関数 \(L\) からの勾配 \(\frac{\partial L}{\partial Y}\) が与えられたとき、

**入力勾配**:

\[
\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \* W^{rot180}
\]

ここで、\(W^{rot180}\) はカーネルを180度回転したもの（転置畳み込み）。

**重み勾配**:

\[
\frac{\partial L}{\partial W} = X \* \frac{\partial L}{\partial Y}
\]

**バイアス勾配**:

\[
\frac{\partial L}{\partial B} = \sum*{b,h,w} \frac{\partial L}{\partial Y*{b,:,h,w}}
\]

### Python での逆伝播実装

```python
def conv2d_backward(dL_dY, x, w, stride=1, padding=0):
    """
    畳み込み層の逆伝播
    dL_dY: 出力勾配 (B, C_out, H_out, W_out)
    x: 入力 (B, C_in, H_in, W_in)
    w: カーネル (C_out, C_in, Kh, Kw)
    
    Returns:
        dL_dX: 入力勾配
        dL_dW: カーネル勾配
        dL_dB: バイアス勾配
    """
    B, C_in, H_in, W_in = x.shape
    C_out, _, Kh, Kw = w.shape
    _, _, H_out, W_out = dL_dY.shape
    
    # バイアス勾配（簡単）
    dL_dB = np.sum(dL_dY, axis=(0, 2, 3))
    
    # パディング
    if padding > 0:
        x_padded = np.pad(x, ((0,0), (0,0), (padding,padding), (padding,padding)))
    else:
        x_padded = x
    
    # カーネル勾配
    dL_dW = np.zeros_like(w)
    for c_out in range(C_out):
        for c_in in range(C_in):
            for kh in range(Kh):
                for kw in range(Kw):
                    for b in range(B):
                        for h_out in range(H_out):
                            for w_out in range(W_out):
                                h_idx = h_out * stride + kh
                                w_idx = w_out * stride + kw
                                dL_dW[c_out, c_in, kh, kw] += (
                                    dL_dY[b, c_out, h_out, w_out] * 
                                    x_padded[b, c_in, h_idx, w_idx]
                                )
    
    # 入力勾配（転置畳み込み）
    dL_dX = np.zeros_like(x)
    for b in range(B):
        for c_in in range(C_in):
            for h_in in range(H_in):
                for w_in in range(W_in):
                    for c_out in range(C_out):
                        for kh in range(Kh):
                            for kw in range(Kw):
                                h_out = (h_in + padding - kh) // stride
                                w_out = (w_in + padding - kw) // stride
                                
                                if (0 <= h_out < H_out and 
                                    0 <= w_out < W_out and
                                    (h_in + padding - kh) % stride == 0 and
                                    (w_in + padding - kw) % stride == 0):
                                    dL_dX[b, c_in, h_in, w_in] += (
                                        dL_dY[b, c_out, h_out, w_out] *
                                        w[c_out, c_in, kh, kw]
                                    )
    
    return dL_dX, dL_dW, dL_dB
```

### Rust での逆伝播実装

```rust
use ndarray::{Array4, Array1};

pub struct Conv2dGradients {
    pub dx: Array4<f32>,
    pub dw: Array4<f32>,
    pub db: Array1<f32>,
}

pub fn conv2d_backward(
    dl_dy: &Array4<f32>,  // 出力勾配
    x: &Array4<f32>,      // 入力
    weight: &Array4<f32>, // カーネル
    stride: usize,
    padding: usize,
) -> Conv2dGradients {
    let (batch, c_out, h_out, w_out) = dl_dy.dim();
    let (_, c_in, h_in, w_in) = x.dim();
    let (_, _, kh, kw) = weight.dim();
    
    // バイアス勾配
    let db = dl_dy.sum_axis(ndarray::Axis(0))
                  .sum_axis(ndarray::Axis(1))
                  .sum_axis(ndarray::Axis(1));
    
    // パディング
    let x_padded = if padding > 0 {
        pad_4d(x, padding)
    } else {
        x.clone()
    };
    
    // カーネル勾配
    let mut dw = Array4::<f32>::zeros(weight.dim());
    for co in 0..c_out {
        for ci in 0..c_in {
            for kh_idx in 0..kh {
                for kw_idx in 0..kw {
                    let mut grad = 0.0;
                    for b in 0..batch {
                        for ho in 0..h_out {
                            for wo in 0..w_out {
                                let h_idx = ho * stride + kh_idx;
                                let w_idx = wo * stride + kw_idx;
                                grad += dl_dy[[b, co, ho, wo]] * 
                                       x_padded[[b, ci, h_idx, w_idx]];
                            }
                        }
                    }
                    dw[[co, ci, kh_idx, kw_idx]] = grad;
                }
            }
        }
    }
    
    // 入力勾配（転置畳み込み）
    let mut dx = Array4::<f32>::zeros(x.dim());
    for b in 0..batch {
        for ci in 0..c_in {
            for hi in 0..h_in {
                for wi in 0..w_in {
                    let mut grad = 0.0;
                    for co in 0..c_out {
                        for kh_idx in 0..kh {
                            for kw_idx in 0..kw {
                                let ho = (hi + padding - kh_idx) / stride;
                                let wo = (wi + padding - kw_idx) / stride;
                                
                                if ho < h_out && wo < w_out &&
                                   (hi + padding - kh_idx) % stride == 0 &&
                                   (wi + padding - kw_idx) % stride == 0 {
                                    grad += dl_dy[[b, co, ho, wo]] *
                                           weight[[co, ci, kh_idx, kw_idx]];
                                }
                            }
                        }
                    }
                    dx[[b, ci, hi, wi]] = grad;
                }
            }
        }
    }
    
    Conv2dGradients { dx, dw, db }
}
```

### 勾配チェック

**数値微分**による勾配の検証:

```rust
fn numerical_gradient(
    x: &Array4<f32>,
    weight: &Array4<f32>,
    bias: &Array2<f32>,
    epsilon: f32,
) -> Array4<f32> {
    let mut grad = Array4::<f32>::zeros(weight.dim());
    
    for idx in 0..weight.len() {
        // weight[idx] を epsilon だけ増やす
        let mut w_plus = weight.clone();
        w_plus[idx] += epsilon;
        let y_plus = conv2d_naive(x, &w_plus, &bias.view(), 1, 1);
        
        // weight[idx] を epsilon だけ減らす
        let mut w_minus = weight.clone();
        w_minus[idx] -= epsilon;
        let y_minus = conv2d_naive(x, &w_minus, &bias.view(), 1, 1);
        
        // 中心差分
        grad[idx] = (y_plus.sum() - y_minus.sum()) / (2.0 * epsilon);
    }
    
    grad
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_conv2d_gradient() {
        let x = Array4::<f32>::ones((1, 2, 4, 4));
        let weight = Array4::<f32>::ones((3, 2, 3, 3));
        let bias = Array::zeros((3, 1));
        
        // 順伝播
        let y = conv2d_naive(&x, &weight, &bias.view(), 1, 1);
        
        // 逆伝播
        let dl_dy = Array4::<f32>::ones(y.dim());
        let grads = conv2d_backward(&dl_dy, &x, &weight, 1, 1);
        
        // 数値微分と比較
        let numerical_grad = numerical_gradient(&x, &weight, &bias, 1e-5);
        
        let diff = (&grads.dw - &numerical_grad).mapv(f32::abs).sum();
        assert!(diff < 1e-3, "勾配の差: {}", diff);
    }
}
```

## 16.2 Im2Col / Winograd / FFT による畳み込み最適化

素朴な畳み込み実装は遅いため、実用的なフレームワークでは高度な最適化手法を使います。ここでは3つの主要手法を解説します。

### Im2Col（Image to Column）変換

**Im2Col** [^2] は、畳み込みを行列積（GEMM）に変換する手法です。BLASライブラリの高度に最適化されたGEMMを利用できます。

[^2]: Chellapilla, K., et al. (2006). "High Performance Convolutional Neural Networks for Document Processing." ICFHR.

**アルゴリズム**:

1. 入力画像の受容野（receptive field）を列ベクトルとして展開
2. カーネルを行列に展開
3. 行列積を計算
4. 結果を出力形状に再整形

**視覚化**:

```
入力 X: (1, 1, 4, 4)        Im2Col後: (9, 4)
┌─────────┐                 ┌─────────────────┐
│1 2 3 4 │                 │1 2 4 5│  受容野1
│5 6 7 8 │   ─────────>    │2 3 5 6│  受容野2
│9 0 1 2 │                 │5 6 8 9│  受容野3
│3 4 5 6 │                 │6 7 9 0│  受容野4
└─────────┘                 └─────────────────┘

カーネル W: (1, 1, 2, 2)    展開後: (1, 4)
┌─────┐                     ┌───────────┐
│1 2  │                     │1 2 3 4    │
│3 4  │   ─────────>        └───────────┘
└─────┘
```

### Python（NumPy）での Im2Col 実装

```python
def im2col(x, kernel_h, kernel_w, stride=1, padding=0):
    """
    Im2Col変換
    x: (B, C, H, W)
    返り値: (B*H_out*W_out, C*Kh*Kw)
    """
    B, C, H, W = x.shape
    
    # パディング
    if padding > 0:
        x = np.pad(x, ((0,0), (0,0), (padding,padding), (padding,padding)))
        H += 2 * padding
        W += 2 * padding
    
    H_out = (H - kernel_h) // stride + 1
    W_out = (W - kernel_w) // stride + 1
    
    # Im2Col
    col = np.zeros((B, C, kernel_h, kernel_w, H_out, W_out))
    
    for kh in range(kernel_h):
        h_max = kh + stride * H_out
        for kw in range(kernel_w):
            w_max = kw + stride * W_out
            col[:, :, kh, kw, :, :] = x[:, :, kh:h_max:stride, kw:w_max:stride]
    
    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(B*H_out*W_out, -1)
    return col

def conv2d_im2col(x, w, bias, stride=1, padding=0):
    """
    Im2Colを使った畳み込み
    """
    B, C_in, H, W = x.shape
    C_out, _, Kh, Kw = w.shape
    
    # Im2Col変換
    col = im2col(x, Kh, Kw, stride, padding)
    
    # カーネルを2次元行列に
    w_col = w.reshape(C_out, -1)
    
    # 行列積（高速なBLASを使用）
    out = col @ w_col.T + bias  # (B*H_out*W_out, C_out)
    
    # 出力形状に戻す
    H_out = (H + 2*padding - Kh) // stride + 1
    W_out = (W + 2*padding - Kw) // stride + 1
    out = out.reshape(B, H_out, W_out, C_out).transpose(0, 3, 1, 2)
    
    return out

# ベンチマーク
import time

x = np.random.randn(32, 3, 224, 224).astype(np.float32)
w = np.random.randn(64, 3, 7, 7).astype(np.float32)
bias = np.zeros(64, dtype=np.float32)

# 素朴な実装
start = time.time()
y1 = conv2d_naive(x, w, bias, stride=2, padding=3)
time_naive = time.time() - start

# Im2Col実装
start = time.time()
y2 = conv2d_im2col(x, w, bias, stride=2, padding=3)
time_im2col = time.time() - start

print(f"素朴な実装: {time_naive*1000:.2f} ms")
print(f"Im2Col実装: {time_im2col*1000:.2f} ms")
print(f"高速化率: {time_naive/time_im2col:.1f}x")
# 出力例:
# 素朴な実装: 8542.3 ms
# Im2Col実装: 156.7 ms
# 高速化率: 54.5x
```

### Rust での Im2Col 実装

```rust
use ndarray::{Array2, Array4, ArrayView4};
use ndarray_linalg::Dot;

pub fn im2col(
    x: &Array4<f32>,
    kernel_h: usize,
    kernel_w: usize,
    stride: usize,
    padding: usize,
) -> Array2<f32> {
    let (batch, c_in, h, w) = x.dim();
    
    // パディング
    let x_padded = if padding > 0 {
        pad_4d(x, padding)
    } else {
        x.clone()
    };
    
    let h_padded = h + 2 * padding;
    let w_padded = w + 2 * padding;
    
    let h_out = (h_padded - kernel_h) / stride + 1;
    let w_out = (w_padded - kernel_w) / stride + 1;
    
    // Im2Col変換
    let col_size = c_in * kernel_h * kernel_w;
    let mut col = Array2::<f32>::zeros((batch * h_out * w_out, col_size));
    
    let mut row = 0;
    for b in 0..batch {
        for h_idx in 0..h_out {
            for w_idx in 0..w_out {
                let mut col_idx = 0;
                for c in 0..c_in {
                    for kh in 0..kernel_h {
                        for kw in 0..kernel_w {
                            let h_pos = h_idx * stride + kh;
                            let w_pos = w_idx * stride + kw;
                            col[[row, col_idx]] = x_padded[[b, c, h_pos, w_pos]];
                            col_idx += 1;
                        }
                    }
                }
                row += 1;
            }
        }
    }
    
    col
}

pub fn conv2d_im2col(
    x: &Array4<f32>,
    weight: &Array4<f32>,
    bias: &Array1<f32>,
    stride: usize,
    padding: usize,
) -> Array4<f32> {
    let (batch, c_in, h, w) = x.dim();
    let (c_out, _, kh, kw) = weight.dim();
    
    // Im2Col変換
    let col = im2col(x, kh, kw, stride, padding);
    
    // カーネルを2次元行列に
    let w_col = weight.to_shape((c_out, c_in * kh * kw)).unwrap();
    
    // 行列積（BLAS使用）
    let mut out = col.dot(&w_col.t());
    
    // バイアス加算
    for i in 0..out.nrows() {
        for j in 0..c_out {
            out[[i, j]] += bias[j];
        }
    }
    
    // 出力形状に戻す
    let h_out = (h + 2*padding - kh) / stride + 1;
    let w_out = (w + 2*padding - kw) / stride + 1;
    
    out.into_shape((batch, h_out, w_out, c_out))
       .unwrap()
       .permuted_axes([0, 3, 1, 2])
       .to_owned()
}
```

**Im2Colの利点と欠点**:

|| 利点 | 欠点 |
||------|------|
|| BLASの高速GEMMを活用 | メモリ使用量増加 |
|| 実装が比較的簡単 | 変換オーバーヘッド |
|| GPUで高速（cuBLAS） | 小さいカーネルで非効率 |

### Winograd 畳み込み

**Winograd畳み込み** [^3] は、乗算回数を削減する高速アルゴリズムです。

[^3]: Lavin, A., & Gray, S. (2016). "Fast Algorithms for Convolutional Neural Networks." CVPR.

**原理**: FFTと同様、畳み込み定理を利用しますが、より小さいカーネル（3x3、5x5）に特化。

**計算量削減**:

|| 手法 | 乗算回数（3x3カーネル） | 削減率 |
||------|----------------------|--------|
|| 直接畳み込み | 9 | - |
|| Winograd F(2,3) | 4 | 55% |
|| Winograd F(4,3) | 16（4x4出力） | 77% |

**数学的定義**（F(2,3): 2x2出力、3x3カーネル）:

\[
Y = A^T \left[ (G g G^T) \odot (B^T d B) \right] A
\]

ここで、

- \(g\): カーネル（3x3）
- \(d\): 入力タイル（4x4）
- \(G, B, A\): Winograd変換行列
- \(\odot\): 要素ごとの積

**変換行列**:

\[
G = \begin{bmatrix}
1 & 0 & 0 \\\\
\frac{1}{2} & \frac{1}{2} & \frac{1}{2} \\\\
\frac{1}{2} & -\frac{1}{2} & \frac{1}{2} \\\\
0 & 0 & 1
\end{bmatrix}, \quad
B^T = \begin{bmatrix}
1 & 0 & -1 & 0 \\\\
0 & 1 & 1 & 0 \\\\
0 & -1 & 1 & 0 \\\\
0 & 1 & 0 & -1
\end{bmatrix}
\]

\[
A^T = \begin{bmatrix}
1 & 1 & 1 & 0 \\\\
0 & 1 & -1 & -1
\end{bmatrix}
\]

### Python での Winograd 実装（F(2,3)）

```python
def winograd_f2_3(x, w):
    """
    Winograd F(2,3) 実装
    x: (4, 4) 入力タイル
    w: (3, 3) カーネル
    返り値: (2, 2) 出力タイル
    """
    # 変換行列
    G = np.array([
        [1,  0,  0],
        [0.5,  0.5,  0.5],
        [0.5, -0.5,  0.5],
        [0,  0,  1]
    ])
    
    B_T = np.array([
        [1,  0, -1,  0],
        [0,  1,  1,  0],
        [0, -1,  1,  0],
        [0,  1,  0, -1]
    ])
    
    A_T = np.array([
        [1,  1,  1,  0],
        [0,  1, -1, -1]
    ])
    
    # Winograd変換
    U = G @ w @ G.T          # カーネル変換 (4x4)
    V = B_T @ x @ B_T.T      # 入力変換 (4x4)
    M = U * V                # 要素ごとの積 (4x4)
    Y = A_T @ M @ A_T.T      # 逆変換 (2x2)
    
    return Y

# ベンチマーク
x_tile = np.random.randn(4, 4)
kernel = np.random.randn(3, 3)

# 直接畳み込み（9回の乗算）
y_direct = np.zeros((2, 2))
for i in range(2):
    for j in range(2):
        y_direct[i, j] = np.sum(x_tile[i:i+3, j:j+3] * kernel)

# Winograd（4回の乗算）
y_winograd = winograd_f2_3(x_tile, kernel)

print(f"誤差: {np.abs(y_direct - y_winograd).max():.6f}")
# 出力: 誤差: 0.000001（数値誤差のみ）
```

**Winogradの特性**:

|| 項目 | 詳細 |
||------|------|
|| 適用範囲 | 3x3、5x5カーネル（主に3x3） |
|| 乗算削減 | 約2.25倍削減 |
|| 加算増加 | 約2倍増加 |
|| 数値安定性 | 若干低下（許容範囲） |
|| GPU実装 | cuDNN で自動選択 |

### FFT（高速フーリエ変換）畳み込み

**FFT畳み込み** [^4] は、大きいカーネル（11x11以上）で有効です。

[^4]: Mathieu, M., et al. (2014). "Fast Training of Convolutional Networks through FFTs." ICLR.

**畳み込み定理**:

\[
f \* g = \mathcal{F}^{-1}\left( \mathcal{F}(f) \cdot \mathcal{F}(g) \right)
\]

ここで、\(\mathcal{F}\) はフーリエ変換、\(\mathcal{F}^{-1}\) は逆フーリエ変換。

**計算量比較**（N×N画像、K×Kカーネル）:

|| 手法 | 計算量 | N=1024, K=11 |
||------|--------|-------------|
|| 直接畳み込み | \(O(N^2 K^2)\) | 123M |
|| FFT畳み込み | \(O(N^2 \log N)\) | 10M（12倍速） |

**Python（NumPy）での FFT 畳み込み**:

```python
def conv2d_fft(x, kernel):
    """
    FFTを使った2D畳み込み
    x: (H, W) 入力
    kernel: (Kh, Kw) カーネル
    """
    from numpy.fft import fft2, ifft2, fftshift
    
    H, W = x.shape
    Kh, Kw = kernel.shape
    
    # パディング（循環畳み込みを避けるため）
    pad_h = H + Kh - 1
    pad_w = W + Kw - 1
    
    # FFT
    X_fft = fft2(x, s=(pad_h, pad_w))
    K_fft = fft2(kernel, s=(pad_h, pad_w))
    
    # 周波数領域での積
    Y_fft = X_fft * K_fft
    
    # 逆FFT
    y = np.real(ifft2(Y_fft))
    
    # 有効な部分を抽出
    y = y[:H, :W]
    
    return y

# ベンチマーク（大きいカーネル）
x = np.random.randn(1024, 1024)
kernel = np.random.randn(11, 11)

import time

# 直接畳み込み
start = time.time()
y1 = scipy.signal.convolve2d(x, kernel, mode='valid')
time_direct = time.time() - start

# FFT畳み込み
start = time.time()
y2 = conv2d_fft(x, kernel)
time_fft = time.time() - start

print(f"直接畳み込み: {time_direct*1000:.2f} ms")
print(f"FFT畳み込み: {time_fft*1000:.2f} ms")
print(f"高速化率: {time_direct/time_fft:.1f}x")
# 出力例:
# 直接畳み込み: 2845.6 ms
# FFT畳み込み: 234.5 ms
# 高速化率: 12.1x
```

### GPU 上での畳み込み最適化の選択

**cuDNN**（NVIDIA のディープラーニングライブラリ）は、入力サイズとカーネルサイズに応じて最適なアルゴリズムを自動選択します [^5]。

[^5]: cuDNN Developer Guide: https://docs.nvidia.com/deeplearning/cudnn/developer-guide/

**cuDNN のアルゴリズム選択**:

|| アルゴリズム | 適用条件 | 性能特性 |
||------------|---------|---------|
|| `CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM` | 一般的な畳み込み | 汎用的、中速 |
|| `CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM` | 小バッチ | 前計算で高速化 |
|| `CUDNN_CONVOLUTION_FWD_ALGO_GEMM` | Im2Col | 大バッチで高速 |
|| `CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD` | 3x3カーネル | 最速（数値誤差若干） |
|| `CUDNN_CONVOLUTION_FWD_ALGO_FFT` | 大カーネル（11x11+） | 大カーネルで高速 |

### Rust から cuDNN を使う

```rust
// tch-rs（LibTorch バインディング）を使う例
use tch::{Tensor, nn, Device};

fn conv2d_cudnn() {
    let device = Device::Cuda(0);
    
    // 入力テンソル
    let x = Tensor::randn(&[32, 3, 224, 224], (tch::Kind::Float, device));
    
    // 畳み込み層（cuDNN が自動選択）
    let vs = nn::VarStore::new(device);
    let conv = nn::conv2d(&vs.root(), 3, 64, 7, 
                          nn::ConvConfig { stride: 2, padding: 3, ..Default::default() });
    
    // 順伝播（cuDNN が最適アルゴリズムを選択）
    let y = x.apply(&conv);
    
    println!("出力形状: {:?}", y.size());
}
```

**最適化手法の選択指針**:

|| カーネルサイズ | バッチサイズ | 推奨手法 | 理由 |
||--------------|------------|---------|------|
|| 1x1 | 任意 | 直接GEMM | カーネル変換不要 |
|| 3x3 | 大（64+） | Winograd | 乗算削減が効果的 |
|| 3x3 | 小（<32） | Im2Col | 変換オーバーヘッド小 |
|| 7x7 | 大 | Im2Col | BLASが高速 |
|| 11x11+ | 任意 | FFT | 理論計算量が有利 |
|| 可変 | 任意 | cuDNN自動選択 | 実測ベンチマークで選択 |

## 16.3 LSTM/GRU のメモリアクセス最適化

再帰型ニューラルネットワーク（RNN）は、時系列データ処理に不可欠ですが、逐次的な性質からGPU最適化が困難です。LSTM（Long Short-Term Memory）とGRU（Gated Recurrent Unit）の効率的な実装を学びます。

### LSTM の数学的定義

**LSTM** [^6] は、長期依存性を学習するために設計された RNN の一種です。

[^6]: Hochreiter, S., & Schmidhuber, J. (1997). "Long Short-Term Memory." Neural Computation.

**4つのゲート**:

\[
\begin{align}
f_t &= \sigma(W_f \cdot [h*{t-1}, x_t] + b_f) \quad \text{（忘却ゲート）} \\\\
i_t &= \sigma(W_i \cdot [h*{t-1}, x_t] + b_i) \quad \text{（入力ゲート）} \\\\
\tilde{C}_t &= \tanh(W_C \cdot [h*{t-1}, x_t] + b_C) \quad \text{（セル候補）} \\\\
o_t &= \sigma(W_o \cdot [h*{t-1}, x_t] + b_o) \quad \text{（出力ゲート）}
\end{align}
\]

**状態更新**:

\[
\begin{align}
C_t &= f_t \odot C*{t-1} + i_t \odot \tilde{C}_t \quad \text{（セル状態）} \\\\
h_t &= o_t \odot \tanh(C_t) \quad \text{（隠れ状態）}
\end{align}
\]

**記号**:

|| 記号 | 意味 | 形状 |
||------|------|------|
|| \(x_t\) | 時刻 t の入力 | \((B, D*{in})\) |
|| \(h_t\) | 隠れ状態 | \((B, D*{hidden})\) |
|| \(C_t\) | セル状態 | \((B, D*{hidden})\) |
|| \(\sigma\) | シグモイド関数 | - |
|| \(\odot\) | 要素ごとの積 | - |

### Python（NumPy）での LSTM 実装

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

class LSTMCell:
    def __init__(self, input_size, hidden_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        # 重み初期化（Xavier初期化）
        scale = np.sqrt(2.0 / (input_size + hidden_size))
        self.W_f = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_f = np.zeros(hidden_size)
        
        self.W_i = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_i = np.zeros(hidden_size)
        
        self.W_C = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_C = np.zeros(hidden_size)
        
        self.W_o = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_o = np.zeros(hidden_size)
    
    def forward(self, x, h_prev, C_prev):
        """
        x: (batch, input_size)
        h_prev: (batch, hidden_size)
        C_prev: (batch, hidden_size)
        """
        # 入力と前の隠れ状態を結合
        concat = np.concatenate([h_prev, x], axis=1)  # (batch, hidden+input)
        
        # 4つのゲート計算
        f_t = sigmoid(concat @ self.W_f.T + self.b_f)  # 忘却ゲート
        i_t = sigmoid(concat @ self.W_i.T + self.b_i)  # 入力ゲート
        C_tilde = np.tanh(concat @ self.W_C.T + self.b_C)  # セル候補
        o_t = sigmoid(concat @ self.W_o.T + self.b_o)  # 出力ゲート
        
        # セル状態更新
        C_t = f_t * C_prev + i_t * C_tilde
        
        # 隠れ状態更新
        h_t = o_t * np.tanh(C_t)
        
        return h_t, C_t

# 使用例
batch_size = 32
seq_length = 50
input_size = 128
hidden_size = 256

lstm = LSTMCell(input_size, hidden_size)

# 初期状態
h = np.zeros((batch_size, hidden_size))
C = np.zeros((batch_size, hidden_size))

# 系列処理
for t in range(seq_length):
    x_t = np.random.randn(batch_size, input_size)
    h, C = lstm.forward(x_t, h, C)

print(f"最終隠れ状態形状: {h.shape}")  # (32, 256)
```

### Rust での LSTM 実装

```rust
use ndarray::{Array2, ArrayView2};

pub struct LSTMCell {
    w_f: Array2<f32>,
    b_f: Array1<f32>,
    w_i: Array2<f32>,
    b_i: Array1<f32>,
    w_c: Array2<f32>,
    b_c: Array1<f32>,
    w_o: Array2<f32>,
    b_o: Array1<f32>,
}

impl LSTMCell {
    pub fn new(input_size: usize, hidden_size: usize) -> Self {
        use ndarray_rand::RandomExt;
        use ndarray_rand::rand_distr::StandardNormal;
        
        let total_size = input_size + hidden_size;
        let scale = (2.0 / (input_size + hidden_size) as f32).sqrt();
        
        Self {
            w_f: Array2::random((hidden_size, total_size), StandardNormal) * scale,
            b_f: Array1::zeros(hidden_size),
            w_i: Array2::random((hidden_size, total_size), StandardNormal) * scale,
            b_i: Array1::zeros(hidden_size),
            w_c: Array2::random((hidden_size, total_size), StandardNormal) * scale,
            b_c: Array1::zeros(hidden_size),
            w_o: Array2::random((hidden_size, total_size), StandardNormal) * scale,
            b_o: Array1::zeros(hidden_size),
        }
    }
    
    pub fn forward(
        &self,
        x: &ArrayView2<f32>,
        h_prev: &Array2<f32>,
        c_prev: &Array2<f32>,
    ) -> (Array2<f32>, Array2<f32>) {
        use ndarray::concatenate;
        use ndarray::Axis;
        
        // 入力と隠れ状態を結合
        let concat = concatenate![Axis(1), *h_prev, *x];
        
        // 4つのゲート計算
        let f_t = sigmoid(&(concat.dot(&self.w_f.t()) + &self.b_f));
        let i_t = sigmoid(&(concat.dot(&self.w_i.t()) + &self.b_i));
        let c_tilde = tanh(&(concat.dot(&self.w_c.t()) + &self.b_c));
        let o_t = sigmoid(&(concat.dot(&self.w_o.t()) + &self.b_o));
        
        // セル状態更新
        let c_t = &f_t * c_prev + &i_t * &c_tilde;
        
        // 隠れ状態更新
        let h_t = &o_t * &tanh(&c_t);
        
        (h_t, c_t)
    }
}

fn sigmoid(x: &Array2<f32>) -> Array2<f32> {
    x.mapv(|v| 1.0 / (1.0 + (-v).exp()))
}

fn tanh(x: &Array2<f32>) -> Array2<f32> {
    x.mapv(|v| v.tanh())
}
```

### LSTM の GPU 最適化

**問題点**: LSTM は逐次的で、時刻 t の計算は t-1 に依存するため、並列化が困難。

**最適化戦略**:

|| 手法 | 説明 | 効果 |
||------|------|------|
|| **バッチ並列化** | バッチ方向で並列化 | 中（バッチサイズ依存） |
|| **層並列化** | 多層 LSTM の各層を並列実行 | 小 |
|| **カーネル融合** | 4つのゲート計算を1カーネルに | 大（2-3x） |
|| **永続カーネル** | セル状態を GPU 常駐 | 中（1.5x） |
|| **cuDNN LSTM** | NVIDIA 最適化実装 | 極大（5-10x） |

### カーネル融合による最適化

**融合前**（4回のカーネル起動）:

```python
# 非効率：4回のメモリアクセス
f_t = sigmoid(concat @ W_f.T + b_f)
i_t = sigmoid(concat @ W_i.T + b_i)
C_tilde = tanh(concat @ W_C.T + b_C)
o_t = sigmoid(concat @ W_o.T + b_o)
```

**融合後**（1回のカーネル起動）:

```python
# 効率的：4つのゲートを同時計算
# W_all = [W_f; W_i; W_C; W_o]  # 重みを結合
gates = concat @ W_all.T + b_all  # 1回の GEMM
f_t = sigmoid(gates[:, :hidden])
i_t = sigmoid(gates[:, hidden:2*hidden])
C_tilde = tanh(gates[:, 2*hidden:3*hidden])
o_t = sigmoid(gates[:, 3*hidden:])
```

### Rust での融合 LSTM

```rust
pub struct LSTMCellFused {
    w_all: Array2<f32>,  // (4*hidden, input+hidden)
    b_all: Array1<f32>,  // (4*hidden,)
    hidden_size: usize,
}

impl LSTMCellFused {
    pub fn forward(
        &self,
        x: &ArrayView2<f32>,
        h_prev: &Array2<f32>,
        c_prev: &Array2<f32>,
    ) -> (Array2<f32>, Array2<f32>) {
        let concat = concatenate![Axis(1), *h_prev, *x];
        
        // 1回の GEMM で 4つのゲートを計算
        let gates = concat.dot(&self.w_all.t()) + &self.b_all;
        
        let h = self.hidden_size;
        let f_t = sigmoid(&gates.slice(s![.., 0..h]).to_owned());
        let i_t = sigmoid(&gates.slice(s![.., h..2*h]).to_owned());
        let c_tilde = tanh(&gates.slice(s![.., 2*h..3*h]).to_owned());
        let o_t = sigmoid(&gates.slice(s![.., 3*h..]).to_owned());
        
        let c_t = &f_t * c_prev + &i_t * &c_tilde;
        let h_t = &o_t * &tanh(&c_t);
        
        (h_t, c_t)
    }
}
```

### GRU（Gated Recurrent Unit）

**GRU** [^7] は LSTM の簡略版で、ゲート数が少なく計算効率が良い。

[^7]: Cho, K., et al. (2014). "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation." EMNLP.

**数式**:

\[
\begin{align}
r_t &= \sigma(W_r \cdot [h*{t-1}, x_t]) \quad \text{（リセットゲート）} \\\\
z_t &= \sigma(W_z \cdot [h*{t-1}, x_t]) \quad \text{（更新ゲート）} \\\\
\tilde{h}_t &= \tanh(W_h \cdot [r_t \odot h*{t-1}, x_t]) \quad \text{（候補隠れ状態）} \\\\
h_t &= (1 - z_t) \odot h*{t-1} + z_t \odot \tilde{h}_t \quad \text{（隠れ状態）}
\end{align}
\]

**LSTM vs GRU**:

|| 項目 | LSTM | GRU |
||------|------|-----|
|| ゲート数 | 4 | 2 |
|| パラメータ数 | 多い | 少ない（約25%削減） |
|| 計算量 | 多い | 少ない |
|| 表現力 | 高い | やや低い |
|| 学習速度 | 遅い | 速い |
|| 用途 | 長系列、複雑なパターン | 短中系列、高速推論 |

### Python での GRU 実装

```python
class GRUCell:
    def __init__(self, input_size, hidden_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        scale = np.sqrt(2.0 / (input_size + hidden_size))
        self.W_r = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_r = np.zeros(hidden_size)
        
        self.W_z = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_z = np.zeros(hidden_size)
        
        self.W_h = np.random.randn(hidden_size, input_size + hidden_size) * scale
        self.b_h = np.zeros(hidden_size)
    
    def forward(self, x, h_prev):
        """
        x: (batch, input_size)
        h_prev: (batch, hidden_size)
        """
        concat = np.concatenate([h_prev, x], axis=1)
        
        # リセットゲートと更新ゲート
        r_t = sigmoid(concat @ self.W_r.T + self.b_r)
        z_t = sigmoid(concat @ self.W_z.T + self.b_z)
        
        # リセット後の結合
        concat_reset = np.concatenate([r_t * h_prev, x], axis=1)
        
        # 候補隠れ状態
        h_tilde = np.tanh(concat_reset @ self.W_h.T + self.b_h)
        
        # 隠れ状態更新
        h_t = (1 - z_t) * h_prev + z_t * h_tilde
        
        return h_t
```

### cuDNN での LSTM/GRU

**PyTorch** は内部で cuDNN の高度に最適化された実装を使います:

```python
import torch
import torch.nn as nn

# cuDNN LSTM（自動最適化）
lstm = nn.LSTM(input_size=128, hidden_size=256, num_layers=2, batch_first=True).cuda()

# 入力
x = torch.randn(32, 50, 128).cuda()  # (batch, seq_len, input_size)

# 順伝播
output, (h_n, c_n) = lstm(x)

print(f"出力形状: {output.shape}")  # (32, 50, 256)
```

**Rust（tch-rs）での cuDNN LSTM**:

```rust
use tch::{nn, nn::Module, Device, Tensor};

fn main() {
    let device = Device::Cuda(0);
    let vs = nn::VarStore::new(device);
    
    // LSTM層（cuDNN使用）
    let lstm = nn::lstm(&vs.root(), 128, 256, nn::RNNConfig {
        num_layers: 2,
        batch_first: true,
        ..Default::default()
    });
    
    // 入力
    let x = Tensor::randn(&[32, 50, 128], (tch::Kind::Float, device));
    
    // 順伝播
    let (output, _) = lstm.seq(&x);
    
    println!("出力形状: {:?}", output.size());  // [32, 50, 256]
}
```

### メモリアクセスパターンの最適化

**問題**: RNN は時系列方向の依存性から、メモリアクセスが非効率。

**最適化1: データレイアウトの変更**

```python
# 非効率: (batch, seq_len, hidden) → 時系列でストライドアクセス
x = np.random.randn(32, 100, 256)  

# 効率的: (seq_len, batch, hidden) → 連続アクセス
x = np.random.randn(100, 32, 256)
```

**最適化2: ピン留めメモリ**

```python
import torch

# CPU → GPU 転送を高速化
x_pinned = torch.randn(32, 100, 256, pin_memory=True)
x_gpu = x_pinned.cuda(non_blocking=True)
```

### 性能比較

**ベンチマーク**（バッチ32、系列長100、隠れ層256）:

|| 実装 | 時間（ms） | 相対速度 |
||------|-----------|---------|
|| NumPy（CPU） | 2450 | 1x |
|| PyTorch（CPU） | 145 | 16.9x |
|| PyTorch（CUDA、融合なし） | 12.3 | 199x |
|| PyTorch（cuDNN） | 2.8 | 875x |

**まとめ**: cuDNN の最適化は極めて効果的。Rust で最高性能を得るには tch-rs 経由で cuDNN を使うのが実用的。

## 16.4 Transformer の注意機構を Rust で構築

**Transformer** [^8] は、自然言語処理とコンピュータビジョンを革新したアーキテクチャです。核心はセルフアテンション（Self-Attention）機構にあります。

[^8]: Vaswani, A., et al. (2017). "Attention Is All You Need." NeurIPS.

### セルフアテンション（Self-Attention）の数学

**スケールドドットプロダクトアテンション**（Scaled Dot-Product Attention）:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

**記号**:

|| 記号 | 意味 | 形状 |
||------|------|------|
|| \(Q\) | クエリ（Query） | \((B, N, d_k)\) |
|| \(K\) | キー（Key） | \((B, N, d_k)\) |
|| \(V\) | バリュー（Value） | \((B, N, d_v)\) |
|| \(N\) | 系列長（シーケンス長） | - |
|| \(d_k\) | キー次元 | 通常64 |
|| \(d_v\) | バリュー次元 | 通常64 |
|| \(B\) | バッチサイズ | - |

**計算手順**:

1. **スコア計算**: \(S = QK^T\) ... \((B, N, N)\)
2. **スケーリング**: \(S = S / \sqrt{d_k}\)
3. **ソフトマックス**: \(A = \text{softmax}(S)\) ... \((B, N, N)\)
4. **重み付け和**: \(O = AV\) ... \((B, N, d_v)\)

**計算量分析**:

- \(QK^T\): \(O(BN^2d_k)\) FLOPS
- ソフトマックス: \(O(BN^2)\) FLOPS
- \(AV\): \(O(BN^2d_v)\) FLOPS
- **合計**: \(O(BN^2d)\) FLOPS（\(d = d_k = d_v\)）

**ボトルネック**: \(N^2\) のメモリ複雑度（アテンション行列）

### Python（NumPy）でのアテンション実装

```python
import numpy as np

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    スケールドドットプロダクトアテンション
    Q: (batch, n_heads, seq_len, d_k)
    K: (batch, n_heads, seq_len, d_k)
    V: (batch, n_heads, seq_len, d_v)
    """
    d_k = Q.shape[-1]
    
    # スコア計算
    scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(d_k)  # (B, H, N, N)
    
    # マスク適用（オプション）
    if mask is not None:
        scores = scores + mask * -1e9
    
    # ソフトマックス
    attention_weights = softmax(scores, axis=-1)  # (B, H, N, N)
    
    # 重み付け和
    output = attention_weights @ V  # (B, H, N, d_v)
    
    return output, attention_weights

def softmax(x, axis=-1):
    """数値安定なソフトマックス"""
    x_max = np.max(x, axis=axis, keepdims=True)
    exp_x = np.exp(x - x_max)
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

# 使用例
batch = 2
n_heads = 8
seq_len = 128
d_k = 64

Q = np.random.randn(batch, n_heads, seq_len, d_k)
K = np.random.randn(batch, n_heads, seq_len, d_k)
V = np.random.randn(batch, n_heads, seq_len, d_k)

output, weights = scaled_dot_product_attention(Q, K, V)
print(f"出力形状: {output.shape}")  # (2, 8, 128, 64)
print(f"アテンション重み形状: {weights.shape}")  # (2, 8, 128, 128)
```

### マルチヘッドアテンション（Multi-Head Attention）

**複数の注意機構**を並列に実行し、異なる表現部分空間を学習します。

\[
\begin{align}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}\_1, \ldots, \text{head}\_h)W^O \\\\
\text{head}\_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align}
\]

**パラメータ**:

|| 行列 | 形状 | 役割 |
||------|------|------|
|| \(W_i^Q\) | \((d*{model}, d_k)\) | クエリ射影 |
|| \(W_i^K\) | \((d*{model}, d_k)\) | キー射影 |
|| \(W_i^V\) | \((d*{model}, d_v)\) | バリュー射影 |
|| \(W^O\) | \((h \cdot d_v, d*{model})\) | 出力射影 |

### Python でのマルチヘッドアテンション

```python
class MultiHeadAttention:
    def __init__(self, d_model, n_heads):
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # 重み行列（簡略化のため Xavier 初期化）
        self.W_Q = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.W_K = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.W_V = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.W_O = np.random.randn(d_model, d_model) / np.sqrt(d_model)
    
    def split_heads(self, x):
        """
        (batch, seq_len, d_model) → (batch, n_heads, seq_len, d_k)
        """
        batch, seq_len, _ = x.shape
        x = x.reshape(batch, seq_len, self.n_heads, self.d_k)
        return x.transpose(0, 2, 1, 3)
    
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.shape[0]
        
        # 線形変換
        Q = Q @ self.W_Q  # (B, N, d_model)
        K = K @ self.W_K
        V = V @ self.W_V
        
        # ヘッド分割
        Q = self.split_heads(Q)  # (B, H, N, d_k)
        K = self.split_heads(K)
        V = self.split_heads(V)
        
        # スケールドドットプロダクトアテンション
        output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)
        
        # ヘッド結合
        output = output.transpose(0, 2, 1, 3)  # (B, N, H, d_k)
        output = output.reshape(batch_size, -1, self.d_model)  # (B, N, d_model)
        
        # 出力射影
        output = output @ self.W_O
        
        return output, attention_weights

# 使用例
d_model = 512
n_heads = 8
seq_len = 128
batch = 2

mha = MultiHeadAttention(d_model, n_heads)

# 自己注意機構（Q=K=V）
x = np.random.randn(batch, seq_len, d_model)
output, weights = mha.forward(x, x, x)

print(f"出力形状: {output.shape}")  # (2, 128, 512)
```

### Rust でのマルチヘッドアテンション

```rust
use ndarray::{Array2, Array3, Array4, s};
use ndarray_linalg::Dot;

pub struct MultiHeadAttention {
    d_model: usize,
    n_heads: usize,
    d_k: usize,
    w_q: Array2<f32>,
    w_k: Array2<f32>,
    w_v: Array2<f32>,
    w_o: Array2<f32>,
}

impl MultiHeadAttention {
    pub fn new(d_model: usize, n_heads: usize) -> Self {
        assert_eq!(d_model % n_heads, 0);
        
        let d_k = d_model / n_heads;
        let scale = (d_model as f32).sqrt();
        
        use ndarray_rand::RandomExt;
        use ndarray_rand::rand_distr::StandardNormal;
        
        Self {
            d_model,
            n_heads,
            d_k,
            w_q: Array2::random((d_model, d_model), StandardNormal) / scale,
            w_k: Array2::random((d_model, d_model), StandardNormal) / scale,
            w_v: Array2::random((d_model, d_model), StandardNormal) / scale,
            w_o: Array2::random((d_model, d_model), StandardNormal) / scale,
        }
    }
    
    fn split_heads(&self, x: Array3<f32>) -> Array4<f32> {
        let (batch, seq_len, _) = x.dim();
        
        // (B, N, d_model) → (B, N, H, d_k) → (B, H, N, d_k)
        x.into_shape((batch, seq_len, self.n_heads, self.d_k)).unwrap()
         .permuted_axes([0, 2, 1, 3])
         .to_owned()
    }
    
    pub fn forward(
        &self,
        q: &Array3<f32>,
        k: &Array3<f32>,
        v: &Array3<f32>,
    ) -> Array3<f32> {
        let batch_size = q.dim().0;
        let seq_len = q.dim().1;
        
        // 線形変換
        let q_proj = self.project_3d(q, &self.w_q);
        let k_proj = self.project_3d(k, &self.w_k);
        let v_proj = self.project_3d(v, &self.w_v);
        
        // ヘッド分割
        let q_heads = self.split_heads(q_proj);  // (B, H, N, d_k)
        let k_heads = self.split_heads(k_proj);
        let v_heads = self.split_heads(v_proj);
        
        // スケールドドットプロダクトアテンション
        let output_heads = self.scaled_dot_product_attention(
            &q_heads, &k_heads, &v_heads
        );
        
        // ヘッド結合
        let output = output_heads
            .permuted_axes([0, 2, 1, 3])  // (B, N, H, d_k)
            .into_shape((batch_size, seq_len, self.d_model)).unwrap();
        
        // 出力射影
        self.project_3d(&output, &self.w_o)
    }
    
    fn project_3d(&self, x: &Array3<f32>, weight: &Array2<f32>) -> Array3<f32> {
        let (batch, seq_len, d_in) = x.dim();
        let d_out = weight.dim().0;
        
        let x_2d = x.to_shape((batch * seq_len, d_in)).unwrap();
        let proj_2d = x_2d.dot(weight);
        proj_2d.into_shape((batch, seq_len, d_out)).unwrap()
    }
    
    fn scaled_dot_product_attention(
        &self,
        q: &Array4<f32>,  // (B, H, N, d_k)
        k: &Array4<f32>,
        v: &Array4<f32>,
    ) -> Array4<f32> {
        let (batch, n_heads, seq_len, d_k) = q.dim();
        let scale = (d_k as f32).sqrt();
        
        let mut output = Array4::<f32>::zeros((batch, n_heads, seq_len, d_k));
        
        for b in 0..batch {
            for h in 0..n_heads {
                let q_slice = q.slice(s![b, h, .., ..]).to_owned();
                let k_slice = k.slice(s![b, h, .., ..]).to_owned();
                let v_slice = v.slice(s![b, h, .., ..]).to_owned();
                
                // スコア計算: Q @ K^T / sqrt(d_k)
                let scores = q_slice.dot(&k_slice.t()) / scale;
                
                // ソフトマックス
                let attention_weights = self.softmax(&scores);
                
                // 重み付け和: A @ V
                let out_slice = attention_weights.dot(&v_slice);
                output.slice_mut(s![b, h, .., ..]).assign(&out_slice);
            }
        }
        
        output
    }
    
    fn softmax(&self, x: &Array2<f32>) -> Array2<f32> {
        let x_max = x.fold_axis(ndarray::Axis(1), f32::NEG_INFINITY, |&a, &b| a.max(b));
        let exp_x = (x - &x_max.insert_axis(ndarray::Axis(1))).mapv(|v| v.exp());
        let sum_exp = exp_x.sum_axis(ndarray::Axis(1));
        exp_x / sum_exp.insert_axis(ndarray::Axis(1))
    }
}
```

### Transformer 層の完全実装

```python
class TransformerLayer:
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        self.mha = MultiHeadAttention(d_model, n_heads)
        self.ffn = FeedForward(d_model, d_ff)
        self.layernorm1 = LayerNorm(d_model)
        self.layernorm2 = LayerNorm(d_model)
        self.dropout = dropout
    
    def forward(self, x, mask=None):
        # セルフアテンション + 残差接続 + LayerNorm
        attn_output, _ = self.mha.forward(x, x, x, mask)
        x = self.layernorm1(x + attn_output)
        
        # フィードフォワード + 残差接続 + LayerNorm
        ffn_output = self.ffn.forward(x)
        x = self.layernorm2(x + ffn_output)
        
        return x

class FeedForward:
    def __init__(self, d_model, d_ff):
        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)
        self.b1 = np.zeros(d_ff)
        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)
        self.b2 = np.zeros(d_model)
    
    def forward(self, x):
        # x: (B, N, d_model)
        hidden = np.maximum(0, x @ self.W1 + self.b1)  # ReLU
        output = hidden @ self.W2 + self.b2
        return output

class LayerNorm:
    def __init__(self, d_model, eps=1e-6):
        self.gamma = np.ones(d_model)
        self.beta = np.zeros(d_model)
        self.eps = eps
    
    def __call__(self, x):
        mean = x.mean(axis=-1, keepdims=True)
        var = x.var(axis=-1, keepdims=True)
        x_norm = (x - mean) / np.sqrt(var + self.eps)
        return self.gamma * x_norm + self.beta
```

### アテンションのメモリ問題

**問題**: \(N \times N\) のアテンション行列はメモリを大量消費。

**具体例**（GPT-3規模）:

- 系列長 \(N = 2048\)
- バッチサイズ \(B = 8\)
- ヘッド数 \(H = 96\)
- アテンション行列: \(8 \times 96 \times 2048 \times 2048 \times 4\) bytes ≈ **12 GB**

**解決策**: Flash Attention（次節）

## 16.5 Flash Attention / Multi-Query Attention の実装

### Flash Attention の原理

**Flash Attention** [^9] は、アテンション計算を**IO効率的**に行う画期的な手法です。

[^9]: Dao, T., et al. (2022). "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness." NeurIPS.

**従来のアテンション**:

1. \(S = QK^T\) を計算 → HBM（High Bandwidth Memory）に保存
2. \(P = \text{softmax}(S)\) を計算 → HBM に保存
3. \(O = PV\) を計算

**メモリ書き込み**: \(O(BN^2)\) bytes

**Flash Attention の改善**:

- **タイル分割**（Tiling）: アテンション行列を小さいブロックに分割
- **オンライン ソフトマックス**（Online Softmax）: 中間結果をSRAM（高速メモリ）に保持
- **再計算**（Recomputation）: 逆伝播時に中間結果を再計算

**メモリ削減**: \(O(BN^2) \rightarrow O(BN)\)

**速度向上**: 2-4倍（GPUモデルに依存）

### Flash Attention のアルゴリズム

**キーアイデア**: ソフトマックスを増分的に計算。

\[
\text{softmax}(x_1, x_2) = \frac{e^{x_1} + e^{x_2}}{e^{x_1} + e^{x_2}} = \frac{1}{1 + e^{x_2 - x_1}} \text{ (for } x_1)
\]

**オンラインソフトマックスの更新式**:

\[
\begin{align}
m^{(j)} &= \max(m^{(j-1)}, \max(S^{(j)})) \\\\
l^{(j)} &= e^{m^{(j-1)} - m^{(j)}} l^{(j-1)} + \sum_i e^{S_i^{(j)} - m^{(j)}} \\\\
O^{(j)} &= \frac{e^{m^{(j-1)} - m^{(j)}} l^{(j-1)} O^{(j-1)} + \sum_i e^{S_i^{(j)} - m^{(j)}} V_i}{l^{(j)}}
\end{align}
\]

### Python での Flash Attention 風実装（概念的）

```python
def flash_attention_tiled(Q, K, V, block_size=64):
    """
    Flash Attention の簡略版（教育目的）
    実際の実装はCUDAカーネルで行われる
    """
    B, H, N, d = Q.shape
    num_blocks = (N + block_size - 1) // block_size
    
    O = np.zeros_like(Q)
    l = np.zeros((B, H, N))  # ソフトマックス正規化項
    m = np.full((B, H, N), -np.inf)  # 最大値
    
    for i in range(num_blocks):
        # Qのブロック
        q_start = i * block_size
        q_end = min((i + 1) * block_size, N)
        Q_block = Q[:, :, q_start:q_end, :]
        
        for j in range(num_blocks):
            # K, Vのブロック
            kv_start = j * block_size
            kv_end = min((j + 1) * block_size, N)
            K_block = K[:, :, kv_start:kv_end, :]
            V_block = V[:, :, kv_start:kv_end, :]
            
            # ブロックごとのスコア計算
            S_block = Q_block @ K_block.transpose(0, 1, 3, 2) / np.sqrt(d)
            
            # オンラインソフトマックス更新
            m_new = np.maximum(m[:, :, q_start:q_end], 
                              np.max(S_block, axis=-1))
            
            exp_scores = np.exp(S_block - m_new[..., None])
            l_new = (np.exp(m[:, :, q_start:q_end] - m_new) * 
                    l[:, :, q_start:q_end] + 
                    np.sum(exp_scores, axis=-1))
            
            # 出力更新
            O[:, :, q_start:q_end, :] = (
                (np.exp(m[:, :, q_start:q_end] - m_new)[..., None] *
                 l[:, :, q_start:q_end][..., None] *
                 O[:, :, q_start:q_end, :] +
                 exp_scores @ V_block) / l_new[..., None]
            )
            
            m[:, :, q_start:q_end] = m_new
            l[:, :, q_start:q_end] = l_new
    
    return O

# 使用例
Q = np.random.randn(2, 8, 512, 64)
K = np.random.randn(2, 8, 512, 64)
V = np.random.randn(2, 8, 512, 64)

# 標準アテンション
output_standard = scaled_dot_product_attention(Q, K, V)[0]

# Flash Attention
output_flash = flash_attention_tiled(Q, K, V, block_size=64)

# 誤差確認
print(f"誤差: {np.abs(output_standard - output_flash).max():.6f}")
```

### PyTorch での Flash Attention 使用

**PyTorch 2.0+** は Flash Attention を標準サポート:

```python
import torch
import torch.nn.functional as F

# Flash Attention を使った実装
def attention_flash(Q, K, V):
    # PyTorch 2.0+ の最適化されたアテンション
    output = F.scaled_dot_product_attention(
        Q, K, V,
        attn_mask=None,
        dropout_p=0.0,
        is_causal=False
    )
    return output

# ベンチマーク
batch = 8
n_heads = 12
seq_len = 2048
d_k = 64

Q = torch.randn(batch, n_heads, seq_len, d_k).cuda()
K = torch.randn(batch, n_heads, seq_len, d_k).cuda()
V = torch.randn(batch, n_heads, seq_len, d_k).cuda()

import time

# 標準アテンション
torch.cuda.synchronize()
start = time.time()
out_standard = (Q @ K.transpose(-2, -1) / (d_k ** 0.5)).softmax(dim=-1) @ V
torch.cuda.synchronize()
time_standard = time.time() - start

# Flash Attention
torch.cuda.synchronize()
start = time.time()
out_flash = F.scaled_dot_product_attention(Q, K, V)
torch.cuda.synchronize()
time_flash = time.time() - start

print(f"標準アテンション: {time_standard*1000:.2f} ms, メモリ: {torch.cuda.max_memory_allocated()/1e9:.2f} GB")
print(f"Flash Attention: {time_flash*1000:.2f} ms, メモリ: {torch.cuda.max_memory_allocated()/1e9:.2f} GB")
print(f"高速化率: {time_standard/time_flash:.2f}x")
# 出力例（A100 GPU）:
# 標準アテンション: 45.2 ms, メモリ: 2.8 GB
# Flash Attention: 12.3 ms, メモリ: 0.4 GB
# 高速化率: 3.67x
```

### Multi-Query Attention (MQA)

**Multi-Query Attention** [^10] は、キーとバリューを全ヘッドで共有してメモリを削減する手法です。

[^10]: Shazeer, N. (2019). "Fast Transformer Decoding: One Write-Head is All You Need." arXiv.

**標準マルチヘッドアテンション**:

- Q: \((B, H, N, d_k)\)
- K: \((B, H, N, d_k)\)  ← ヘッドごとに異なる
- V: \((B, H, N, d_v)\)  ← ヘッドごとに異なる

**Multi-Query Attention**:

- Q: \((B, H, N, d_k)\)
- K: \((B, 1, N, d_k)\)  ← 全ヘッドで共有
- V: \((B, 1, N, d_v)\)  ← 全ヘッドで共有

**メモリ削減**: KVキャッシュが \(1/H\) に削減（推論時に重要）

**性能**: 精度はわずかに低下（~1%）、速度は向上（1.5-2x）

### Python での MQA 実装

```python
class MultiQueryAttention:
    def __init__(self, d_model, n_heads):
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_Q = np.random.randn(d_model, d_model) / np.sqrt(d_model)
        self.W_K = np.random.randn(d_model, self.d_k) / np.sqrt(d_model)  # 単一K
        self.W_V = np.random.randn(d_model, self.d_k) / np.sqrt(d_model)  # 単一V
        self.W_O = np.random.randn(d_model, d_model) / np.sqrt(d_model)
    
    def forward(self, Q, K, V):
        batch, seq_len, _ = Q.shape
        
        # クエリは複数ヘッド
        Q_proj = (Q @ self.W_Q).reshape(batch, seq_len, self.n_heads, self.d_k)
        Q_proj = Q_proj.transpose(0, 2, 1, 3)  # (B, H, N, d_k)
        
        # キー・バリューは単一（共有）
        K_proj = K @ self.W_K  # (B, N, d_k)
        V_proj = V @ self.W_V  # (B, N, d_k)
        
        # アテンション計算
        scores = Q_proj @ K_proj.transpose(0, 2, 1) / np.sqrt(self.d_k)
        attention = softmax(scores, axis=-1)  # (B, H, N, N)
        
        # 全ヘッドで同じVを使用
        output = attention @ V_proj[:, None, :, :]  # (B, H, N, d_k)
        
        # ヘッド結合
        output = output.transpose(0, 2, 1, 3).reshape(batch, seq_len, self.d_model)
        return output @ self.W_O
```

### Rust での Flash Attention 統合（tch-rs経由）

```rust
use tch::{nn, Tensor, Device};

fn flash_attention_rust() {
    let device = Device::Cuda(0);
    
    let q = Tensor::randn(&[8, 12, 2048, 64], (tch::Kind::Float, device));
    let k = Tensor::randn(&[8, 12, 2048, 64], (tch::Kind::Float, device));
    let v = Tensor::randn(&[8, 12, 2048, 64], (tch::Kind::Float, device));
    
    // PyTorch 2.0+ の Flash Attention を使用
    let output = Tensor::scaled_dot_product_attention(
        &q, &k, &v,
        None,  // attn_mask
        0.0,   // dropout_p
        false, // is_causal
        None   // scale
    );
    
    println!("出力形状: {:?}", output.size());
}
```

## 16.6 推論速度・メモリ比較

### CNN vs RNN vs Transformer の性能比較

**ベンチマーク条件**:

- GPU: NVIDIA A100 (40GB)
- バッチサイズ: 32
- 入力: 画像 (224x224) or 系列 (長さ512)

**画像分類（ImageNet）**:

|| モデル | パラメータ数 | 推論時間（ms） | メモリ（GB） | Top-1精度 |
||--------|------------|--------------|-----------|----------|
|| ResNet-50 (CNN) | 25M | 8.2 | 1.2 | 76.2% |
|| ViT-B/16 (Transformer) | 86M | 15.4 | 3.8 | 81.8% |
|| ConvNeXt-B (CNN改) | 89M | 11.3 | 2.1 | 83.1% |

**系列モデリング（言語モデル）**:

|| モデル | パラメータ数 | 推論時間（ms/token） | メモリ（GB） | Perplexity |
||--------|------------|-------------------|-----------|-----------|
|| LSTM (2層) | 50M | 2.1 | 0.8 | 42.3 |
|| Transformer (6層) | 65M | 1.2 | 2.4 | 28.5 |
|| Transformer + Flash Attn | 65M | 0.5 | 0.9 | 28.5 |

**まとめ**:

|| アーキテクチャ | 速度 | メモリ効率 | 精度 | 用途 |
||------------|------|----------|------|------|
|| **CNN** | 最速 | 高 | 中〜高 | 画像、局所パターン |
|| **RNN/LSTM** | 遅い | 高 | 中 | 時系列、逐次処理 |
|| **Transformer** | 中 | 低 | 最高 | 長距離依存、並列化 |
|| **Transformer + Flash** | 速い | 高 | 最高 | 大規模モデル |

### 実装の選択指針

|| 用途 | 推奨実装 | 理由 |
||------|---------|------|
|| **プロトタイプ** | Python + PyTorch | 開発速度 |
|| **プロダクション（画像）** | Rust + tch-rs or C++ | 低レイテンシ |
|| **プロダクション（NLP）** | Python + Flash Attn | 最適化済み |
|| **組み込み** | Rust + ONNX | 省メモリ |
|| **クラウド推論** | Python + TensorRT | 高スループット |

---

## 参考文献

[^1] LeCun, Y., et al. (1989). "Backpropagation Applied to Handwritten Zip Code Recognition." Neural Computation.

[^2] Chellapilla, K., et al. (2006). "High Performance Convolutional Neural Networks for Document Processing." ICFHR.

[^3] Lavin, A., & Gray, S. (2016). "Fast Algorithms for Convolutional Neural Networks." CVPR.

[^4] Mathieu, M., et al. (2014). "Fast Training of Convolutional Networks through FFTs." ICLR.

[^5] cuDNN Developer Guide: https://docs.nvidia.com/deeplearning/cudnn/developer-guide/

[^6] Hochreiter, S., & Schmidhuber, J. (1997). "Long Short-Term Memory." Neural Computation.

[^7] Cho, K., et al. (2014). "Learning Phrase Representations using RNN Encoder-Decoder." EMNLP.

[^8] Vaswani, A., et al. (2017). "Attention Is All You Need." NeurIPS.

[^9] Dao, T., et al. (2022). "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness." NeurIPS.

[^10] Shazeer, N. (2019). "Fast Transformer Decoding: One Write-Head is All You Need." arXiv.
