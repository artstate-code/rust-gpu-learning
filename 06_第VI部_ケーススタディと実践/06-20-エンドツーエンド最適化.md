[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬19ç« ](06-19-CNNãƒ»RNNãƒ»Transformerã‚’å®Ÿè£…ã™ã‚‹.md) | [â¡ï¸ ç¬¬21ç« ](06-21-å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ.md)

---

# ç¬¬ 17 ç« ã€€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æœ€é©åŒ–

ã“ã®ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªç’°å¢ƒã«å±•é–‹ã™ã‚‹ãŸã‚ã®ç·åˆçš„ãªæœ€é©åŒ–æ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã‹ã‚‰ã€ã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–ã€ã‚°ãƒ©ãƒ•æœ€é©åŒ–ã€ãã—ã¦æ§˜ã€…ãªãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¾ã§ã‚’ã‚«ãƒãƒ¼ã—ã¾ã™ã€‚

**ç›®çš„**: å®Ÿç”¨çš„ãªMLã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ã‚’æ¥µé™ã¾ã§å¼•ãå‡ºã—ã€æ§˜ã€…ãªç’°å¢ƒã§åŠ¹ç‡çš„ã«å‹•ä½œã•ã›ã‚‹æŠ€è¡“ã‚’ç¿’å¾—ã—ã¾ã™ã€‚

## 17.1 ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è§£æã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º

æœ€é©åŒ–ã®ç¬¬ä¸€æ­©ã¯ã€**æ­£ç¢ºãªè¨ˆæ¸¬**ã§ã™ã€‚ã©ã“ãŒé…ã„ã®ã‹ã‚’çŸ¥ã‚‰ãšã«æœ€é©åŒ–ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ [^1]ã€‚

[^1]: "Premature optimization is the root of all evil" - Donald Knuth

### GPU ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«

|| ãƒ„ãƒ¼ãƒ« | å¯¾è±¡ | ç‰¹å¾´ | ç”¨é€” |
||--------|------|------|------|
|| **NVIDIA Nsight Systems** | ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ | ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³è¡¨ç¤º | å…¨ä½“åƒæŠŠæ¡ |
|| **NVIDIA Nsight Compute** | ã‚«ãƒ¼ãƒãƒ«è©³ç´° | å‘½ä»¤ãƒ¬ãƒ™ãƒ«è§£æ | ã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ– |
|| **nvprof** | GPUæ€§èƒ½ | ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ | CI/CDçµ±åˆ |
|| **PyTorch Profiler** | PyTorch | è‡ªå‹•è¨ˆè£… | MLç‰¹åŒ– |
|| **Rust perf** | CPU | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° | CPUå´ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ |
|| **flamegraph** | CPU | å¯è¦–åŒ– | ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆç‰¹å®š |

### Nsight Systems ã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

**Nsight Systems** [^2] ã¯ã€ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®å‹•ä½œã‚’ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚

[^2]: NVIDIA Nsight Systems: https://developer.nvidia.com/nsight-systems

**ä½¿ç”¨æ‰‹é †**:

```bash
# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ï¼ˆPythonï¼‰
nsys profile -o resnet50_profile python train.py

# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ï¼ˆRustï¼‰
nsys profile -o rust_ml_profile ./target/release/ml_app

# çµæœè¡¨ç¤ºï¼ˆGUIï¼‰
nsys-ui resnet50_profile.nsys-rep
```

**åˆ†æé …ç›®**:

1. **GPUä½¿ç”¨ç‡**: ã‚¢ã‚¤ãƒ‰ãƒ«æ™‚é–“ãŒé•·ã„ â†’ ãƒãƒƒãƒã‚µã‚¤ã‚ºå¢—åŠ 
2. **ãƒ‡ãƒ¼ã‚¿è»¢é€**: H2D/D2HãŒé »ç¹ â†’ ãƒ‡ãƒ¼ã‚¿ã®GPUå¸¸é§åŒ–
3. **ã‚«ãƒ¼ãƒãƒ«èµ·å‹•**: å°ã•ã„ã‚«ãƒ¼ãƒãƒ«ãŒå¤šæ•° â†’ ã‚«ãƒ¼ãƒãƒ«èåˆ
4. **CPUå¾…æ©Ÿ**: GPUåŒæœŸå¾…ã¡ â†’ éåŒæœŸå®Ÿè¡Œ

### Pythonï¼ˆPyTorchï¼‰ã§ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```python
import torch
import torch.profiler as profiler

model = ResNet50().cuda()
inputs = torch.randn(32, 3, 224, 224).cuda()

with profiler.profile(
    activities=[
        profiler.ProfilerActivity.CPU,
        profiler.ProfilerActivity.CUDA,
    ],
    record_shapes=True,
    profile_memory=True,
    with_stack=True
) as prof:
    with profiler.record_function("model_forward"):
        outputs = model(inputs)

# çµæœè¡¨ç¤º
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))

# å‡ºåŠ›ä¾‹:
# -----------------------------------  ------------  ------------  ------------
# Name                                 Self CPU %   Self CUDA %  CUDA time avg
# -----------------------------------  ------------  ------------  ------------
# model_forward                        1.23%        100.00%      45.23ms
# aten::conv2d                         0.45%        67.82%       2.34ms
# aten::batch_norm                     0.12%        8.45%        0.45ms
# aten::relu                           0.08%        3.21%        0.12ms
# cudaMemcpyAsync                      0.34%        2.34%        0.98ms
# -----------------------------------  ------------  ------------  ------------

# TensorBoard ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
prof.export_chrome_trace("trace.json")
# chrome://tracing ã§é–‹ã
```

### Rust ã§ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

#### 1. cargo-flamegraphï¼ˆCPUï¼‰

```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
cargo install flamegraph

# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
cargo flamegraph --bin ml_app

# flamegraph.svg ãŒç”Ÿæˆã•ã‚Œã‚‹
```

**flamegraph ã®èª­ã¿æ–¹**:

- æ¨ªè»¸: CPUæ™‚é–“ã®å‰²åˆ
- ç¸¦è»¸: å‘¼ã³å‡ºã—ã‚¹ã‚¿ãƒƒã‚¯
- è‰²: ãƒ©ãƒ³ãƒ€ãƒ ï¼ˆæ„å‘³ãªã—ï¼‰
- å¹…ãŒåºƒã„ = ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆ

#### 2. perfï¼ˆLinuxï¼‰

```bash
# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
perf record --call-graph dwarf ./target/release/ml_app

# çµæœè¡¨ç¤º
perf report

# å‡ºåŠ›ä¾‹:
# Overhead  Command  Shared Object      Symbol
# 45.23%    ml_app   libblas.so         sgemm_
# 12.34%    ml_app   ml_app             conv2d_forward
# 8.76%     ml_app   libcudart.so       cudaLaunchKernel
```

#### 3. tch-rs ã§ã® GPU ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```rust
use tch::{nn, Tensor, Device};

fn profile_model() {
    let device = Device::Cuda(0);
    
    // CUDAã‚¤ãƒ™ãƒ³ãƒˆã§ã‚¿ã‚¤ãƒŸãƒ³ã‚°è¨ˆæ¸¬
    let start = tch::cuda::Event::new();
    let end = tch::cuda::Event::new();
    
    let model = create_resnet50();
    let input = Tensor::randn(&[32, 3, 224, 224], (tch::Kind::Float, device));
    
    // ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    for _ in 0..10 {
        let _ = model.forward(&input);
    }
    
    // è¨ˆæ¸¬
    start.record();
    for _ in 0..100 {
        let _ = model.forward(&input);
    }
    end.record();
    end.synchronize();
    
    let elapsed = start.elapsed_time(&end).unwrap();
    println!("å¹³å‡æ¨è«–æ™‚é–“: {:.3} ms", elapsed / 100.0);
    println!("ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {:.1} samples/sec", 32.0 * 100.0 / (elapsed / 1000.0));
}
```

### ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†é¡ã¨å¯¾ç­–

**1. ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…å¾‹é€Ÿ**ï¼ˆMemory Boundï¼‰

**ç—‡çŠ¶**: GPUä½¿ç”¨ç‡ãŒä½ã„ã€ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãŒå¤šã„

**å¯¾ç­–**:

- ãƒ¡ãƒ¢ãƒªåˆä½“ï¼ˆCoalescingï¼‰ã®æ”¹å–„
- å…±æœ‰ãƒ¡ãƒ¢ãƒªï¼ˆShared Memoryï¼‰ã®æ´»ç”¨
- ã‚«ãƒ¼ãƒãƒ«èåˆ

**2. è¨ˆç®—å¾‹é€Ÿ**ï¼ˆCompute Boundï¼‰

**ç—‡çŠ¶**: GPUä½¿ç”¨ç‡ãŒé«˜ã„ã€FLOPS ãŒç†è«–å€¤ã«è¿‘ã„

**å¯¾ç­–**:

- ã‚ˆã‚Šé«˜é€Ÿãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆWinogradã€FFTï¼‰
- Tensor Cores ã®æ´»ç”¨ï¼ˆFP16/BF16ï¼‰
- ã‚ˆã‚Šé«˜æ€§èƒ½ãªGPUã¸ã®ç§»è¡Œ

**3. è»¢é€å¾‹é€Ÿ**ï¼ˆTransfer Boundï¼‰

**ç—‡çŠ¶**: H2D/D2Hè»¢é€ãŒé »ç¹

**å¯¾ç­–**:

- ãƒ‡ãƒ¼ã‚¿ã®GPUå¸¸é§åŒ–
- ãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒª
- éåŒæœŸè»¢é€

**4. åŒæœŸå¾‹é€Ÿ**ï¼ˆSynchronization Boundï¼‰

**ç—‡çŠ¶**: CPUå¾…æ©Ÿæ™‚é–“ãŒé•·ã„

**å¯¾ç­–**:

- ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®æ´»ç”¨
- ã‚«ãƒ¼ãƒãƒ«ã®éåŒæœŸå®Ÿè¡Œ
- CPU/GPUé‡è¤‡å®Ÿè¡Œ

### å®Ÿæ¸¬ä¾‹ï¼ˆResNet-50æ¨è«–ï¼‰

**ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«çµæœ**:

|| å‡¦ç† | æ™‚é–“ï¼ˆmsï¼‰ | å‰²åˆ | ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ |
||------|-----------|------|------------|
|| Conv2D | 28.5 | 63% | è¨ˆç®— |
|| BatchNorm | 6.2 | 14% | ãƒ¡ãƒ¢ãƒª |
|| ReLU | 2.1 | 5% | ãƒ¡ãƒ¢ãƒª |
|| ãƒ‡ãƒ¼ã‚¿è»¢é€ | 4.8 | 11% | è»¢é€ |
|| ãã®ä»– | 3.4 | 7% | - |
|| **åˆè¨ˆ** | **45.0** | **100%** | - |

**æœ€é©åŒ–å‰å¾Œã®æ¯”è¼ƒ**:

|| æœ€é©åŒ– | åŠ¹æœï¼ˆmsï¼‰ | æ‰‹æ³• |
||--------|-----------|------|
|| 1. Tensor Coresä½¿ç”¨ï¼ˆFP16ï¼‰ | 28.5 â†’ 14.2 | cuDNNè‡ªå‹• |
|| 2. BatchNorm + ReLUèåˆ | 8.3 â†’ 4.1 | ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ« |
|| 3. ãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒª | 4.8 â†’ 1.6 | PyTorchè¨­å®š |
|| **åˆè¨ˆ** | 45.0 â†’ 21.9 | **2.05xé«˜é€ŸåŒ–** |

## 17.2 ã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–ãƒ»ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆ†å‰²

### ã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–ã®åŸå‰‡

**æœ€é©åŒ–ã®å„ªå…ˆé †ä½**ï¼ˆAmdahlã®æ³•å‰‡ã«åŸºã¥ãï¼‰:

1. **æœ€ã‚‚ãƒ›ãƒƒãƒˆãª** ã‚«ãƒ¼ãƒãƒ«ã‹ã‚‰æœ€é©åŒ–
2. **ä¸¦åˆ—åº¦** ã‚’æœ€å¤§åŒ–
3. **ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹** ã‚’æœ€å°åŒ–
4. **ãƒ¬ã‚¸ã‚¹ã‚¿ä½¿ç”¨é‡** ã‚’æœ€é©åŒ–

### Nsight Compute ã«ã‚ˆã‚‹ã‚«ãƒ¼ãƒãƒ«è§£æ

```bash
# ç‰¹å®šã‚«ãƒ¼ãƒãƒ«ã®è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
ncu --target-processes all --set full ./ml_app

# ä¸»è¦ãªæŒ‡æ¨™:
# - SM Efficiency: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ãƒƒã‚µã®åŠ¹ç‡
# - Occupancy: å æœ‰ç‡ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ—æ•° / æœ€å¤§ãƒ¯ãƒ¼ãƒ—æ•°ï¼‰
# - Memory Throughput: ãƒ¡ãƒ¢ãƒªã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
# - Compute Throughput: è¨ˆç®—ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
```

**å‡ºåŠ›ä¾‹**:

```
Kernel: matrix_multiply_kernel
Duration: 2.45 ms
Grid Size: (256, 256, 1)
Block Size: (16, 16, 1)
Registers Per Thread: 32
Shared Memory Per Block: 8192 bytes
Occupancy: 50.0% (ç†è«–å€¤: 100%)
Memory Throughput: 45% (ä½ã„ï¼)
Compute Throughput: 78%

ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: ãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…
æ¨å¥¨: 
  - ã‚¿ã‚¤ãƒ«åŒ–ã‚’ä½¿ã£ãŸå…±æœ‰ãƒ¡ãƒ¢ãƒªæ´»ç”¨
  - ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ”¹å–„
```

### è¡Œåˆ—ç©ã®æ®µéšçš„æœ€é©åŒ–

#### ãƒãƒ¼ã‚¸ãƒ§ãƒ³1: ç´ æœ´ãªå®Ÿè£…

```cuda
__global__ void matmul_naive(float* C, const float* A, const float* B, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < N && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < N; k++) {
            sum += A[row * N + k] * B[k * N + col];  // ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã‹ã‚‰æ¯å›èª­ã¿è¾¼ã¿
        }
        C[row * N + col] = sum;
    }
}

// æ€§èƒ½: 50 GFLOPSï¼ˆRTX 4090ã®ç†è«–å€¤ 82,580 GFLOPSã®0.06%ï¼‰
```

#### ãƒãƒ¼ã‚¸ãƒ§ãƒ³2: å…±æœ‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨

```cuda
#define TILE_SIZE 16

__global__ void matmul_shared(float* C, const float* A, const float* B, int N) {
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];
    
    int row = blockIdx.y * TILE_SIZE + threadIdx.y;
    int col = blockIdx.x * TILE_SIZE + threadIdx.x;
    
    float sum = 0.0f;
    
    for (int t = 0; t < N / TILE_SIZE; t++) {
        // ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªã‹ã‚‰å…±æœ‰ãƒ¡ãƒ¢ãƒªã¸ï¼ˆåˆä½“ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
        As[threadIdx.y][threadIdx.x] = A[row * N + t * TILE_SIZE + threadIdx.x];
        Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];
        
        __syncthreads();  // å…¨ã‚¹ãƒ¬ãƒƒãƒ‰åŒæœŸ
        
        // å…±æœ‰ãƒ¡ãƒ¢ãƒªã‹ã‚‰è¨ˆç®—ï¼ˆé«˜é€Ÿï¼‰
        for (int k = 0; k < TILE_SIZE; k++) {
            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];
        }
        
        __syncthreads();
    }
    
    if (row < N && col < N) {
        C[row * N + col] = sum;
    }
}

// æ€§èƒ½: 2,500 GFLOPSï¼ˆ50å€é«˜é€ŸåŒ–ï¼‰
```

#### ãƒãƒ¼ã‚¸ãƒ§ãƒ³3: Tensor Cores ä½¿ç”¨ï¼ˆFP16ï¼‰

```cuda
#include <mma.h>
using namespace nvcuda::wmma;

__global__ void matmul_tensor_core(
    half* C, const half* A, const half* B, int M, int N, int K
) {
    // Tensor Coreç”¨ã®16x16x16è¡Œåˆ—ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆ
    fragment<matrix_a, 16, 16, 16, half, row_major> a_frag;
    fragment<matrix_b, 16, 16, 16, half, col_major> b_frag;
    fragment<accumulator, 16, 16, 16, half> c_frag;
    
    fill_fragment(c_frag, 0.0h);
    
    int warp_row = (blockIdx.y * blockDim.y + threadIdx.y) / 32;
    int warp_col = (blockIdx.x * blockDim.x + threadIdx.x) / 32;
    
    for (int i = 0; i < K; i += 16) {
        int a_row = warp_row * 16;
        int a_col = i;
        int b_row = i;
        int b_col = warp_col * 16;
        
        // Tensor Coreã§è¨ˆç®—ï¼ˆè‡ªå‹•æœ€é©åŒ–ï¼‰
        load_matrix_sync(a_frag, A + a_row * K + a_col, K);
        load_matrix_sync(b_frag, B + b_row * N + b_col, N);
        mma_sync(c_frag, a_frag, b_frag, c_frag);
    }
    
    // çµæœã‚’æ›¸ãæˆ»ã—
    store_matrix_sync(C + warp_row * 16 * N + warp_col * 16, c_frag, N, mem_row_major);
}

// æ€§èƒ½: 35,000 GFLOPSï¼ˆ700å€é«˜é€ŸåŒ–ã€ç†è«–å€¤ã®42%ï¼‰
```

**æ€§èƒ½æ¯”è¼ƒ**:

|| å®Ÿè£… | GFLOPS | ç›¸å¯¾é€Ÿåº¦ | ç†è«–åŠ¹ç‡ |
||------|--------|---------|---------|
|| ç´ æœ´ | 50 | 1x | 0.06% |
|| å…±æœ‰ãƒ¡ãƒ¢ãƒª | 2,500 | 50x | 3% |
|| Tensor Coreï¼ˆFP16ï¼‰ | 35,000 | 700x | 42% |
|| cuBLASï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰ | 70,000 | 1400x | 85% |

### Rust ã‹ã‚‰ã®ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«å‘¼ã³å‡ºã—

```rust
use cudarc::driver::*;

const PTX_SRC: &str = r#"
.version 7.0
.target sm_80
.address_size 64

.visible .entry matmul_shared(
    .param .u64 C,
    .param .u64 A,
    .param .u64 B,
    .param .u32 N
) {
    // ... PTXã‚³ãƒ¼ãƒ‰ ...
}
"#;

fn matmul_gpu(
    a: &[f32],
    b: &[f32],
    n: usize,
) -> Result<Vec<f32>, CudaError> {
    let device = CudaDevice::new(0)?;
    
    // ã‚«ãƒ¼ãƒãƒ«ãƒ­ãƒ¼ãƒ‰
    let ptx = CudaModule::from_ptx(PTX_SRC, "matmul", &[])?;
    let kernel = device.get_func(&ptx, "matmul_shared")?;
    
    // GPU ãƒ¡ãƒ¢ãƒªç¢ºä¿ãƒ»è»¢é€
    let d_a = device.htod_copy(a)?;
    let d_b = device.htod_copy(b)?;
    let mut d_c = device.alloc_zeros::<f32>(n * n)?;
    
    // ã‚«ãƒ¼ãƒãƒ«èµ·å‹•
    let block_size = 16;
    let grid_size = (n + block_size - 1) / block_size;
    let cfg = LaunchConfig {
        grid_dim: (grid_size as u32, grid_size as u32, 1),
        block_dim: (block_size as u32, block_size as u32, 1),
        shared_mem_bytes: 0,
    };
    
    unsafe {
        kernel.launch(cfg, (&d_c, &d_a, &d_b, n as u32))?;
    }
    
    // çµæœå–å¾—
    device.dtoh_sync_copy(&d_c)
}
```

### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆ†å‰²

**ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—åŒ–**ï¼ˆPipeline Parallelismï¼‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã«åˆ†å‰²ã—ã€ç•°ãªã‚‹GPUã§ä¸¦åˆ—å®Ÿè¡Œã—ã¾ã™ [^3]ã€‚

[^3]: Huang, Y., et al. (2019). "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism." NeurIPS.

**GPipeã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```python
import torch
import torch.nn as nn

class PipelineModel(nn.Module):
    def __init__(self, num_layers, hidden_size, num_partitions):
        super().__init__()
        self.num_partitions = num_partitions
        layers_per_partition = num_layers // num_partitions
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’åˆ†å‰²
        self.partitions = nn.ModuleList()
        for i in range(num_partitions):
            partition = nn.Sequential(*[
                TransformerLayer(hidden_size)
                for _ in range(layers_per_partition)
            ]).cuda(i)  # GPU i ã«é…ç½®
            self.partitions.append(partition)
    
    def forward(self, x, micro_batch_size=32):
        batch_size = x.size(0)
        num_micro_batches = batch_size // micro_batch_size
        
        # ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã«åˆ†å‰²
        micro_batches = x.chunk(num_micro_batches)
        outputs = []
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        for micro_batch in micro_batches:
            h = micro_batch
            for i, partition in enumerate(self.partitions):
                h = h.cuda(i)  # GPU i ã«è»¢é€
                h = partition(h)
            outputs.append(h.cpu())
        
        return torch.cat(outputs, dim=0)

# ä½¿ç”¨ä¾‹
model = PipelineModel(num_layers=24, hidden_size=1024, num_partitions=4)
x = torch.randn(128, 512, 1024)  # ãƒãƒƒãƒã‚µã‚¤ã‚º128
output = model(x, micro_batch_size=32)
```

**ãƒãƒ–ãƒ«ã‚¿ã‚¤ãƒ **ï¼ˆBubble Timeï¼‰:

ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æœ€åˆã¨æœ€å¾Œã§GPUãŒã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã«ãªã‚‹æ™‚é–“ã€‚

\[
\text{Bubble Ratio} = \frac{N*{partitions} - 1}{N*{micro*batches}}
\]

**æœ€é©åŒ–**:

- ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒæ•°ã‚’å¢—ã‚„ã™ï¼ˆãŸã ã—ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰
- 1F1Bï¼ˆOne-Forward-One-Backwardï¼‰ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°

## 17.3 æ¼”ç®—å­èåˆã¨ã‚°ãƒ©ãƒ•æœ€é©åŒ–

**æ¼”ç®—å­èåˆ**ï¼ˆOperator Fusionï¼‰ã¯ã€è¤‡æ•°ã®æ¼”ç®—ã‚’1ã¤ã®ã‚«ãƒ¼ãƒãƒ«ã«ã¾ã¨ã‚ã¦ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’å‰Šæ¸›ã™ã‚‹æŠ€è¡“ã§ã™ [^4]ã€‚

[^4]: Chen, T., et al. (2018). "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning." OSDI.

### èåˆãƒ‘ã‚¿ãƒ¼ãƒ³

#### 1. è¦ç´ ã”ã¨æ¼”ç®—ã®èåˆï¼ˆElementwise Fusionï¼‰

**èåˆå‰**:

```python
# 3å›ã®ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ï¼ˆå„æ¼”ç®—ã§èª­ã¿æ›¸ãï¼‰
y = x + bias    # ã‚«ãƒ¼ãƒãƒ«1
z = relu(y)     # ã‚«ãƒ¼ãƒãƒ«2
out = z * scale # ã‚«ãƒ¼ãƒãƒ«3
```

**èåˆå¾Œ**:

```python
# 1å›ã®ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹
out = relu(x + bias) * scale  # 1ã¤ã®ã‚«ãƒ¼ãƒãƒ«
```

**Rustï¼ˆcudarcï¼‰ã§ã®å®Ÿè£…**:

```cuda
__global__ void fused_add_relu_scale(
    float* out,
    const float* x,
    const float* bias,
    float scale,
    int n
) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float val = x[i] + bias[i];      // åŠ ç®—
        val = val > 0.0f ? val : 0.0f;   // ReLU
        out[i] = val * scale;            // ã‚¹ã‚±ãƒ¼ãƒ«
    }
}
```

**æ€§èƒ½å‘ä¸Š**: 3-4å€ï¼ˆãƒ¡ãƒ¢ãƒªãƒãƒ³ãƒ‰å¹…å¾‹é€Ÿã®å ´åˆï¼‰

#### 2. Batch Normalization + ReLU èåˆ

```cuda
__global__ void fused_batchnorm_relu(
    float* out,
    const float* x,
    const float* mean,
    const float* var,
    const float* gamma,
    const float* beta,
    float eps,
    int n
) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        // Batch Normalization
        float x_norm = (x[i] - mean[i]) / sqrtf(var[i] + eps);
        float x_scaled = gamma[i] * x_norm + beta[i];
        
        // ReLU
        out[i] = x_scaled > 0.0f ? x_scaled : 0.0f;
    }
}
```

#### 3. Convolution + Batch Norm + ReLU èåˆ

**cuDNN ã§ã®è‡ªå‹•èåˆ**:

```python
import torch
import torch.nn as nn

# PyTorch 2.0+ ã§ã¯è‡ªå‹•èåˆã•ã‚Œã‚‹
model = nn.Sequential(
    nn.Conv2d(3, 64, 3, padding=1),
    nn.BatchNorm2d(64),
    nn.ReLU(inplace=True)
).cuda()

# torch.compile ã§èåˆãŒæœ‰åŠ¹åŒ–
model = torch.compile(model)

x = torch.randn(32, 3, 224, 224).cuda()
y = model(x)  # å†…éƒ¨ã§èåˆã‚«ãƒ¼ãƒãƒ«ãŒä½¿ã‚ã‚Œã‚‹
```

### ã‚°ãƒ©ãƒ•æœ€é©åŒ–

**è¨ˆç®—ã‚°ãƒ©ãƒ•**ï¼ˆComputational Graphï¼‰ã®æœ€é©åŒ–ã«ã‚ˆã‚Šã€å®Ÿè¡ŒåŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚

#### æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³

**1. å®šæ•°ç•³ã¿è¾¼ã¿**ï¼ˆConstant Foldingï¼‰

```python
# æœ€é©åŒ–å‰
y = x * 2 * 3  # 2å›ã®ä¹—ç®—

# æœ€é©åŒ–å¾Œ
y = x * 6      # 1å›ã®ä¹—ç®—ï¼ˆ2*3=6ã¯äº‹å‰è¨ˆç®—ï¼‰
```

**2. å…±é€šéƒ¨åˆ†å¼ã®é™¤å»**ï¼ˆCommon Subexpression Eliminationï¼‰

```python
# æœ€é©åŒ–å‰
a = x * y + z
b = x * y - z  # x*yãŒé‡è¤‡

# æœ€é©åŒ–å¾Œ
tmp = x * y
a = tmp + z
b = tmp - z
```

**3. ãƒ‡ãƒƒãƒ‰ã‚³ãƒ¼ãƒ‰é™¤å»**ï¼ˆDead Code Eliminationï¼‰

```python
# æœ€é©åŒ–å‰
y = x * 2
z = x * 3  # ä½¿ã‚ã‚Œãªã„
return y

# æœ€é©åŒ–å¾Œ
y = x * 2
return y  # zã¯å‰Šé™¤
```

### ONNX ã«ã‚ˆã‚‹ã‚°ãƒ©ãƒ•æœ€é©åŒ–

**ONNX Runtime** [^5] ã¯ã€è‡ªå‹•çš„ã«ã‚°ãƒ©ãƒ•æœ€é©åŒ–ã‚’è¡Œã„ã¾ã™ã€‚

[^5]: ONNX Runtime: https://onnxruntime.ai/

**Python ã§ã®ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¨æœ€é©åŒ–**:

```python
import torch
import onnx
from onnxruntime.transformers import optimizer

# PyTorch ãƒ¢ãƒ‡ãƒ«
model = MyModel().eval()
dummy_input = torch.randn(1, 3, 224, 224)

# ONNX ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    input_names=["input"],
    output_names=["output"],
    dynamic_axes={
        "input": {0: "batch_size"},
        "output": {0: "batch_size"}
    }
)

# ONNX ã‚°ãƒ©ãƒ•æœ€é©åŒ–
from onnxruntime.transformers.onnx_model import OnnxModel
onnx_model = OnnxModel(onnx.load("model.onnx"))

# æœ€é©åŒ–ã‚’é©ç”¨
onnx_model.fuse_reshape()
onnx_model.fuse_shape()
onnx_model.remove_unused_constant()

onnx_model.save_model_to_file("model_optimized.onnx")
```

**æœ€é©åŒ–åŠ¹æœ**:

|| æœ€é©åŒ– | åŠ¹æœ |
||--------|------|
|| ãƒãƒ¼ãƒ‰æ•°å‰Šæ¸› | 234 â†’ 167 (29%å‰Šæ¸›) |
|| æ¨è«–æ™‚é–“ | 45.2ms â†’ 32.1ms (29%é«˜é€ŸåŒ–) |
|| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | 1.2GB â†’ 0.9GB (25%å‰Šæ¸›) |

### Rust ã§ã® ONNX æ¨è«–

```rust
use tract_onnx::prelude::*;

fn run_onnx_inference() -> TractResult<()> {
    // ONNXãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    let model = tract_onnx::onnx()
        .model_for_path("model_optimized.onnx")?
        .with_input_fact(0, InferenceFact::dt_shape(f32::datum_type(), tvec!(1, 3, 224, 224)))?
        .into_optimized()?  // ã‚°ãƒ©ãƒ•æœ€é©åŒ–
        .into_runnable()?;
    
    // å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æº–å‚™
    let input = Tensor::from_shape(
        &[1, 3, 224, 224],
        &vec![0.0f32; 3 * 224 * 224]
    )?;
    
    // æ¨è«–å®Ÿè¡Œ
    let result = model.run(tvec!(input.into()))?;
    
    // çµæœå–å¾—
    let output: ArrayD<f32> = result[0].to_array_view::<f32>()?.to_owned();
    
    println!("å‡ºåŠ›å½¢çŠ¶: {:?}", output.shape());
    
    Ok(())
}
```

## 17.4 ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»æ¨è«–ã‚µãƒ¼ãƒæ§‹ç¯‰

æœ¬ç•ªç’°å¢ƒã§ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã«ã¯ã€**é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**ã¨**ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚

### æ¨è«–ã‚µãƒ¼ãƒã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µ â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
   â”‚       â”‚       â”‚       â”‚
â”Œâ”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”
â”‚ API  â”‚ â”‚ API  â”‚ â”‚ API  â”‚ â”‚ API  â”‚
â”‚Serverâ”‚ â”‚Serverâ”‚ â”‚Serverâ”‚ â”‚Serverâ”‚
â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜
   â”‚       â”‚       â”‚       â”‚
â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚    Model Inference Engine     â”‚
â”‚  (ãƒãƒƒãƒãƒ³ã‚°ãƒ»GPUç®¡ç†)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
      â”‚  GPU 0  â”‚
      â”‚  GPU 1  â”‚
      â”‚  GPU 2  â”‚
      â”‚  GPU 3  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Rustï¼ˆActix-Webï¼‰ã§ã®æ¨è«–ã‚µãƒ¼ãƒ

```rust
use actix_web::{web, App, HttpServer, HttpResponse};
use serde::{Deserialize, Serialize};
use tch::{nn, Tensor, Device, CModule};
use std::sync::{Arc, Mutex};

#[derive(Deserialize)]
struct InferenceRequest {
    image: Vec<Vec<Vec<f32>>>,  // [C, H, W]
}

#[derive(Serialize)]
struct InferenceResponse {
    predictions: Vec<f32>,
    latency_ms: f64,
}

struct AppState {
    model: Arc<Mutex<CModule>>,
    device: Device,
}

async fn predict(
    data: web::Json<InferenceRequest>,
    state: web::Data<AppState>,
) -> HttpResponse {
    let start = std::time::Instant::now();
    
    // å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
    let image = &data.image;
    let tensor = Tensor::of_slice3(image)
        .unsqueeze(0)  // ãƒãƒƒãƒæ¬¡å…ƒè¿½åŠ 
        .to_device(state.device);
    
    // æ¨è«–å®Ÿè¡Œ
    let model = state.model.lock().unwrap();
    let output = model.forward_ts(&[tensor]).unwrap();
    
    // çµæœã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
    let predictions: Vec<f32> = output.try_into().unwrap();
    
    let latency = start.elapsed().as_secs_f64() * 1000.0;
    
    HttpResponse::Ok().json(InferenceResponse {
        predictions,
        latency_ms: latency,
    })
}

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    // ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    let device = Device::Cuda(0);
    let model = tch::CModule::load("model_traced.pt").unwrap();
    
    let state = web::Data::new(AppState {
        model: Arc::new(Mutex::new(model)),
        device,
    });
    
    // HTTPã‚µãƒ¼ãƒèµ·å‹•
    HttpServer::new(move || {
        App::new()
            .app_data(state.clone())
            .route("/predict", web::post().to(predict))
    })
    .bind("0.0.0.0:8080")?
    .run()
    .await
}
```

### å‹•çš„ãƒãƒƒãƒãƒ³ã‚°

**å‹•çš„ãƒãƒƒãƒãƒ³ã‚°**ï¼ˆDynamic Batchingï¼‰ã¯ã€è¤‡æ•°ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã¾ã¨ã‚ã¦æ¨è«–ã™ã‚‹ã“ã¨ã§ã€GPUã®åˆ©ç”¨åŠ¹ç‡ã‚’é«˜ã‚ã¾ã™ [^6]ã€‚

[^6]: NVIDIA Triton Inference Server: https://github.com/triton-inference-server/server

```rust
use tokio::sync::mpsc;
use tokio::time::{sleep, Duration};

struct BatchingEngine {
    batch_size: usize,
    timeout_ms: u64,
    model: CModule,
}

impl BatchingEngine {
    async fn run(&self, rx: mpsc::Receiver<(Tensor, oneshot::Sender<Tensor>)>) {
        let mut pending_requests = Vec::new();
        let mut deadline = Instant::now() + Duration::from_millis(self.timeout_ms);
        
        loop {
            select! {
                Some((input, response_tx)) = rx.recv() => {
                    pending_requests.push((input, response_tx));
                    
                    // ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰æ¨è«–å®Ÿè¡Œ
                    if pending_requests.len() >= self.batch_size {
                        self.process_batch(&mut pending_requests).await;
                        deadline = Instant::now() + Duration::from_millis(self.timeout_ms);
                    }
                }
                _ = sleep_until(deadline) => {
                    // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: æºœã¾ã£ã¦ã„ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†
                    if !pending_requests.is_empty() {
                        self.process_batch(&mut pending_requests).await;
                    }
                    deadline = Instant::now() + Duration::from_millis(self.timeout_ms);
                }
            }
        }
    }
    
    async fn process_batch(&self, requests: &mut Vec<(Tensor, oneshot::Sender<Tensor>)>) {
        // å…¥åŠ›ã‚’ãƒãƒƒãƒåŒ–
        let inputs: Vec<Tensor> = requests.iter().map(|(t, _)| t.shallow_clone()).collect();
        let batch = Tensor::cat(&inputs, 0);
        
        // ãƒãƒƒãƒæ¨è«–
        let outputs = self.model.forward_ts(&[batch]).unwrap();
        
        // çµæœã‚’åˆ†å‰²ã—ã¦å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«è¿”ã™
        for (i, (_, response_tx)) in requests.drain(..).enumerate() {
            let output = outputs.get(i as i64);
            let _ = response_tx.send(output);
        }
    }
}
```

**å‹•çš„ãƒãƒƒãƒãƒ³ã‚°ã®åŠ¹æœ**:

|| è¨­å®š | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆmsï¼‰ | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆreq/sï¼‰ |
||------|----------------|-------------------|
|| ãƒãƒƒãƒãªã— | 15 | 67 |
|| ãƒãƒƒãƒã‚µã‚¤ã‚º4 | 25 | 160 |
|| ãƒãƒƒãƒã‚µã‚¤ã‚º8 | 35 | 228 |
|| ãƒãƒƒãƒã‚µã‚¤ã‚º16 | 50 | 320 |

### TensorRT ã«ã‚ˆã‚‹æœ€é©åŒ–

**NVIDIA TensorRT** [^7] ã¯ã€æ¨è«–å°‚ç”¨ã®é«˜åº¦ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚

[^7]: NVIDIA TensorRT: https://developer.nvidia.com/tensorrt

**æœ€é©åŒ–æ‰‹æ³•**:

1. **å±¤èåˆ**: Conv+BN+ReLU ã‚’1ã¤ã®ã‚«ãƒ¼ãƒãƒ«ã«
2. **ç²¾åº¦æœ€é©åŒ–**: FP16/INT8 é‡å­åŒ–
3. **ã‚«ãƒ¼ãƒãƒ«é¸æŠ**: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«æœ€é©ãªã‚«ãƒ¼ãƒãƒ«
4. **å‹•çš„ãƒ†ãƒ³ã‚½ãƒ«ãƒ¡ãƒ¢ãƒª**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€å°åŒ–

**Python ã§ã®ä½¿ç”¨**:

```python
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

# TensorRT ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

def build_engine(onnx_path, engine_path):
    builder = trt.Builder(TRT_LOGGER)
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    parser = trt.OnnxParser(network, TRT_LOGGER)
    
    # ONNX ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    with open(onnx_path, 'rb') as model:
        parser.parse(model.read())
    
    # ãƒ“ãƒ«ãƒ‰è¨­å®š
    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30  # 1GB
    config.set_flag(trt.BuilderFlag.FP16)  # FP16 æœ‰åŠ¹åŒ–
    
    # ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰
    engine = builder.build_engine(network, config)
    
    # ä¿å­˜
    with open(engine_path, 'wb') as f:
        f.write(engine.serialize())
    
    return engine

# æ¨è«–å®Ÿè¡Œ
def infer(engine, input_data):
    context = engine.create_execution_context()
    
    # GPU ãƒ¡ãƒ¢ãƒªç¢ºä¿
    d_input = cuda.mem_alloc(input_data.nbytes)
    d_output = cuda.mem_alloc(output_size)
    
    # ãƒ‡ãƒ¼ã‚¿è»¢é€
    cuda.memcpy_htod(d_input, input_data)
    
    # æ¨è«–
    context.execute_v2([int(d_input), int(d_output)])
    
    # çµæœå–å¾—
    cuda.memcpy_dtoh(output_data, d_output)
    
    return output_data
```

**æ€§èƒ½å‘ä¸Š**:

|| å®Ÿè£… | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆmsï¼‰ | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆimg/sï¼‰ |
||------|----------------|-------------------|
|| PyTorchï¼ˆFP32ï¼‰ | 15.2 | 67 |
|| PyTorchï¼ˆFP16ï¼‰ | 8.1 | 125 |
|| TensorRTï¼ˆFP16ï¼‰ | 3.2 | 315 |
|| TensorRTï¼ˆINT8ï¼‰ | 1.8 | 560 |

## 17.5 WebGPU çµŒç”±ã§ã®ãƒ–ãƒ©ã‚¦ã‚¶æ¨è«–

**WebGPU** [^8] ã¯ã€ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰GPUã‚’ç›´æ¥åˆ¶å¾¡ã§ãã‚‹æ–°ã—ã„APIã§ã™ã€‚

[^8] WebGPU Specification: https://www.w3.org/TR/webgpu/

### WebGPU vs WebGL

|| é …ç›® | WebGL | WebGPU |
||------|-------|--------|
|| APIè¨­è¨ˆ | ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚¹å°‚ç”¨ | æ±ç”¨è¨ˆç®—å¯¾å¿œ |
|| ã‚·ã‚§ãƒ¼ãƒ€è¨€èª | GLSL | WGSL |
|| è¨ˆç®—ã‚·ã‚§ãƒ¼ãƒ€ | é™å®šçš„ | å®Œå…¨å¯¾å¿œ |
|| ãƒ¡ãƒ¢ãƒªç®¡ç† | æš—é»™çš„ | æ˜ç¤ºçš„ |
|| æ€§èƒ½ | ä¸­ | é«˜ |

### Rustï¼ˆwgpuï¼‰ã§ã®å®Ÿè£…

```rust
use wgpu::util::DeviceExt;

async fn matrix_multiply_webgpu(
    a: &[f32],
    b: &[f32],
    n: usize,
) -> Vec<f32> {
    // WebGPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    let instance = wgpu::Instance::new(wgpu::Backends::all());
    let adapter = instance
        .request_adapter(&wgpu::RequestAdapterOptions::default())
        .await
        .unwrap();
    
    let (device, queue) = adapter
        .request_device(&wgpu::DeviceDescriptor::default(), None)
        .await
        .unwrap();
    
    // ã‚·ã‚§ãƒ¼ãƒ€ã‚³ãƒ¼ãƒ‰ï¼ˆWGSLï¼‰
    let shader = device.create_shader_module(&wgpu::ShaderModuleDescriptor {
        label: None,
        source: wgpu::ShaderSource::Wgsl(r#"
            @group(0) @binding(0) var<storage, read> a: array<f32>;
            @group(0) @binding(1) var<storage, read> b: array<f32>;
            @group(0) @binding(2) var<storage, read_write> result: array<f32>;
            @group(0) @binding(3) var<uniform> n: u32;
            
            @compute @workgroup_size(16, 16)
            fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
                let row = global_id.x;
                let col = global_id.y;
                
                if (row >= n || col >= n) {
                    return;
                }
                
                var sum = 0.0;
                for (var k = 0u; k < n; k = k + 1u) {
                    sum += a[row * n + k] * b[k * n + col];
                }
                
                result[row * n + col] = sum;
            }
        "#.into()),
    });
    
    // ãƒãƒƒãƒ•ã‚¡ä½œæˆ
    let buffer_a = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
        label: None,
        contents: bytemuck::cast_slice(a),
        usage: wgpu::BufferUsages::STORAGE,
    });
    
    let buffer_b = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
        label: None,
        contents: bytemuck::cast_slice(b),
        usage: wgpu::BufferUsages::STORAGE,
    });
    
    let buffer_result = device.create_buffer(&wgpu::BufferDescriptor {
        label: None,
        size: (n * n * std::mem::size_of::<f32>()) as u64,
        usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_SRC,
        mapped_at_creation: false,
    });
    
    // ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
    let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
        label: None,
        layout: None,
        module: &shader,
        entry_point: "main",
    });
    
    // ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
    let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor::default());
    {
        let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor::default());
        compute_pass.set_pipeline(&compute_pipeline);
        compute_pass.set_bind_group(0, &bind_group, &[]);
        compute_pass.dispatch(
            (n as u32 + 15) / 16,
            (n as u32 + 15) / 16,
            1
        );
    }
    
    queue.submit(Some(encoder.finish()));
    
    // çµæœå–å¾—
    let buffer_slice = buffer_result.slice(..);
    let (tx, rx) = futures::channel::oneshot::channel();
    buffer_slice.map_async(wgpu::MapMode::Read, move |result| {
        tx.send(result).unwrap();
    });
    device.poll(wgpu::Maintain::Wait);
    rx.await.unwrap().unwrap();
    
    let data = buffer_slice.get_mapped_range();
    let result: Vec<f32> = bytemuck::cast_slice(&data).to_vec();
    
    result
}
```

### WebAssembly + WebGPU

**Rust â†’ WASM â†’ ãƒ–ãƒ©ã‚¦ã‚¶** ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³:

```bash
# WASMã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¿½åŠ 
rustup target add wasm32-unknown-unknown

# wasm-pack ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
cargo install wasm-pack

# WASMãƒ“ãƒ«ãƒ‰
wasm-pack build --target web
```

**Rustå´ï¼ˆlib.rsï¼‰**:

```rust
use wasm_bindgen::prelude::*;

#[wasm_bindgen]
pub async fn run_inference(image_data: &[u8]) -> Vec<f32> {
    // WebGPU ã§æ¨è«–
    let result = webgpu_inference(image_data).await;
    result
}
```

**JavaScriptå´**:

```javascript
import init, { run_inference } from './pkg/ml_wasm.js';

async function main() {
    await init();
    
    const imageData = new Uint8Array(/* ç”»åƒãƒ‡ãƒ¼ã‚¿ */);
    const predictions = await run_inference(imageData);
    
    console.log('äºˆæ¸¬çµæœ:', predictions);
}
```

**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**ï¼ˆResNet-18æ¨è«–ï¼‰:

|| ç’°å¢ƒ | æ¨è«–æ™‚é–“ï¼ˆmsï¼‰ | ç›¸å¯¾é€Ÿåº¦ |
||------|--------------|---------|
|| Native CPU | 150 | 1x |
|| WASMï¼ˆCPUï¼‰ | 380 | 0.39x |
|| WebGL | 45 | 3.3x |
|| WebGPU | 28 | 5.4x |
|| Native CUDA | 5 | 30x |

## 17.6 çµ„ã¿è¾¼ã¿ãƒ»ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã¸ã®å±•é–‹

**ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹**ï¼ˆEdge Deviceï¼‰ã§ã®æ©Ÿæ¢°å­¦ç¿’æ¨è«–ã¯ã€**çœé›»åŠ›**ã¨**ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**ãŒé‡è¦ã§ã™ã€‚

### ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒã‚¤ã‚¹

|| ãƒ‡ãƒã‚¤ã‚¹ | GPU/NPU | ãƒ¡ãƒ¢ãƒª | ç”¨é€” |
||---------|---------|--------|------|
|| Raspberry Pi 4 | ãªã— | 4-8GB | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ— |
|| NVIDIA Jetson Nano | Maxwell GPU | 4GB | ä½ã‚³ã‚¹ãƒˆã‚¨ãƒƒã‚¸AI |
|| NVIDIA Jetson Xavier NX | Volta GPU | 8GB | é«˜æ€§èƒ½ã‚¨ãƒƒã‚¸AI |
|| Coral Dev Board | Edge TPU | 1GB | Googleç”Ÿæ…‹ç³» |
|| Apple Neural Engine | ANE | - | iOS/macOS |

### é‡å­åŒ–ï¼ˆQuantizationï¼‰

**INT8é‡å­åŒ–**ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨è¨ˆç®—é‡ã‚’å‰Šæ¸› [^9]ã€‚

[^9]: Jacob, B., et al. (2018). "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference." CVPR.

**é‡å­åŒ–ã®æ•°å¼**:

\[
x*{int8} = \text{round}\left(\frac{x*{fp32} - z}{s}\right)
\]

ã“ã“ã§ã€\(s\) ã¯ã‚¹ã‚±ãƒ¼ãƒ«ã€\(z\) ã¯ã‚¼ãƒ­ãƒã‚¤ãƒ³ãƒˆã€‚

**Pythonï¼ˆPyTorchï¼‰ã§ã®é‡å­åŒ–**:

```python
import torch
import torch.quantization

model = ResNet50().eval()

# é‡å­åŒ–è¨­å®š
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')

# é‡å­åŒ–æº–å‚™
model_prepared = torch.quantization.prepare(model)

# ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä»£è¡¨çš„ãªãƒ‡ãƒ¼ã‚¿ã§çµ±è¨ˆåé›†ï¼‰
for data in calibration_data:
    model_prepared(data)

# é‡å­åŒ–å®Ÿè¡Œ
model_quantized = torch.quantization.convert(model_prepared)

# ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºæ¯”è¼ƒ
print(f"FP32: {get_model_size(model):.2f} MB")
print(f"INT8: {get_model_size(model_quantized):.2f} MB")
# å‡ºåŠ›:
# FP32: 97.8 MB
# INT8: 24.4 MB (4å€å‰Šæ¸›)

# æ¨è«–é€Ÿåº¦æ¯”è¼ƒ
import time
x = torch.randn(1, 3, 224, 224)

start = time.time()
for _ in range(100):
    _ = model(x)
time_fp32 = time.time() - start

start = time.time()
for _ in range(100):
    _ = model_quantized(x)
time_int8 = time.time() - start

print(f"FP32: {time_fp32*10:.2f} ms/inference")
print(f"INT8: {time_int8*10:.2f} ms/inference")
print(f"é«˜é€ŸåŒ–ç‡: {time_fp32/time_int8:.2f}x")
# å‡ºåŠ›:
# FP32: 45.2 ms/inference
# INT8: 18.3 ms/inference
# é«˜é€ŸåŒ–ç‡: 2.47x
```

### NVIDIA Jetson ã§ã®å±•é–‹

**Jetson Nano ã§ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**:

```bash
# JetPack SDK ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆUbuntu 18.04ãƒ™ãƒ¼ã‚¹ï¼‰
sudo apt update
sudo apt install nvidia-jetpack

# Rust ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# ã‚¯ãƒ­ã‚¹ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«è¨­å®š
rustup target add aarch64-unknown-linux-gnu
```

**Cargo.toml**:

```toml
[dependencies]
tch = "0.13"  # LibTorch ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
```

**æ¨è«–ã‚³ãƒ¼ãƒ‰**:

```rust
use tch::{nn, Tensor, Device, CModule};

fn main() {
    // Jetson ã® GPU ä½¿ç”¨
    let device = Device::Cuda(0);
    
    // TorchScript ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    let model = CModule::load("model_jetson.pt").unwrap();
    
    // ã‚«ãƒ¡ãƒ©ã‹ã‚‰ç”»åƒå–å¾—ï¼ˆçœç•¥ï¼‰
    let image = capture_image();
    let tensor = preprocess(image).to_device(device);
    
    // æ¨è«–
    let output = model.forward_ts(&[tensor]).unwrap();
    let predictions = postprocess(output);
    
    println!("äºˆæ¸¬: {:?}", predictions);
}
```

### ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆPruningï¼‰

**ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°**ã¯ã€é‡è¦ã§ãªã„é‡ã¿ã‚’å‰Šé™¤ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è»½é‡åŒ–ã—ã¾ã™ [^10]ã€‚

[^10]: Han, S., et al. (2015). "Learning both Weights and Connections for Efficient Neural Networks." NeurIPS.

**ãƒã‚°ãƒ‹ãƒãƒ¥ãƒ¼ãƒ‰ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°**:

```python
import torch
import torch.nn.utils.prune as prune

model = ResNet50()

# ç•³ã¿è¾¼ã¿å±¤ã®50%ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Conv2d):
        prune.l1_unstructured(module, name='weight', amount=0.5)

# ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ’ä¹…åŒ–
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Conv2d):
        prune.remove(module, 'weight')

# ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ç¢ºèª
def count_zero_weights(model):
    total = 0
    zero = 0
    for param in model.parameters():
        total += param.numel()
        zero += (param == 0).sum().item()
    return zero / total * 100

print(f"ã‚¹ãƒ‘ãƒ¼ã‚¹ç‡: {count_zero_weights(model):.1f}%")
# å‡ºåŠ›: ã‚¹ãƒ‘ãƒ¼ã‚¹ç‡: 50.2%

# æ¨è«–é€Ÿåº¦ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹ã‚«ãƒ¼ãƒãƒ«ä½¿ç”¨æ™‚ï¼‰
# æ³¨: PyTorch ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚¹ãƒ‘ãƒ¼ã‚¹æœ€é©åŒ–ã—ãªã„ãŸã‚ã€
#     å°‚ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆNM-sparsityç­‰ï¼‰ãŒå¿…è¦
```

### æ€§èƒ½æ¯”è¼ƒï¼ˆã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ï¼‰

**ResNet-18 æ¨è«–**:

|| ãƒ‡ãƒã‚¤ã‚¹ | FP32ï¼ˆmsï¼‰ | INT8ï¼ˆmsï¼‰ | ãƒ¡ãƒ¢ãƒªï¼ˆMBï¼‰ |
||---------|-----------|-----------|------------|
|| Desktop RTX 3080 | 5.2 | - | 45 |
|| Jetson Xavier NX | 23.4 | 12.1 | 45 |
|| Jetson Nano | 156.3 | 78.2 | 45 |
|| Raspberry Pi 4ï¼ˆCPUï¼‰ | 892.1 | - | 45 |
|| Coral Edge TPUï¼ˆINT8ï¼‰ | - | 8.7 | 11 |

---

## å‚è€ƒæ–‡çŒ®

[^1] Knuth, D. E. (1974). "Structured Programming with go to Statements." ACM Computing Surveys.

[^2] NVIDIA Nsight Systems Documentation: https://docs.nvidia.com/nsight-systems/

[^3] Huang, Y., et al. (2019). "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism." NeurIPS.

[^4] Chen, T., et al. (2018). "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning." OSDI.

[^5] ONNX Runtime: https://onnxruntime.ai/

[^6] NVIDIA Triton Inference Server: https://github.com/triton-inference-server/server

[^7] NVIDIA TensorRT Documentation: https://developer.nvidia.com/tensorrt

[^8] WebGPU Specification: https://www.w3.org/TR/webgpu/

[^9] Jacob, B., et al. (2018). "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference." CVPR.

[^10] Han, S., et al. (2015). "Learning both Weights and Connections for Efficient Neural Networks." NeurIPS.
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬19ç« : CNNãƒ»RNNãƒ»Transformerã‚’å®Ÿè£…ã™ã‚‹](06-19-CNNãƒ»RNNãƒ»Transformerã‚’å®Ÿè£…ã™ã‚‹.md) | [â¡ï¸ ç¬¬21ç« : å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ](06-21-å®Ÿè·µãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ.md)