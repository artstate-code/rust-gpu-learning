# 第 17 章　エンドツーエンド最適化

この章では、機械学習モデルを本番環境に展開するための総合的な最適化手法を学びます。プロファイリングによるボトルネック特定から、カーネル最適化、グラフ最適化、そして様々なプラットフォームへのデプロイメントまでをカバーします。

**目的**: 実用的なMLシステムの性能を極限まで引き出し、様々な環境で効率的に動作させる技術を習得します。

## 17.1 プロファイル解析とボトルネック検出

最適化の第一歩は、**正確な計測**です。どこが遅いのかを知らずに最適化することはできません [^1]。

[^1]: "Premature optimization is the root of all evil" - Donald Knuth

### GPU プロファイリングツール

|| ツール | 対象 | 特徴 | 用途 |
||--------|------|------|------|
|| **NVIDIA Nsight Systems** | システム全体 | タイムライン表示 | 全体像把握 |
|| **NVIDIA Nsight Compute** | カーネル詳細 | 命令レベル解析 | カーネル最適化 |
|| **nvprof** | GPU性能 | コマンドライン | CI/CD統合 |
|| **PyTorch Profiler** | PyTorch | 自動計装 | ML特化 |
|| **Rust perf** | CPU | サンプリング | CPU側ボトルネック |
|| **flamegraph** | CPU | 可視化 | ホットスポット特定 |

### Nsight Systems によるプロファイリング

**Nsight Systems** [^2] は、システム全体の動作をタイムラインで可視化します。

[^2]: NVIDIA Nsight Systems: https://developer.nvidia.com/nsight-systems

**使用手順**:

```bash
# プロファイル取得（Python）
nsys profile -o resnet50_profile python train.py

# プロファイル取得（Rust）
nsys profile -o rust_ml_profile ./target/release/ml_app

# 結果表示（GUI）
nsys-ui resnet50_profile.nsys-rep
```

**分析項目**:

1. **GPU使用率**: アイドル時間が長い → バッチサイズ増加
2. **データ転送**: H2D/D2Hが頻繁 → データのGPU常駐化
3. **カーネル起動**: 小さいカーネルが多数 → カーネル融合
4. **CPU待機**: GPU同期待ち → 非同期実行

### Python（PyTorch）でのプロファイリング

```python
import torch
import torch.profiler as profiler

model = ResNet50().cuda()
inputs = torch.randn(32, 3, 224, 224).cuda()

with profiler.profile(
    activities=[
        profiler.ProfilerActivity.CPU,
        profiler.ProfilerActivity.CUDA,
    ],
    record_shapes=True,
    profile_memory=True,
    with_stack=True
) as prof:
    with profiler.record_function("model_forward"):
        outputs = model(inputs)

# 結果表示
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))

# 出力例:
# -----------------------------------  ------------  ------------  ------------
# Name                                 Self CPU %   Self CUDA %  CUDA time avg
# -----------------------------------  ------------  ------------  ------------
# model_forward                        1.23%        100.00%      45.23ms
# aten::conv2d                         0.45%        67.82%       2.34ms
# aten::batch_norm                     0.12%        8.45%        0.45ms
# aten::relu                           0.08%        3.21%        0.12ms
# cudaMemcpyAsync                      0.34%        2.34%        0.98ms
# -----------------------------------  ------------  ------------  ------------

# TensorBoard エクスポート
prof.export_chrome_trace("trace.json")
# chrome://tracing で開く
```

### Rust でのプロファイリング

#### 1. cargo-flamegraph（CPU）

```bash
# インストール
cargo install flamegraph

# プロファイル取得
cargo flamegraph --bin ml_app

# flamegraph.svg が生成される
```

**flamegraph の読み方**:

- 横軸: CPU時間の割合
- 縦軸: 呼び出しスタック
- 色: ランダム（意味なし）
- 幅が広い = ホットスポット

#### 2. perf（Linux）

```bash
# プロファイル取得
perf record --call-graph dwarf ./target/release/ml_app

# 結果表示
perf report

# 出力例:
# Overhead  Command  Shared Object      Symbol
# 45.23%    ml_app   libblas.so         sgemm_
# 12.34%    ml_app   ml_app             conv2d_forward
# 8.76%     ml_app   libcudart.so       cudaLaunchKernel
```

#### 3. tch-rs での GPU プロファイリング

```rust
use tch::{nn, Tensor, Device};

fn profile_model() {
    let device = Device::Cuda(0);
    
    // CUDAイベントでタイミング計測
    let start = tch::cuda::Event::new();
    let end = tch::cuda::Event::new();
    
    let model = create_resnet50();
    let input = Tensor::randn(&[32, 3, 224, 224], (tch::Kind::Float, device));
    
    // ウォームアップ
    for _ in 0..10 {
        let _ = model.forward(&input);
    }
    
    // 計測
    start.record();
    for _ in 0..100 {
        let _ = model.forward(&input);
    }
    end.record();
    end.synchronize();
    
    let elapsed = start.elapsed_time(&end).unwrap();
    println!("平均推論時間: {:.3} ms", elapsed / 100.0);
    println!("スループット: {:.1} samples/sec", 32.0 * 100.0 / (elapsed / 1000.0));
}
```

### ボトルネック分類と対策

**1. メモリバンド幅律速**（Memory Bound）

**症状**: GPU使用率が低い、メモリアクセスが多い

**対策**:

- メモリ合体（Coalescing）の改善
- 共有メモリ（Shared Memory）の活用
- カーネル融合

**2. 計算律速**（Compute Bound）

**症状**: GPU使用率が高い、FLOPS が理論値に近い

**対策**:

- より高速なアルゴリズム（Winograd、FFT）
- Tensor Cores の活用（FP16/BF16）
- より高性能なGPUへの移行

**3. 転送律速**（Transfer Bound）

**症状**: H2D/D2H転送が頻繁

**対策**:

- データのGPU常駐化
- ピン留めメモリ
- 非同期転送

**4. 同期律速**（Synchronization Bound）

**症状**: CPU待機時間が長い

**対策**:

- ストリームの活用
- カーネルの非同期実行
- CPU/GPU重複実行

### 実測例（ResNet-50推論）

**プロファイル結果**:

|| 処理 | 時間（ms） | 割合 | ボトルネック |
||------|-----------|------|------------|
|| Conv2D | 28.5 | 63% | 計算 |
|| BatchNorm | 6.2 | 14% | メモリ |
|| ReLU | 2.1 | 5% | メモリ |
|| データ転送 | 4.8 | 11% | 転送 |
|| その他 | 3.4 | 7% | - |
|| **合計** | **45.0** | **100%** | - |

**最適化前後の比較**:

|| 最適化 | 効果（ms） | 手法 |
||--------|-----------|------|
|| 1. Tensor Cores使用（FP16） | 28.5 → 14.2 | cuDNN自動 |
|| 2. BatchNorm + ReLU融合 | 8.3 → 4.1 | カスタムカーネル |
|| 3. ピン留めメモリ | 4.8 → 1.6 | PyTorch設定 |
|| **合計** | 45.0 → 21.9 | **2.05x高速化** |

## 17.2 カーネル最適化・パイプライン分割

### カーネル最適化の原則

**最適化の優先順位**（Amdahlの法則に基づく）:

1. **最もホットな** カーネルから最適化
2. **並列度** を最大化
3. **メモリアクセス** を最小化
4. **レジスタ使用量** を最適化

### Nsight Compute によるカーネル解析

```bash
# 特定カーネルの詳細プロファイル
ncu --target-processes all --set full ./ml_app

# 主要な指標:
# - SM Efficiency: ストリーミングマルチプロセッサの効率
# - Occupancy: 占有率（アクティブワープ数 / 最大ワープ数）
# - Memory Throughput: メモリスループット
# - Compute Throughput: 計算スループット
```

**出力例**:

```
Kernel: matrix_multiply_kernel
Duration: 2.45 ms
Grid Size: (256, 256, 1)
Block Size: (16, 16, 1)
Registers Per Thread: 32
Shared Memory Per Block: 8192 bytes
Occupancy: 50.0% (理論値: 100%)
Memory Throughput: 45% (低い！)
Compute Throughput: 78%

ボトルネック: メモリバンド幅
推奨: 
  - タイル化を使った共有メモリ活用
  - メモリアクセスパターンの改善
```

### 行列積の段階的最適化

#### バージョン1: 素朴な実装

```cuda
__global__ void matmul_naive(float* C, const float* A, const float* B, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < N && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < N; k++) {
            sum += A[row * N + k] * B[k * N + col];  // グローバルメモリから毎回読み込み
        }
        C[row * N + col] = sum;
    }
}

// 性能: 50 GFLOPS（RTX 4090の理論値 82,580 GFLOPSの0.06%）
```

#### バージョン2: 共有メモリ使用

```cuda
#define TILE_SIZE 16

__global__ void matmul_shared(float* C, const float* A, const float* B, int N) {
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];
    
    int row = blockIdx.y * TILE_SIZE + threadIdx.y;
    int col = blockIdx.x * TILE_SIZE + threadIdx.x;
    
    float sum = 0.0f;
    
    for (int t = 0; t < N / TILE_SIZE; t++) {
        // グローバルメモリから共有メモリへ（合体アクセス）
        As[threadIdx.y][threadIdx.x] = A[row * N + t * TILE_SIZE + threadIdx.x];
        Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];
        
        __syncthreads();  // 全スレッド同期
        
        // 共有メモリから計算（高速）
        for (int k = 0; k < TILE_SIZE; k++) {
            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];
        }
        
        __syncthreads();
    }
    
    if (row < N && col < N) {
        C[row * N + col] = sum;
    }
}

// 性能: 2,500 GFLOPS（50倍高速化）
```

#### バージョン3: Tensor Cores 使用（FP16）

```cuda
#include <mma.h>
using namespace nvcuda::wmma;

__global__ void matmul_tensor_core(
    half* C, const half* A, const half* B, int M, int N, int K
) {
    // Tensor Core用の16x16x16行列フラグメント
    fragment<matrix_a, 16, 16, 16, half, row_major> a_frag;
    fragment<matrix_b, 16, 16, 16, half, col_major> b_frag;
    fragment<accumulator, 16, 16, 16, half> c_frag;
    
    fill_fragment(c_frag, 0.0h);
    
    int warp_row = (blockIdx.y * blockDim.y + threadIdx.y) / 32;
    int warp_col = (blockIdx.x * blockDim.x + threadIdx.x) / 32;
    
    for (int i = 0; i < K; i += 16) {
        int a_row = warp_row * 16;
        int a_col = i;
        int b_row = i;
        int b_col = warp_col * 16;
        
        // Tensor Coreで計算（自動最適化）
        load_matrix_sync(a_frag, A + a_row * K + a_col, K);
        load_matrix_sync(b_frag, B + b_row * N + b_col, N);
        mma_sync(c_frag, a_frag, b_frag, c_frag);
    }
    
    // 結果を書き戻し
    store_matrix_sync(C + warp_row * 16 * N + warp_col * 16, c_frag, N, mem_row_major);
}

// 性能: 35,000 GFLOPS（700倍高速化、理論値の42%）
```

**性能比較**:

|| 実装 | GFLOPS | 相対速度 | 理論効率 |
||------|--------|---------|---------|
|| 素朴 | 50 | 1x | 0.06% |
|| 共有メモリ | 2,500 | 50x | 3% |
|| Tensor Core（FP16） | 35,000 | 700x | 42% |
|| cuBLAS（最適化済み） | 70,000 | 1400x | 85% |

### Rust からのカスタムカーネル呼び出し

```rust
use cudarc::driver::*;

const PTX_SRC: &str = r#"
.version 7.0
.target sm_80
.address_size 64

.visible .entry matmul_shared(
    .param .u64 C,
    .param .u64 A,
    .param .u64 B,
    .param .u32 N
) {
    // ... PTXコード ...
}
"#;

fn matmul_gpu(
    a: &[f32],
    b: &[f32],
    n: usize,
) -> Result<Vec<f32>, CudaError> {
    let device = CudaDevice::new(0)?;
    
    // カーネルロード
    let ptx = CudaModule::from_ptx(PTX_SRC, "matmul", &[])?;
    let kernel = device.get_func(&ptx, "matmul_shared")?;
    
    // GPU メモリ確保・転送
    let d_a = device.htod_copy(a)?;
    let d_b = device.htod_copy(b)?;
    let mut d_c = device.alloc_zeros::<f32>(n * n)?;
    
    // カーネル起動
    let block_size = 16;
    let grid_size = (n + block_size - 1) / block_size;
    let cfg = LaunchConfig {
        grid_dim: (grid_size as u32, grid_size as u32, 1),
        block_dim: (block_size as u32, block_size as u32, 1),
        shared_mem_bytes: 0,
    };
    
    unsafe {
        kernel.launch(cfg, (&d_c, &d_a, &d_b, n as u32))?;
    }
    
    // 結果取得
    device.dtoh_sync_copy(&d_c)
}
```

### パイプライン分割

**パイプライン並列化**（Pipeline Parallelism）は、モデルを複数のステージに分割し、異なるGPUで並列実行します [^3]。

[^3]: Huang, Y., et al. (2019). "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism." NeurIPS.

**GPipeアルゴリズム**:

```python
import torch
import torch.nn as nn

class PipelineModel(nn.Module):
    def __init__(self, num_layers, hidden_size, num_partitions):
        super().__init__()
        self.num_partitions = num_partitions
        layers_per_partition = num_layers // num_partitions
        
        # モデルを分割
        self.partitions = nn.ModuleList()
        for i in range(num_partitions):
            partition = nn.Sequential(*[
                TransformerLayer(hidden_size)
                for _ in range(layers_per_partition)
            ]).cuda(i)  # GPU i に配置
            self.partitions.append(partition)
    
    def forward(self, x, micro_batch_size=32):
        batch_size = x.size(0)
        num_micro_batches = batch_size // micro_batch_size
        
        # マイクロバッチに分割
        micro_batches = x.chunk(num_micro_batches)
        outputs = []
        
        # パイプライン実行
        for micro_batch in micro_batches:
            h = micro_batch
            for i, partition in enumerate(self.partitions):
                h = h.cuda(i)  # GPU i に転送
                h = partition(h)
            outputs.append(h.cpu())
        
        return torch.cat(outputs, dim=0)

# 使用例
model = PipelineModel(num_layers=24, hidden_size=1024, num_partitions=4)
x = torch.randn(128, 512, 1024)  # バッチサイズ128
output = model(x, micro_batch_size=32)
```

**バブルタイム**（Bubble Time）:

パイプラインの最初と最後でGPUがアイドル状態になる時間。

\[
\text{Bubble Ratio} = \frac{N*{partitions} - 1}{N*{micro*batches}}
\]

**最適化**:

- マイクロバッチ数を増やす（ただしメモリ使用量とトレードオフ）
- 1F1B（One-Forward-One-Backward）スケジューリング

## 17.3 演算子融合とグラフ最適化

**演算子融合**（Operator Fusion）は、複数の演算を1つのカーネルにまとめてメモリアクセスを削減する技術です [^4]。

[^4]: Chen, T., et al. (2018). "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning." OSDI.

### 融合パターン

#### 1. 要素ごと演算の融合（Elementwise Fusion）

**融合前**:

```python
# 3回のメモリアクセス（各演算で読み書き）
y = x + bias    # カーネル1
z = relu(y)     # カーネル2
out = z * scale # カーネル3
```

**融合後**:

```python
# 1回のメモリアクセス
out = relu(x + bias) * scale  # 1つのカーネル
```

**Rust（cudarc）での実装**:

```cuda
__global__ void fused_add_relu_scale(
    float* out,
    const float* x,
    const float* bias,
    float scale,
    int n
) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        float val = x[i] + bias[i];      // 加算
        val = val > 0.0f ? val : 0.0f;   // ReLU
        out[i] = val * scale;            // スケール
    }
}
```

**性能向上**: 3-4倍（メモリバンド幅律速の場合）

#### 2. Batch Normalization + ReLU 融合

```cuda
__global__ void fused_batchnorm_relu(
    float* out,
    const float* x,
    const float* mean,
    const float* var,
    const float* gamma,
    const float* beta,
    float eps,
    int n
) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        // Batch Normalization
        float x_norm = (x[i] - mean[i]) / sqrtf(var[i] + eps);
        float x_scaled = gamma[i] * x_norm + beta[i];
        
        // ReLU
        out[i] = x_scaled > 0.0f ? x_scaled : 0.0f;
    }
}
```

#### 3. Convolution + Batch Norm + ReLU 融合

**cuDNN での自動融合**:

```python
import torch
import torch.nn as nn

# PyTorch 2.0+ では自動融合される
model = nn.Sequential(
    nn.Conv2d(3, 64, 3, padding=1),
    nn.BatchNorm2d(64),
    nn.ReLU(inplace=True)
).cuda()

# torch.compile で融合が有効化
model = torch.compile(model)

x = torch.randn(32, 3, 224, 224).cuda()
y = model(x)  # 内部で融合カーネルが使われる
```

### グラフ最適化

**計算グラフ**（Computational Graph）の最適化により、実行効率を向上させます。

#### 最適化パターン

**1. 定数畳み込み**（Constant Folding）

```python
# 最適化前
y = x * 2 * 3  # 2回の乗算

# 最適化後
y = x * 6      # 1回の乗算（2*3=6は事前計算）
```

**2. 共通部分式の除去**（Common Subexpression Elimination）

```python
# 最適化前
a = x * y + z
b = x * y - z  # x*yが重複

# 最適化後
tmp = x * y
a = tmp + z
b = tmp - z
```

**3. デッドコード除去**（Dead Code Elimination）

```python
# 最適化前
y = x * 2
z = x * 3  # 使われない
return y

# 最適化後
y = x * 2
return y  # zは削除
```

### ONNX によるグラフ最適化

**ONNX Runtime** [^5] は、自動的にグラフ最適化を行います。

[^5]: ONNX Runtime: https://onnxruntime.ai/

**Python でのモデルエクスポートと最適化**:

```python
import torch
import onnx
from onnxruntime.transformers import optimizer

# PyTorch モデル
model = MyModel().eval()
dummy_input = torch.randn(1, 3, 224, 224)

# ONNX エクスポート
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    input_names=["input"],
    output_names=["output"],
    dynamic_axes={
        "input": {0: "batch_size"},
        "output": {0: "batch_size"}
    }
)

# ONNX グラフ最適化
from onnxruntime.transformers.onnx_model import OnnxModel
onnx_model = OnnxModel(onnx.load("model.onnx"))

# 最適化を適用
onnx_model.fuse_reshape()
onnx_model.fuse_shape()
onnx_model.remove_unused_constant()

onnx_model.save_model_to_file("model_optimized.onnx")
```

**最適化効果**:

|| 最適化 | 効果 |
||--------|------|
|| ノード数削減 | 234 → 167 (29%削減) |
|| 推論時間 | 45.2ms → 32.1ms (29%高速化) |
|| メモリ使用量 | 1.2GB → 0.9GB (25%削減) |

### Rust での ONNX 推論

```rust
use tract_onnx::prelude::*;

fn run_onnx_inference() -> TractResult<()> {
    // ONNXモデルロード
    let model = tract_onnx::onnx()
        .model_for_path("model_optimized.onnx")?
        .with_input_fact(0, InferenceFact::dt_shape(f32::datum_type(), tvec!(1, 3, 224, 224)))?
        .into_optimized()?  // グラフ最適化
        .into_runnable()?;
    
    // 入力データ準備
    let input = Tensor::from_shape(
        &[1, 3, 224, 224],
        &vec![0.0f32; 3 * 224 * 224]
    )?;
    
    // 推論実行
    let result = model.run(tvec!(input.into()))?;
    
    // 結果取得
    let output: ArrayD<f32> = result[0].to_array_view::<f32>()?.to_owned();
    
    println!("出力形状: {:?}", output.shape());
    
    Ok(())
}
```

## 17.4 モデルデプロイ・推論サーバ構築

本番環境での機械学習モデルのデプロイには、**高スループット**と**低レイテンシ**が求められます。

### 推論サーバのアーキテクチャ

```
┌──────────────┐
│  ロードバランサ │
└──────┬───────┘
       │
   ┌───┴───┬───────┬───────┐
   │       │       │       │
┌──▼──┐ ┌──▼──┐ ┌──▼──┐ ┌──▼──┐
│ API  │ │ API  │ │ API  │ │ API  │
│Server│ │Server│ │Server│ │Server│
└──┬───┘ └──┬───┘ └──┬───┘ └──┬───┘
   │       │       │       │
┌──▼───────▼───────▼───────▼───┐
│    Model Inference Engine     │
│  (バッチング・GPU管理)        │
└──────────┬────────────────────┘
           │
      ┌────▼────┐
      │  GPU 0  │
      │  GPU 1  │
      │  GPU 2  │
      │  GPU 3  │
      └─────────┘
```

### Rust（Actix-Web）での推論サーバ

```rust
use actix_web::{web, App, HttpServer, HttpResponse};
use serde::{Deserialize, Serialize};
use tch::{nn, Tensor, Device, CModule};
use std::sync::{Arc, Mutex};

#[derive(Deserialize)]
struct InferenceRequest {
    image: Vec<Vec<Vec<f32>>>,  // [C, H, W]
}

#[derive(Serialize)]
struct InferenceResponse {
    predictions: Vec<f32>,
    latency_ms: f64,
}

struct AppState {
    model: Arc<Mutex<CModule>>,
    device: Device,
}

async fn predict(
    data: web::Json<InferenceRequest>,
    state: web::Data<AppState>,
) -> HttpResponse {
    let start = std::time::Instant::now();
    
    // 入力データをテンソルに変換
    let image = &data.image;
    let tensor = Tensor::of_slice3(image)
        .unsqueeze(0)  // バッチ次元追加
        .to_device(state.device);
    
    // 推論実行
    let model = state.model.lock().unwrap();
    let output = model.forward_ts(&[tensor]).unwrap();
    
    // 結果をベクトルに変換
    let predictions: Vec<f32> = output.try_into().unwrap();
    
    let latency = start.elapsed().as_secs_f64() * 1000.0;
    
    HttpResponse::Ok().json(InferenceResponse {
        predictions,
        latency_ms: latency,
    })
}

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    // モデルロード
    let device = Device::Cuda(0);
    let model = tch::CModule::load("model_traced.pt").unwrap();
    
    let state = web::Data::new(AppState {
        model: Arc::new(Mutex::new(model)),
        device,
    });
    
    // HTTPサーバ起動
    HttpServer::new(move || {
        App::new()
            .app_data(state.clone())
            .route("/predict", web::post().to(predict))
    })
    .bind("0.0.0.0:8080")?
    .run()
    .await
}
```

### 動的バッチング

**動的バッチング**（Dynamic Batching）は、複数のリクエストをまとめて推論することで、GPUの利用効率を高めます [^6]。

[^6]: NVIDIA Triton Inference Server: https://github.com/triton-inference-server/server

```rust
use tokio::sync::mpsc;
use tokio::time::{sleep, Duration};

struct BatchingEngine {
    batch_size: usize,
    timeout_ms: u64,
    model: CModule,
}

impl BatchingEngine {
    async fn run(&self, rx: mpsc::Receiver<(Tensor, oneshot::Sender<Tensor>)>) {
        let mut pending_requests = Vec::new();
        let mut deadline = Instant::now() + Duration::from_millis(self.timeout_ms);
        
        loop {
            select! {
                Some((input, response_tx)) = rx.recv() => {
                    pending_requests.push((input, response_tx));
                    
                    // バッチサイズに達したら推論実行
                    if pending_requests.len() >= self.batch_size {
                        self.process_batch(&mut pending_requests).await;
                        deadline = Instant::now() + Duration::from_millis(self.timeout_ms);
                    }
                }
                _ = sleep_until(deadline) => {
                    // タイムアウト: 溜まっているリクエストを処理
                    if !pending_requests.is_empty() {
                        self.process_batch(&mut pending_requests).await;
                    }
                    deadline = Instant::now() + Duration::from_millis(self.timeout_ms);
                }
            }
        }
    }
    
    async fn process_batch(&self, requests: &mut Vec<(Tensor, oneshot::Sender<Tensor>)>) {
        // 入力をバッチ化
        let inputs: Vec<Tensor> = requests.iter().map(|(t, _)| t.shallow_clone()).collect();
        let batch = Tensor::cat(&inputs, 0);
        
        // バッチ推論
        let outputs = self.model.forward_ts(&[batch]).unwrap();
        
        // 結果を分割して各リクエストに返す
        for (i, (_, response_tx)) in requests.drain(..).enumerate() {
            let output = outputs.get(i as i64);
            let _ = response_tx.send(output);
        }
    }
}
```

**動的バッチングの効果**:

|| 設定 | レイテンシ（ms） | スループット（req/s） |
||------|----------------|-------------------|
|| バッチなし | 15 | 67 |
|| バッチサイズ4 | 25 | 160 |
|| バッチサイズ8 | 35 | 228 |
|| バッチサイズ16 | 50 | 320 |

### TensorRT による最適化

**NVIDIA TensorRT** [^7] は、推論専用の高度に最適化されたエンジンです。

[^7]: NVIDIA TensorRT: https://developer.nvidia.com/tensorrt

**最適化手法**:

1. **層融合**: Conv+BN+ReLU を1つのカーネルに
2. **精度最適化**: FP16/INT8 量子化
3. **カーネル選択**: ハードウェアに最適なカーネル
4. **動的テンソルメモリ**: メモリ使用量最小化

**Python での使用**:

```python
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

# TensorRT エンジン構築
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

def build_engine(onnx_path, engine_path):
    builder = trt.Builder(TRT_LOGGER)
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    parser = trt.OnnxParser(network, TRT_LOGGER)
    
    # ONNX モデル読み込み
    with open(onnx_path, 'rb') as model:
        parser.parse(model.read())
    
    # ビルド設定
    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30  # 1GB
    config.set_flag(trt.BuilderFlag.FP16)  # FP16 有効化
    
    # エンジン構築
    engine = builder.build_engine(network, config)
    
    # 保存
    with open(engine_path, 'wb') as f:
        f.write(engine.serialize())
    
    return engine

# 推論実行
def infer(engine, input_data):
    context = engine.create_execution_context()
    
    # GPU メモリ確保
    d_input = cuda.mem_alloc(input_data.nbytes)
    d_output = cuda.mem_alloc(output_size)
    
    # データ転送
    cuda.memcpy_htod(d_input, input_data)
    
    # 推論
    context.execute_v2([int(d_input), int(d_output)])
    
    # 結果取得
    cuda.memcpy_dtoh(output_data, d_output)
    
    return output_data
```

**性能向上**:

|| 実装 | レイテンシ（ms） | スループット（img/s） |
||------|----------------|-------------------|
|| PyTorch（FP32） | 15.2 | 67 |
|| PyTorch（FP16） | 8.1 | 125 |
|| TensorRT（FP16） | 3.2 | 315 |
|| TensorRT（INT8） | 1.8 | 560 |

## 17.5 WebGPU 経由でのブラウザ推論

**WebGPU** [^8] は、ブラウザからGPUを直接制御できる新しいAPIです。

[^8] WebGPU Specification: https://www.w3.org/TR/webgpu/

### WebGPU vs WebGL

|| 項目 | WebGL | WebGPU |
||------|-------|--------|
|| API設計 | グラフィックス専用 | 汎用計算対応 |
|| シェーダ言語 | GLSL | WGSL |
|| 計算シェーダ | 限定的 | 完全対応 |
|| メモリ管理 | 暗黙的 | 明示的 |
|| 性能 | 中 | 高 |

### Rust（wgpu）での実装

```rust
use wgpu::util::DeviceExt;

async fn matrix_multiply_webgpu(
    a: &[f32],
    b: &[f32],
    n: usize,
) -> Vec<f32> {
    // WebGPU インスタンス作成
    let instance = wgpu::Instance::new(wgpu::Backends::all());
    let adapter = instance
        .request_adapter(&wgpu::RequestAdapterOptions::default())
        .await
        .unwrap();
    
    let (device, queue) = adapter
        .request_device(&wgpu::DeviceDescriptor::default(), None)
        .await
        .unwrap();
    
    // シェーダコード（WGSL）
    let shader = device.create_shader_module(&wgpu::ShaderModuleDescriptor {
        label: None,
        source: wgpu::ShaderSource::Wgsl(r#"
            @group(0) @binding(0) var<storage, read> a: array<f32>;
            @group(0) @binding(1) var<storage, read> b: array<f32>;
            @group(0) @binding(2) var<storage, read_write> result: array<f32>;
            @group(0) @binding(3) var<uniform> n: u32;
            
            @compute @workgroup_size(16, 16)
            fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
                let row = global_id.x;
                let col = global_id.y;
                
                if (row >= n || col >= n) {
                    return;
                }
                
                var sum = 0.0;
                for (var k = 0u; k < n; k = k + 1u) {
                    sum += a[row * n + k] * b[k * n + col];
                }
                
                result[row * n + col] = sum;
            }
        "#.into()),
    });
    
    // バッファ作成
    let buffer_a = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
        label: None,
        contents: bytemuck::cast_slice(a),
        usage: wgpu::BufferUsages::STORAGE,
    });
    
    let buffer_b = device.create_buffer_init(&wgpu::util::BufferInitDescriptor {
        label: None,
        contents: bytemuck::cast_slice(b),
        usage: wgpu::BufferUsages::STORAGE,
    });
    
    let buffer_result = device.create_buffer(&wgpu::BufferDescriptor {
        label: None,
        size: (n * n * std::mem::size_of::<f32>()) as u64,
        usage: wgpu::BufferUsages::STORAGE | wgpu::BufferUsages::COPY_SRC,
        mapped_at_creation: false,
    });
    
    // コンピュートパイプライン作成
    let compute_pipeline = device.create_compute_pipeline(&wgpu::ComputePipelineDescriptor {
        label: None,
        layout: None,
        module: &shader,
        entry_point: "main",
    });
    
    // コマンド実行
    let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor::default());
    {
        let mut compute_pass = encoder.begin_compute_pass(&wgpu::ComputePassDescriptor::default());
        compute_pass.set_pipeline(&compute_pipeline);
        compute_pass.set_bind_group(0, &bind_group, &[]);
        compute_pass.dispatch(
            (n as u32 + 15) / 16,
            (n as u32 + 15) / 16,
            1
        );
    }
    
    queue.submit(Some(encoder.finish()));
    
    // 結果取得
    let buffer_slice = buffer_result.slice(..);
    let (tx, rx) = futures::channel::oneshot::channel();
    buffer_slice.map_async(wgpu::MapMode::Read, move |result| {
        tx.send(result).unwrap();
    });
    device.poll(wgpu::Maintain::Wait);
    rx.await.unwrap().unwrap();
    
    let data = buffer_slice.get_mapped_range();
    let result: Vec<f32> = bytemuck::cast_slice(&data).to_vec();
    
    result
}
```

### WebAssembly + WebGPU

**Rust → WASM → ブラウザ** のパイプライン:

```bash
# WASMターゲット追加
rustup target add wasm32-unknown-unknown

# wasm-pack インストール
cargo install wasm-pack

# WASMビルド
wasm-pack build --target web
```

**Rust側（lib.rs）**:

```rust
use wasm_bindgen::prelude::*;

#[wasm_bindgen]
pub async fn run_inference(image_data: &[u8]) -> Vec<f32> {
    // WebGPU で推論
    let result = webgpu_inference(image_data).await;
    result
}
```

**JavaScript側**:

```javascript
import init, { run_inference } from './pkg/ml_wasm.js';

async function main() {
    await init();
    
    const imageData = new Uint8Array(/* 画像データ */);
    const predictions = await run_inference(imageData);
    
    console.log('予測結果:', predictions);
}
```

**ベンチマーク**（ResNet-18推論）:

|| 環境 | 推論時間（ms） | 相対速度 |
||------|--------------|---------|
|| Native CPU | 150 | 1x |
|| WASM（CPU） | 380 | 0.39x |
|| WebGL | 45 | 3.3x |
|| WebGPU | 28 | 5.4x |
|| Native CUDA | 5 | 30x |

## 17.6 組み込み・エッジデバイスへの展開

**エッジデバイス**（Edge Device）での機械学習推論は、**省電力**と**低レイテンシ**が重要です。

### ターゲットデバイス

|| デバイス | GPU/NPU | メモリ | 用途 |
||---------|---------|--------|------|
|| Raspberry Pi 4 | なし | 4-8GB | プロトタイプ |
|| NVIDIA Jetson Nano | Maxwell GPU | 4GB | 低コストエッジAI |
|| NVIDIA Jetson Xavier NX | Volta GPU | 8GB | 高性能エッジAI |
|| Coral Dev Board | Edge TPU | 1GB | Google生態系 |
|| Apple Neural Engine | ANE | - | iOS/macOS |

### 量子化（Quantization）

**INT8量子化**により、モデルサイズと計算量を削減 [^9]。

[^9]: Jacob, B., et al. (2018). "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference." CVPR.

**量子化の数式**:

\[
x*{int8} = \text{round}\left(\frac{x*{fp32} - z}{s}\right)
\]

ここで、\(s\) はスケール、\(z\) はゼロポイント。

**Python（PyTorch）での量子化**:

```python
import torch
import torch.quantization

model = ResNet50().eval()

# 量子化設定
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')

# 量子化準備
model_prepared = torch.quantization.prepare(model)

# キャリブレーション（代表的なデータで統計収集）
for data in calibration_data:
    model_prepared(data)

# 量子化実行
model_quantized = torch.quantization.convert(model_prepared)

# モデルサイズ比較
print(f"FP32: {get_model_size(model):.2f} MB")
print(f"INT8: {get_model_size(model_quantized):.2f} MB")
# 出力:
# FP32: 97.8 MB
# INT8: 24.4 MB (4倍削減)

# 推論速度比較
import time
x = torch.randn(1, 3, 224, 224)

start = time.time()
for _ in range(100):
    _ = model(x)
time_fp32 = time.time() - start

start = time.time()
for _ in range(100):
    _ = model_quantized(x)
time_int8 = time.time() - start

print(f"FP32: {time_fp32*10:.2f} ms/inference")
print(f"INT8: {time_int8*10:.2f} ms/inference")
print(f"高速化率: {time_fp32/time_int8:.2f}x")
# 出力:
# FP32: 45.2 ms/inference
# INT8: 18.3 ms/inference
# 高速化率: 2.47x
```

### NVIDIA Jetson での展開

**Jetson Nano でのセットアップ**:

```bash
# JetPack SDK インストール（Ubuntu 18.04ベース）
sudo apt update
sudo apt install nvidia-jetpack

# Rust インストール
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# クロスコンパイル設定
rustup target add aarch64-unknown-linux-gnu
```

**Cargo.toml**:

```toml
[dependencies]
tch = "0.13"  # LibTorch バインディング

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
```

**推論コード**:

```rust
use tch::{nn, Tensor, Device, CModule};

fn main() {
    // Jetson の GPU 使用
    let device = Device::Cuda(0);
    
    // TorchScript モデルロード
    let model = CModule::load("model_jetson.pt").unwrap();
    
    // カメラから画像取得（省略）
    let image = capture_image();
    let tensor = preprocess(image).to_device(device);
    
    // 推論
    let output = model.forward_ts(&[tensor]).unwrap();
    let predictions = postprocess(output);
    
    println!("予測: {:?}", predictions);
}
```

### プルーニング（Pruning）

**プルーニング**は、重要でない重みを削除してモデルを軽量化します [^10]。

[^10]: Han, S., et al. (2015). "Learning both Weights and Connections for Efficient Neural Networks." NeurIPS.

**マグニチュードプルーニング**:

```python
import torch
import torch.nn.utils.prune as prune

model = ResNet50()

# 畳み込み層の50%をプルーニング
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Conv2d):
        prune.l1_unstructured(module, name='weight', amount=0.5)

# プルーニングを恒久化
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Conv2d):
        prune.remove(module, 'weight')

# スパース性確認
def count_zero_weights(model):
    total = 0
    zero = 0
    for param in model.parameters():
        total += param.numel()
        zero += (param == 0).sum().item()
    return zero / total * 100

print(f"スパース率: {count_zero_weights(model):.1f}%")
# 出力: スパース率: 50.2%

# 推論速度（スパースカーネル使用時）
# 注: PyTorch はデフォルトでスパース最適化しないため、
#     専用ライブラリ（NM-sparsity等）が必要
```

### 性能比較（エッジデバイス）

**ResNet-18 推論**:

|| デバイス | FP32（ms） | INT8（ms） | メモリ（MB） |
||---------|-----------|-----------|------------|
|| Desktop RTX 3080 | 5.2 | - | 45 |
|| Jetson Xavier NX | 23.4 | 12.1 | 45 |
|| Jetson Nano | 156.3 | 78.2 | 45 |
|| Raspberry Pi 4（CPU） | 892.1 | - | 45 |
|| Coral Edge TPU（INT8） | - | 8.7 | 11 |

---

## 参考文献

[^1] Knuth, D. E. (1974). "Structured Programming with go to Statements." ACM Computing Surveys.

[^2] NVIDIA Nsight Systems Documentation: https://docs.nvidia.com/nsight-systems/

[^3] Huang, Y., et al. (2019). "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism." NeurIPS.

[^4] Chen, T., et al. (2018). "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning." OSDI.

[^5] ONNX Runtime: https://onnxruntime.ai/

[^6] NVIDIA Triton Inference Server: https://github.com/triton-inference-server/server

[^7] NVIDIA TensorRT Documentation: https://developer.nvidia.com/tensorrt

[^8] WebGPU Specification: https://www.w3.org/TR/webgpu/

[^9] Jacob, B., et al. (2018). "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference." CVPR.

[^10] Han, S., et al. (2015). "Learning both Weights and Connections for Efficient Neural Networks." NeurIPS.
