[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬11ç« ](04-11-æå¤±é–¢æ•°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®Ÿè£….md) | [â¡ï¸ ç¬¬13ç« ](04-13-å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã¨æœ€é©åŒ–æ‰‹æ³•.md)

---

# ç¬¬ 12 ç« ï¼šãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†

æœ¬ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®å­¦ç¿’ãƒ»æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ãŠã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã€ãƒãƒƒãƒåŒ–ã€GPU ã¸ã®è»¢é€ã‚’åŠ¹ç‡çš„ã«è¡Œã†æ–¹æ³•ã‚’ Rust ã§å®Ÿè£…ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯ã€GPU ã®è¨ˆç®—èƒ½åŠ›ã‚’ååˆ†ã«æ´»ç”¨ã§ããªã„ä¸»è¦ãªåŸå› ã®ä¸€ã¤ã§ã‚ã‚Šã€é©åˆ‡ãªä¸¦åˆ—åŒ–ã¨éåŒæœŸå‡¦ç†ãŒé‡è¦ã§ã™ã€‚

Pythonï¼ˆPyTorch/TensorFlowï¼‰ã¨ã®æ¯”è¼ƒã‚’é€šã˜ã¦ã€Rust ã§ã®é«˜åŠ¹ç‡ãªãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰æ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

---

## 12.1 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®è¨­è¨ˆ

### 12.1.1 Dataset ãƒˆãƒ¬ã‚¤ãƒˆã®å®šç¾©

Dataset ã¯ã€å€‹ã€…ã®ã‚µãƒ³ãƒ—ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚

**Pythonï¼ˆPyTorchï¼‰ã® Dataset**ï¼š

```python
import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# ä½¿ç”¨ä¾‹
data = np.random.randn(1000, 3, 32, 32).astype(np.float32)
labels = np.random.randint(0, 10, 1000)

dataset = CustomDataset(data, labels)
dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    pin_memory=True,
)

for batch_data, batch_labels in dataloader:
    # å­¦ç¿’å‡¦ç†
    pass
```

**Rust ã§ã®ãƒˆãƒ¬ã‚¤ãƒˆå®šç¾©**ï¼š

```rust
use ndarray::{ArrayD, Array4, Array1};
use std::error::Error;

/// Dataset ãƒˆãƒ¬ã‚¤ãƒˆ: å€‹ã€…ã®ã‚µãƒ³ãƒ—ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’æä¾›
pub trait Dataset {
    type Item;
    type Error: Error;

    /// ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’è¿”ã™
    fn len(&self) -> usize;

    /// ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç©ºã‹ã©ã†ã‹
    fn is_empty(&self) -> bool {
        self.len() == 0
    }

    /// ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—
    fn get(&self, index: usize) -> Result<Self::Item, Self::Error>;
}

/// ç”»åƒåˆ†é¡ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¾‹
pub struct ImageDataset {
    images: Vec<Array4<f32>>,  // (C, H, W)
    labels: Vec<usize>,
}

impl ImageDataset {
    pub fn new(images: Vec<Array4<f32>>, labels: Vec<usize>) -> Self {
        assert_eq!(images.len(), labels.len());
        Self { images, labels }
    }

    pub fn from_arrays(
        data: Array4<f32>,  // (N, C, H, W)
        labels: Array1<usize>,
    ) -> Self {
        let n = data.shape()[0];
        let mut images = Vec::with_capacity(n);
        
        for i in 0..n {
            let image = data.slice(s![i, .., .., ..]).to_owned();
            images.push(image);
        }
        
        let labels = labels.to_vec();
        Self { images, labels }
    }
}

#[derive(Debug)]
pub enum DatasetError {
    IndexOutOfBounds,
}

impl std::fmt::Display for DatasetError {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            DatasetError::IndexOutOfBounds => write!(f, "Index out of bounds"),
        }
    }
}

impl Error for DatasetError {}

impl Dataset for ImageDataset {
    type Item = (Array4<f32>, usize);
    type Error = DatasetError;

    fn len(&self) -> usize {
        self.images.len()
    }

    fn get(&self, index: usize) -> Result<Self::Item, Self::Error> {
        if index >= self.len() {
            return Err(DatasetError::IndexOutOfBounds);
        }
        
        Ok((self.images[index].clone(), self.labels[index]))
    }
}
```

### 12.1.2 ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®å®Ÿè£…

ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒãƒƒãƒå˜ä½ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

**Pythonï¼ˆPyTorchï¼‰ã®ã‚µãƒ³ãƒ—ãƒ©ãƒ¼**ï¼š

```python
from torch.utils.data import Sampler
import random

class RandomSampler(Sampler):
    def __init__(self, data_source, shuffle=True):
        self.data_source = data_source
        self.shuffle = shuffle
    
    def __iter__(self):
        indices = list(range(len(self.data_source)))
        if self.shuffle:
            random.shuffle(indices)
        return iter(indices)
    
    def __len__(self):
        return len(self.data_source)
```

**Rust å®Ÿè£…**ï¼š

```rust
use rand::seq::SliceRandom;
use rand::thread_rng;

pub trait Sampler {
    fn sample(&mut self, num_samples: usize) -> Vec<usize>;
    fn len(&self) -> usize;
}

pub struct RandomSampler {
    num_samples: usize,
    shuffle: bool,
}

impl RandomSampler {
    pub fn new(num_samples: usize, shuffle: bool) -> Self {
        Self { num_samples, shuffle }
    }
}

impl Sampler for RandomSampler {
    fn sample(&mut self, _num_samples: usize) -> Vec<usize> {
        let mut indices: Vec<usize> = (0..self.num_samples).collect();
        
        if self.shuffle {
            let mut rng = thread_rng();
            indices.shuffle(&mut rng);
        }
        
        indices
    }

    fn len(&self) -> usize {
        self.num_samples
    }
}

pub struct BatchSampler<S: Sampler> {
    sampler: S,
    batch_size: usize,
    drop_last: bool,
}

impl<S: Sampler> BatchSampler<S> {
    pub fn new(sampler: S, batch_size: usize, drop_last: bool) -> Self {
        Self {
            sampler,
            batch_size,
            drop_last,
        }
    }

    pub fn batches(&mut self) -> Vec<Vec<usize>> {
        let indices = self.sampler.sample(self.sampler.len());
        let mut batches = Vec::new();
        
        for chunk in indices.chunks(self.batch_size) {
            if chunk.len() == self.batch_size || !self.drop_last {
                batches.push(chunk.to_vec());
            }
        }
        
        batches
    }
}
```

### 12.1.3 DataLoader ã®å®Ÿè£…

DataLoader ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒãƒƒãƒã‚’ç”Ÿæˆã—ã€ä¸¦åˆ—å‡¦ç†ã‚’ç®¡ç†ã—ã¾ã™ã€‚

**Rust ã§ã®åŸºæœ¬çš„ãª DataLoader**ï¼š

```rust
use std::sync::Arc;
use rayon::prelude::*;

pub struct DataLoader<D: Dataset> {
    dataset: Arc<D>,
    batch_size: usize,
    shuffle: bool,
    num_workers: usize,
    drop_last: bool,
}

impl<D: Dataset> DataLoader<D>
where
    D: Send + Sync + 'static,
    D::Item: Send,
{
    pub fn new(
        dataset: D,
        batch_size: usize,
        shuffle: bool,
        num_workers: usize,
        drop_last: bool,
    ) -> Self {
        Self {
            dataset: Arc::new(dataset),
            batch_size,
            shuffle,
            num_workers,
            drop_last,
        }
    }

    pub fn iter(&self) -> DataLoaderIterator<D> {
        let mut sampler = RandomSampler::new(self.dataset.len(), self.shuffle);
        let batch_sampler = BatchSampler::new(sampler, self.batch_size, self.drop_last);
        let batches = batch_sampler.batches();
        
        DataLoaderIterator {
            dataset: Arc::clone(&self.dataset),
            batches,
            current_batch: 0,
            num_workers: self.num_workers,
        }
    }
}

pub struct DataLoaderIterator<D: Dataset> {
    dataset: Arc<D>,
    batches: Vec<Vec<usize>>,
    current_batch: usize,
    num_workers: usize,
}

impl<D: Dataset> Iterator for DataLoaderIterator<D>
where
    D: Send + Sync + 'static,
    D::Item: Send,
{
    type Item = Vec<D::Item>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.current_batch >= self.batches.len() {
            return None;
        }

        let indices = &self.batches[self.current_batch];
        self.current_batch += 1;

        // ä¸¦åˆ—ã«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        let batch: Vec<_> = if self.num_workers > 1 {
            indices
                .par_iter()
                .filter_map(|&idx| self.dataset.get(idx).ok())
                .collect()
        } else {
            indices
                .iter()
                .filter_map(|&idx| self.dataset.get(idx).ok())
                .collect()
        };

        Some(batch)
    }
}
```

---

## 12.2 ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ

### 12.2.1 å¤‰æ›ï¼ˆTransformï¼‰ã®è¨­è¨ˆ

**Pythonï¼ˆtorchvisionï¼‰ã® Transform**ï¼š

```python
import torchvision.transforms as transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225]),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomCrop(32, padding=4),
])
```

**Rust ã§ã® Transform ãƒˆãƒ¬ã‚¤ãƒˆ**ï¼š

```rust
use ndarray::{Array3, Array4};
use rand::Rng;

pub trait Transform {
    fn apply(&self, image: Array3<f32>) -> Array3<f32>;
}

/// æ­£è¦åŒ–å¤‰æ›
pub struct Normalize {
    mean: [f32; 3],
    std: [f32; 3],
}

impl Normalize {
    pub fn new(mean: [f32; 3], std: [f32; 3]) -> Self {
        Self { mean, std }
    }
}

impl Transform for Normalize {
    fn apply(&self, image: Array3<f32>) -> Array3<f32> {
        let (c, h, w) = image.dim();
        let mut normalized = image.clone();
        
        for ch in 0..c {
            for y in 0..h {
                for x in 0..w {
                    normalized[[ch, y, x]] = 
                        (normalized[[ch, y, x]] - self.mean[ch]) / self.std[ch];
                }
            }
        }
        
        normalized
    }
}

/// ãƒ©ãƒ³ãƒ€ãƒ æ°´å¹³åè»¢
pub struct RandomHorizontalFlip {
    p: f32,
}

impl RandomHorizontalFlip {
    pub fn new(p: f32) -> Self {
        assert!(p >= 0.0 && p <= 1.0);
        Self { p }
    }
}

impl Transform for RandomHorizontalFlip {
    fn apply(&self, image: Array3<f32>) -> Array3<f32> {
        let mut rng = rand::thread_rng();
        
        if rng.gen::<f32>() < self.p {
            // æ°´å¹³åè»¢
            let (c, h, w) = image.dim();
            let mut flipped = Array3::zeros((c, h, w));
            
            for ch in 0..c {
                for y in 0..h {
                    for x in 0..w {
                        flipped[[ch, y, x]] = image[[ch, y, w - 1 - x]];
                    }
                }
            }
            
            flipped
        } else {
            image
        }
    }
}

/// ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ãƒ­ãƒƒãƒ—
pub struct RandomCrop {
    size: usize,
    padding: usize,
}

impl RandomCrop {
    pub fn new(size: usize, padding: usize) -> Self {
        Self { size, padding }
    }
    
    fn pad_image(&self, image: &Array3<f32>) -> Array3<f32> {
        let (c, h, w) = image.dim();
        let new_h = h + 2 * self.padding;
        let new_w = w + 2 * self.padding;
        
        let mut padded = Array3::zeros((c, new_h, new_w));
        
        for ch in 0..c {
            for y in 0..h {
                for x in 0..w {
                    padded[[ch, y + self.padding, x + self.padding]] = image[[ch, y, x]];
                }
            }
        }
        
        padded
    }
}

impl Transform for RandomCrop {
    fn apply(&self, image: Array3<f32>) -> Array3<f32> {
        let padded = self.pad_image(&image);
        let (c, h, w) = padded.dim();
        
        let mut rng = rand::thread_rng();
        let y_start = rng.gen_range(0..=(h - self.size));
        let x_start = rng.gen_range(0..=(w - self.size));
        
        let mut cropped = Array3::zeros((c, self.size, self.size));
        
        for ch in 0..c {
            for y in 0..self.size {
                for x in 0..self.size {
                    cropped[[ch, y, x]] = padded[[ch, y_start + y, x_start + x]];
                }
            }
        }
        
        cropped
    }
}

/// è¤‡æ•°ã®å¤‰æ›ã‚’é †ç•ªã«é©ç”¨
pub struct Compose {
    transforms: Vec<Box<dyn Transform + Send + Sync>>,
}

impl Compose {
    pub fn new(transforms: Vec<Box<dyn Transform + Send + Sync>>) -> Self {
        Self { transforms }
    }
}

impl Transform for Compose {
    fn apply(&self, mut image: Array3<f32>) -> Array3<f32> {
        for transform in &self.transforms {
            image = transform.apply(image);
        }
        image
    }
}
```

### 12.2.2 GPU ã§ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ

ç”»åƒã®å‰å‡¦ç†ã‚’CPUã§è¡Œã†ã¨ã€GPUã®è¨ˆç®—ã‚’å¾…ãŸã›ã¦ã—ã¾ã†å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ç°¡å˜ãªå¤‰æ›ã¯GPUã‚«ãƒ¼ãƒãƒ«ã§å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã€åŠ¹ç‡ã‚’å‘ä¸Šã§ãã¾ã™ã€‚

**CUDA ã‚«ãƒ¼ãƒãƒ«ã§ã®æ­£è¦åŒ–**ï¼š

```cuda
__global__ void normalize_kernel(
    const float* input,
    float* output,
    const float* mean,
    const float* std,
    int batch_size,
    int channels,
    int height,
    int width
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = batch_size * channels * height * width;
    
    if (idx < total) {
        int w = idx % width;
        int h = (idx / width) % height;
        int c = (idx / (width * height)) % channels;
        int n = idx / (width * height * channels);
        
        output[idx] = (input[idx] - mean[c]) / std[c];
    }
}

__global__ void random_horizontal_flip_kernel(
    const float* input,
    float* output,
    const bool* flip_flags,
    int batch_size,
    int channels,
    int height,
    int width
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = batch_size * channels * height * width;
    
    if (idx < total) {
        int w = idx % width;
        int h = (idx / width) % height;
        int c = (idx / (width * height)) % channels;
        int n = idx / (width * height * channels);
        
        if (flip_flags[n]) {
            int flipped_w = width - 1 - w;
            int src_idx = ((n * channels + c) * height + h) * width + flipped_w;
            output[idx] = input[src_idx];
        } else {
            output[idx] = input[idx];
        }
    }
}
```

---

## 12.3 åŠ¹ç‡çš„ãª GPU ãƒ¡ãƒ¢ãƒªè»¢é€

### 12.3.1 ãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒªï¼ˆPinned Memoryï¼‰

ãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒªã‚’ä½¿ã†ã¨ã€CPU ã‹ã‚‰ GPU ã¸ã®ãƒ‡ãƒ¼ã‚¿è»¢é€ãŒé«˜é€ŸåŒ–ã•ã‚Œã¾ã™ã€‚

**Pythonï¼ˆPyTorchï¼‰ã§ã® pin_memory**ï¼š

```python
import torch

# ãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒªã‚’æœ‰åŠ¹ã«ã™ã‚‹
dataloader = DataLoader(
    dataset,
    batch_size=32,
    pin_memory=True,  # â† é‡è¦
)

for data, labels in dataloader:
    # non_blocking=True ã§éåŒæœŸè»¢é€
    data = data.cuda(non_blocking=True)
    labels = labels.cuda(non_blocking=True)
    
    # è¨ˆç®—å‡¦ç†
    output = model(data)
```

**CUDA ã§ã®ãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒªç¢ºä¿**ï¼š

```c
// ãƒ›ã‚¹ãƒˆå´ã§ãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒªã‚’ç¢ºä¿
float* h_data_pinned;
cudaMallocHost(&h_data_pinned, size * sizeof(float));

// é€šå¸¸ã®ãƒ¡ãƒ¢ãƒªè»¢é€ã‚ˆã‚Šé«˜é€Ÿ
cudaMemcpy(d_data, h_data_pinned, size * sizeof(float), cudaMemcpyHostToDevice);

// è§£æ”¾
cudaFreeHost(h_data_pinned);
```

**Rustï¼ˆcudarcï¼‰ã§ã®å®Ÿè£…**ï¼š

```rust
use cudarc::driver::*;
use std::sync::Arc;

pub struct PinnedMemoryAllocator {
    device: Arc<CudaDevice>,
}

impl PinnedMemoryAllocator {
    pub fn new(device: Arc<CudaDevice>) -> Self {
        Self { device }
    }

    pub fn alloc_pinned(&self, size: usize) -> Result<HostBuffer<f32>, CudaError> {
        // ãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒªã®ç¢ºä¿
        self.device.alloc_host(size)
    }

    pub fn copy_to_device_async(
        &self,
        host_data: &HostBuffer<f32>,
        stream: &CudaStream,
    ) -> Result<CudaSlice<f32>, CudaError> {
        let device_buffer = self.device.alloc_zeros::<f32>(host_data.len())?;
        
        // éåŒæœŸã§ãƒ‡ãƒã‚¤ã‚¹ã«ã‚³ãƒ”ãƒ¼
        self.device.htod_copy_into_async(host_data, &device_buffer, stream)?;
        
        Ok(device_buffer)
    }
}
```

### 12.3.2 ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä½¿ã£ãŸä¸¦åˆ—è»¢é€ã¨è¨ˆç®—

CUDA ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä½¿ã†ã¨ã€ãƒ‡ãƒ¼ã‚¿è»¢é€ã¨è¨ˆç®—ã‚’ä¸¦åˆ—åŒ–ã§ãã¾ã™ã€‚

**ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã®ãƒ‘ã‚¿ãƒ¼ãƒ³**ï¼š

```
Batch 0: [Transfer] â†’ [Compute] â†’ [Transfer Result]
Batch 1:               [Transfer] â†’ [Compute] â†’ [Transfer Result]
Batch 2:                             [Transfer] â†’ [Compute] â†’ ...
```

**Pythonï¼ˆPyTorchï¼‰ã§ã®å®Ÿè£…**ï¼š

```python
import torch

# è¤‡æ•°ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä½œæˆ
stream1 = torch.cuda.Stream()
stream2 = torch.cuda.Stream()

for i, (data, labels) in enumerate(dataloader):
    # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’äº¤äº’ã«ä½¿ã†
    stream = stream1 if i % 2 == 0 else stream2
    
    with torch.cuda.stream(stream):
        data = data.cuda(non_blocking=True)
        labels = labels.cuda(non_blocking=True)
        
        output = model(data)
        loss = criterion(output, labels)
        loss.backward()
```

**Rustï¼ˆcudarcï¼‰ã§ã®å®Ÿè£…**ï¼š

```rust
use cudarc::driver::*;
use std::sync::Arc;

pub struct PipelinedDataLoader {
    device: Arc<CudaDevice>,
    streams: Vec<CudaStream>,
    num_streams: usize,
}

impl PipelinedDataLoader {
    pub fn new(device: Arc<CudaDevice>, num_streams: usize) -> Result<Self, CudaError> {
        let mut streams = Vec::new();
        for _ in 0..num_streams {
            streams.push(device.fork_default_stream()?);
        }
        
        Ok(Self {
            device,
            streams,
            num_streams,
        })
    }

    pub fn load_batch_async(
        &self,
        batch_idx: usize,
        host_data: &HostBuffer<f32>,
    ) -> Result<CudaSlice<f32>, CudaError> {
        let stream_idx = batch_idx % self.num_streams;
        let stream = &self.streams[stream_idx];
        
        // éåŒæœŸè»¢é€
        let device_buffer = self.device.alloc_zeros::<f32>(host_data.len())?;
        self.device.htod_copy_into_async(host_data, &device_buffer, stream)?;
        
        Ok(device_buffer)
    }

    pub fn synchronize(&self, batch_idx: usize) -> Result<(), CudaError> {
        let stream_idx = batch_idx % self.num_streams;
        self.streams[stream_idx].synchronize()
    }
}
```

---

## 12.4 ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒãƒƒãƒå‡¦ç†

### 12.4.1 å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´

GPU ãƒ¡ãƒ¢ãƒªãŒé™ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‹•çš„ã«èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€Out of Memory ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã§ãã¾ã™ã€‚

**Python å®Ÿè£…**ï¼š

```python
import torch

def find_max_batch_size(model, input_shape, device):
    """æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’äºŒåˆ†æ¢ç´¢ã§è¦‹ã¤ã‘ã‚‹"""
    low, high = 1, 1024
    max_batch_size = 1
    
    while low <= high:
        mid = (low + high) // 2
        try:
            # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
            dummy_input = torch.randn(mid, *input_shape).to(device)
            dummy_target = torch.randint(0, 10, (mid,)).to(device)
            
            # Forward pass
            output = model(dummy_input)
            loss = F.cross_entropy(output, dummy_target)
            
            # Backward pass
            loss.backward()
            
            # æˆåŠŸã—ãŸã‚‰ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™
            max_batch_size = mid
            low = mid + 1
            
            # ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
            del dummy_input, dummy_target, output, loss
            torch.cuda.empty_cache()
            
        except RuntimeError as e:
            if 'out of memory' in str(e):
                # OOM ãªã®ã§ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
                high = mid - 1
                torch.cuda.empty_cache()
            else:
                raise e
    
    return max_batch_size

# ä½¿ç”¨ä¾‹
max_bs = find_max_batch_size(model, (3, 224, 224), 'cuda:0')
print(f"Max batch size: {max_bs}")
```

### 12.4.2 å‹¾é…ç´¯ç©ï¼ˆGradient Accumulationï¼‰

ãƒ¡ãƒ¢ãƒªãŒè¶³ã‚Šãªã„å ´åˆã€å°ã•ãªãƒãƒƒãƒã§è¤‡æ•°å› forward/backward ã‚’è¡Œã„ã€å‹¾é…ã‚’ç´¯ç©ã—ã¦ã‹ã‚‰ optimizer ã‚’æ›´æ–°ã—ã¾ã™ã€‚

**Python å®Ÿè£…**ï¼š

```python
accumulation_steps = 4  # 4 ãƒãƒƒãƒåˆ†ã®å‹¾é…ã‚’ç´¯ç©

optimizer.zero_grad()

for i, (data, labels) in enumerate(dataloader):
    data, labels = data.cuda(), labels.cuda()
    
    # Forward pass
    output = model(data)
    loss = criterion(output, labels)
    
    # å‹¾é…ã‚’ç´¯ç©ã™ã‚‹ãŸã‚ã€æå¤±ã‚’ accumulation_steps ã§å‰²ã‚‹
    loss = loss / accumulation_steps
    
    # Backward pass
    loss.backward()
    
    # accumulation_steps ã”ã¨ã« optimizer ã‚’æ›´æ–°
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**Rust ã§ã®æ¦‚å¿µçš„ãªå®Ÿè£…**ï¼š

```rust
pub struct GradientAccumulator {
    accumulation_steps: usize,
    current_step: usize,
}

impl GradientAccumulator {
    pub fn new(accumulation_steps: usize) -> Self {
        Self {
            accumulation_steps,
            current_step: 0,
        }
    }

    pub fn should_update(&mut self) -> bool {
        self.current_step += 1;
        if self.current_step >= self.accumulation_steps {
            self.current_step = 0;
            true
        } else {
            false
        }
    }

    pub fn scale_loss(&self, loss: f32) -> f32 {
        loss / self.accumulation_steps as f32
    }
}

// ä½¿ç”¨ä¾‹ï¼ˆç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰
let mut accumulator = GradientAccumulator::new(4);

for (data, labels) in dataloader.iter() {
    let output = model.forward(&data);
    let mut loss = criterion.compute(&output, &labels);
    
    // å‹¾é…ç´¯ç©ã®ãŸã‚ã«ã‚¹ã‚±ãƒ¼ãƒ«
    loss = accumulator.scale_loss(loss);
    
    model.backward(loss);
    
    if accumulator.should_update() {
        optimizer.step();
        optimizer.zero_grad();
    }
}
```

---

## 12.5 ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ

### 12.5.1 ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿

è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—ã«èª­ã¿è¾¼ã¿ã¾ã™ã€‚

**Rust ã§ã®å®Ÿè£…ï¼ˆrayon ã¨ crossbeam ã‚’ä½¿ç”¨ï¼‰**ï¼š

```rust
use crossbeam::channel::{bounded, Sender, Receiver};
use std::thread;
use std::sync::Arc;

pub struct ParallelDataLoader<D: Dataset> {
    dataset: Arc<D>,
    batch_size: usize,
    num_workers: usize,
    prefetch_factor: usize,
}

impl<D: Dataset> ParallelDataLoader<D>
where
    D: Send + Sync + 'static,
    D::Item: Send + 'static,
{
    pub fn new(
        dataset: D,
        batch_size: usize,
        num_workers: usize,
        prefetch_factor: usize,
    ) -> Self {
        Self {
            dataset: Arc::new(dataset),
            batch_size,
            num_workers,
            prefetch_factor,
        }
    }

    pub fn iter(&self) -> ParallelDataLoaderIterator<D> {
        let (sender, receiver) = bounded(self.prefetch_factor);
        
        let dataset = Arc::clone(&self.dataset);
        let batch_size = self.batch_size;
        let num_workers = self.num_workers;
        let num_batches = (dataset.len() + batch_size - 1) / batch_size;
        
        // ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’èµ·å‹•
        thread::spawn(move || {
            for batch_idx in 0..num_batches {
                let start_idx = batch_idx * batch_size;
                let end_idx = (start_idx + batch_size).min(dataset.len());
                let indices: Vec<usize> = (start_idx..end_idx).collect();
                
                // ä¸¦åˆ—ã«ãƒãƒƒãƒã‚’èª­ã¿è¾¼ã‚€
                let batch: Vec<_> = indices
                    .into_par_iter()
                    .filter_map(|idx| dataset.get(idx).ok())
                    .collect();
                
                if sender.send(batch).is_err() {
                    break;
                }
            }
        });
        
        ParallelDataLoaderIterator { receiver }
    }
}

pub struct ParallelDataLoaderIterator<D: Dataset> {
    receiver: Receiver<Vec<D::Item>>,
}

impl<D: Dataset> Iterator for ParallelDataLoaderIterator<D> {
    type Item = Vec<D::Item>;

    fn next(&mut self) -> Option<Self::Item> {
        self.receiver.recv().ok()
    }
}
```

### 12.5.2 ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ‘ã‚¿ãƒ¼ãƒ³

æ¬¡ã®ãƒãƒƒãƒã‚’äº‹å‰ã« GPU ã«è»¢é€ã—ã¦ãŠãã“ã¨ã§ã€è¨ˆç®—ã¨è»¢é€ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚

**ãƒ€ãƒ–ãƒ«ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³**ï¼š

```rust
use std::collections::VecDeque;

pub struct PrefetchBuffer<T> {
    buffer: VecDeque<T>,
    capacity: usize,
}

impl<T> PrefetchBuffer<T> {
    pub fn new(capacity: usize) -> Self {
        Self {
            buffer: VecDeque::with_capacity(capacity),
            capacity,
        }
    }

    pub fn push(&mut self, item: T) -> bool {
        if self.buffer.len() < self.capacity {
            self.buffer.push_back(item);
            true
        } else {
            false
        }
    }

    pub fn pop(&mut self) -> Option<T> {
        self.buffer.pop_front()
    }

    pub fn is_full(&self) -> bool {
        self.buffer.len() >= self.capacity
    }

    pub fn len(&self) -> usize {
        self.buffer.len()
    }
}

// ä½¿ç”¨ä¾‹
pub struct PrefetchingDataLoader<D: Dataset> {
    dataloader: ParallelDataLoader<D>,
    prefetch_buffer: PrefetchBuffer<Vec<D::Item>>,
}

impl<D: Dataset> PrefetchingDataLoader<D>
where
    D: Send + Sync + 'static,
    D::Item: Send + 'static,
{
    pub fn new(dataloader: ParallelDataLoader<D>, prefetch_size: usize) -> Self {
        Self {
            dataloader,
            prefetch_buffer: PrefetchBuffer::new(prefetch_size),
        }
    }

    pub fn next_batch(&mut self) -> Option<Vec<D::Item>> {
        // ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰å–ã‚Šå‡ºã™
        let batch = self.prefetch_buffer.pop();
        
        // ãƒãƒƒãƒ•ã‚¡ã‚’è£œå……
        for batch in self.dataloader.iter() {
            if !self.prefetch_buffer.push(batch) {
                break;
            }
        }
        
        batch
    }
}
```

---

## 12.6 ã¾ã¨ã‚

æœ¬ç« ã§ã¯ã€åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ Rust ã§å®Ÿè£…ã—ã¾ã—ãŸã€‚

**ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆ**ï¼š

1. **Dataset ã¨ DataLoader**: ãƒˆãƒ¬ã‚¤ãƒˆã‚’ä½¿ã£ãŸæŠ½è±¡åŒ–ã¨ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
2. **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ**: CPU/GPU ã§ã®ç”»åƒå¤‰æ›ã®å®Ÿè£…
3. **ãƒ¡ãƒ¢ãƒªè»¢é€æœ€é©åŒ–**: ãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒªã€ã‚¹ãƒˆãƒªãƒ¼ãƒ ã€éåŒæœŸè»¢é€
4. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–**: å‹¾é…ç´¯ç©ã€å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
5. **ä¸¦åˆ—å‡¦ç†**: ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**ï¼š

- `num_workers` ã‚’é©åˆ‡ã«è¨­å®šï¼ˆé€šå¸¸ 4-8ï¼‰
- `prefetch_factor` ã§ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’èª¿æ•´ï¼ˆé€šå¸¸ 2-4ï¼‰
- ãƒ”ãƒ³ç•™ã‚ãƒ¡ãƒ¢ãƒªã§è»¢é€ã‚’é«˜é€ŸåŒ–
- ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§è»¢é€ã¨è¨ˆç®—ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—

**æ¬¡ç« ã¸ã®æ¥ç¶š**ï¼š

æ¬¡ã®ç¬¬ 13 ç« ï¼ˆæ—§ç¬¬ 10 ç« ï¼‰ã§ã¯ã€å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã¨æœ€é©åŒ–æ‰‹æ³•ã®å®Ÿè£…ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã€æœ¬ç« ã§æ§‹ç¯‰ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½¿ã£ãŸå®Ÿéš›ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’å­¦ã³ã¾ã™ã€‚

---

## å‚è€ƒæ–‡çŒ®

- PyTorch Data Loading Tutorial: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html
- CUDA Best Practices Guide - Memory Optimization: https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/
- NVIDIA DALI (Data Loading Library): https://github.com/NVIDIA/DALI
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬11ç« : æå¤±é–¢æ•°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®Ÿè£…](04-11-æå¤±é–¢æ•°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®Ÿè£….md) | [â¡ï¸ ç¬¬13ç« : å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã¨æœ€é©åŒ–æ‰‹æ³•](04-13-å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã¨æœ€é©åŒ–æ‰‹æ³•.md)