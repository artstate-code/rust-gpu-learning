[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬12ç« ](04-12-ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†.md) | [â¡ï¸ ç¬¬14ç« ](04-14-ãƒ‡ãƒãƒƒã‚°ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°.md)

---

# ç¬¬ 10 ç« ã€€å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã¨æœ€é©åŒ–æ‰‹æ³•

ã“ã®ç« ã§ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã¨ã€å„ç¨®æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆSGDã€Adamã€AdamWç­‰ï¼‰ã‚’å®Ÿè£…ã—ã¾ã™ã€‚PyTorchã®å®Ÿè£…ã‚’å‚è€ƒã«ã—ãªãŒã‚‰ã€Rustã§å‹å®‰å…¨ã‹ã¤é«˜æ€§èƒ½ãªæœ€é©åŒ–å™¨ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

**ç›®çš„**: æ©Ÿæ¢°å­¦ç¿’ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’ç†è§£ã—ã€æœ€æ–°ã®æœ€é©åŒ–æ‰‹æ³•ã‚’ã‚¼ãƒ­ã‹ã‚‰å®Ÿè£…ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

## 10.1 forward â†’ loss â†’ backward â†’ optimizer ã®æµã‚Œ

### å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å…¨ä½“åƒ

```mermaid
graph LR
    subgraph Epoch["ã‚¨ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—"]
        subgraph Batch["ãƒãƒƒãƒãƒ«ãƒ¼ãƒ—"]
            Forward[Forward Pass<br/>äºˆæ¸¬è¨ˆç®—] --> Loss[Loss<br/>æå¤±è¨ˆç®—]
            Loss --> Backward[Backward Pass<br/>å‹¾é…è¨ˆç®—]
            Backward --> Update[Optimizer<br/>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°]
            Update --> Forward
        end
    end
    
    style Forward fill:#e1f5ff
    style Loss fill:#ffe1f5
    style Backward fill:#fff4e1
    style Update fill:#e1ffe1
```

**æ•°å¼è¡¨ç¾**:

1. **Forward**: $\hat{y} = f(x; \theta)$
2. **Loss**: $L = \mathcal{L}(\hat{y}, y)$
3. **Backward**: $\nabla_\theta L = \frac{\partial L}{\partial \theta}$
4. **Update**: $\theta_{t+1} = \theta_t - \eta \nabla_\theta L$

### Pythonï¼ˆPyTorchï¼‰ã§ã®å®Ÿè£…

```python
import torch
import torch.nn as nn
import torch.optim as optim

# ãƒ¢ãƒ‡ãƒ«å®šç¾©
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Linear(256, 10)
)

# æœ€é©åŒ–å™¨
optimizer = optim.SGD(model.parameters(), lr=0.01)

# æå¤±é–¢æ•°
criterion = nn.CrossEntropyLoss()

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—
for epoch in range(10):
    for batch_idx, (data, target) in enumerate(train_loader):
        # 1. Forward pass
        output = model(data)
        
        # 2. Loss
        loss = criterion(output, target)
        
        # 3. Backward pass
        optimizer.zero_grad()  # å‹¾é…ã‚’ã‚¼ãƒ­åŒ–
        loss.backward()
        
        # 4. Update
        optimizer.step()
        
        if batch_idx % 100 == 0:
            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')
```

### Rust ã§ã®å®Ÿè£…

```rust
use ndarray::{Array1, Array2};

pub struct LinearLayer {
    weight: Array2<f32>,  // (out_features, in_features)
    bias: Array1<f32>,
    // å‹¾é…ä¿å­˜ç”¨
    grad_weight: Option<Array2<f32>>,
    grad_bias: Option<Array1<f32>>,
    // Forwardæ™‚ã®å…¥åŠ›ä¿å­˜ï¼ˆBackwardç”¨ï¼‰
    last_input: Option<Array2<f32>>,
}

impl LinearLayer {
    pub fn new(in_features: usize, out_features: usize) -> Self {
        use ndarray_rand::RandomExt;
        use ndarray_rand::rand_distr::Normal;
        
        // HeåˆæœŸåŒ–
        let std = (2.0 / in_features as f32).sqrt();
        let weight = Array2::random((out_features, in_features), Normal::new(0.0, std).unwrap());
        let bias = Array1::zeros(out_features);
        
        Self {
            weight,
            bias,
            grad_weight: None,
            grad_bias: None,
            last_input: None,
        }
    }
    
    pub fn forward(&mut self, input: &Array2<f32>) -> Array2<f32> {
        // å…¥åŠ›ã‚’ä¿å­˜ï¼ˆBackward ã§ä½¿ç”¨ï¼‰
        self.last_input = Some(input.clone());
        
        // y = xW^T + b
        input.dot(&self.weight.t()) + &self.bias
    }
    
    pub fn backward(&mut self, grad_output: &Array2<f32>) -> Array2<f32> {
        let input = self.last_input.as_ref().expect("forward not called");
        
        // âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚y Â· x^T
        self.grad_weight = Some(grad_output.t().dot(input));
        
        // âˆ‚L/âˆ‚b = sum(âˆ‚L/âˆ‚y, axis=0)
        self.grad_bias = Some(grad_output.sum_axis(ndarray::Axis(0)));
        
        // âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚y Â· W
        grad_output.dot(&self.weight)
    }
    
    pub fn parameters(&self) -> Vec<&Array2<f32>> {
        vec![&self.weight]
    }
    
    pub fn gradients(&self) -> Option<Vec<&Array2<f32>>> {
        self.grad_weight.as_ref().map(|gw| vec![gw])
    }
}

// å­¦ç¿’ãƒ«ãƒ¼ãƒ—
fn train_loop() {
    let mut model = LinearLayer::new(784, 10);
    let lr = 0.01;
    
    for epoch in 0..10 {
        for (batch_idx, (data, target)) in train_data.iter().enumerate() {
            // 1. Forward
            let output = model.forward(data);
            
            // 2. Lossï¼ˆç°¡ç•¥åŒ–ï¼‰
            let loss = cross_entropy_loss(&output, target);
            
            // 3. Backward
            let grad_output = cross_entropy_backward(&output, target);
            model.backward(&grad_output);
            
            // 4. Updateï¼ˆSGDï¼‰
            if let Some(grad_w) = &model.grad_weight {
                model.weight = &model.weight - &(grad_w * lr);
            }
            if let Some(grad_b) = &model.grad_bias {
                model.bias = &model.bias - &(grad_b * lr);
            }
        }
    }
}
```

## 10.2 SGD / Adam / AdamW / RMSProp ã®å®Ÿè£…

### æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒ

| ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | ç™ºè¡¨å¹´ | ä¸»ãªç‰¹å¾´ | è¨ˆç®—é‡ | ãƒ¡ãƒ¢ãƒª |
|------------|-------|---------|--------|--------|
| **SGD** | 1951 | ã‚·ãƒ³ãƒ—ãƒ«ã€ç†è«–çš„ä¿è¨¼ | $O(n)$ | $O(n)$ |
| **Momentum** | 1964 | æ…£æ€§é …ã§åŠ é€Ÿ | $O(n)$ | $O(2n)$ |
| **RMSProp** | 2012 | é©å¿œçš„å­¦ç¿’ç‡ | $O(n)$ | $O(2n)$ |
| **Adam** | 2014 | Momentum + RMSProp | $O(n)$ | $O(3n)$ |
| **AdamW** | 2017 | é‡ã¿æ¸›è¡°ã®ä¿®æ­£ | $O(n)$ | $O(3n)$ |

### SGDï¼ˆç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ï¼‰

**æ›´æ–°å¼**:

$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta L
$$

ã“ã“ã§ã€$\eta$ ã¯å­¦ç¿’ç‡ï¼ˆlearning rateï¼‰ã§ã™ã€‚

**Pythonï¼ˆPyTorchï¼‰**:

```python
import torch.optim as optim

optimizer = optim.SGD(model.parameters(), lr=0.01)

# æ›´æ–°
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

**Rust å®Ÿè£…**:

```rust
pub struct SGD {
    lr: f32,
}

impl SGD {
    pub fn new(lr: f32) -> Self {
        Self { lr }
    }
    
    pub fn step(&self, params: &mut [Array2<f32>], grads: &[Array2<f32>]) {
        for (param, grad) in params.iter_mut().zip(grads) {
            // Î¸ = Î¸ - Î·âˆ‡L
            *param = &*param - &(grad * self.lr);
        }
    }
}
```

### Momentum SGD

**æ›´æ–°å¼**:

$$
\begin{align}
v_t &= \beta v_{t-1} + \nabla_\theta L \\
\theta_{t+1} &= \theta_t - \eta v_t
\end{align}
$$

$\beta$ ã¯æ…£æ€§ä¿‚æ•°ï¼ˆé€šå¸¸0.9ï¼‰

**ç‰©ç†çš„è§£é‡ˆ**: ãƒœãƒ¼ãƒ«ãŒå‚ã‚’è»¢ãŒã‚‹æ§˜å­ï¼ˆæ…£æ€§ãŒã‚ã‚‹ï¼‰

**Rust å®Ÿè£…**:

```rust
pub struct MomentumSGD {
    lr: f32,
    momentum: f32,
    velocity: Vec<Array2<f32>>,  // é€Ÿåº¦ã®çŠ¶æ…‹
}

impl MomentumSGD {
    pub fn new(params: &[Array2<f32>], lr: f32, momentum: f32) -> Self {
        let velocity = params.iter()
            .map(|p| Array2::zeros(p.raw_dim()))
            .collect();
        
        Self { lr, momentum, velocity }
    }
    
    pub fn step(&mut self, params: &mut [Array2<f32>], grads: &[Array2<f32>]) {
        for ((param, grad), vel) in params.iter_mut()
            .zip(grads)
            .zip(&mut self.velocity) 
        {
            // v = Î²Â·v + âˆ‡L
            *vel = &*vel * self.momentum + grad;
            
            // Î¸ = Î¸ - Î·Â·v
            *param = &*param - &(vel * self.lr);
        }
    }
}
```

### Adamï¼ˆAdaptive Moment Estimationï¼‰

**æ›´æ–°å¼** [^1]:

$$
\begin{align}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta L \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) (\nabla_\theta L)^2 \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
\theta_{t+1} &= \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align}
$$

[^1]: Kingma, D. P., & Ba, J. (2014). "Adam: A Method for Stochastic Optimization." arXiv:1412.6980

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- $\beta_1 = 0.9$: 1æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼ˆå¹³å‡ï¼‰ã®æ¸›è¡°ç‡
- $\beta_2 = 0.999$: 2æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆï¼ˆåˆ†æ•£ï¼‰ã®æ¸›è¡°ç‡
- $\epsilon = 10^{-8}$: æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®å°ã•ã„å€¤

**Pythonï¼ˆPyTorchï¼‰**:

```python
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=0.001,
    betas=(0.9, 0.999),
    eps=1e-8
)
```

**Rust å®Ÿè£…**:

```rust
pub struct Adam {
    lr: f32,
    beta1: f32,
    beta2: f32,
    eps: f32,
    t: usize,  // ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—
    m: Vec<Array2<f32>>,  // 1æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ
    v: Vec<Array2<f32>>,  // 2æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ
}

impl Adam {
    pub fn new(params: &[Array2<f32>], lr: f32) -> Self {
        let m = params.iter().map(|p| Array2::zeros(p.raw_dim())).collect();
        let v = params.iter().map(|p| Array2::zeros(p.raw_dim())).collect();
        
        Self {
            lr,
            beta1: 0.9,
            beta2: 0.999,
            eps: 1e-8,
            t: 0,
            m,
            v,
        }
    }
    
    pub fn step(&mut self, params: &mut [Array2<f32>], grads: &[Array2<f32>]) {
        self.t += 1;
        let t = self.t as f32;
        
        for ((param, grad), (m, v)) in params.iter_mut()
            .zip(grads)
            .zip(self.m.iter_mut().zip(&mut self.v))
        {
            // m = Î²â‚Â·m + (1-Î²â‚)Â·âˆ‡L
            *m = &*m * self.beta1 + &(grad * (1.0 - self.beta1));
            
            // v = Î²â‚‚Â·v + (1-Î²â‚‚)Â·(âˆ‡L)Â²
            *v = &*v * self.beta2 + &(grad.mapv(|x| x * x) * (1.0 - self.beta2));
            
            // ãƒã‚¤ã‚¢ã‚¹è£œæ­£
            let m_hat = m / (1.0 - self.beta1.powf(t));
            let v_hat = v / (1.0 - self.beta2.powf(t));
            
            // Î¸ = Î¸ - Î·Â·mÌ‚/(âˆšvÌ‚ + Îµ)
            let update = m_hat / (v_hat.mapv(|x| x.sqrt()) + self.eps);
            *param = &*param - &(update * self.lr);
        }
    }
}
```

### AdamWï¼ˆAdam with Decoupled Weight Decayï¼‰

**AdamW** [^2] ã¯ã€Adamã®é‡ã¿æ¸›è¡°ï¼ˆweight decayï¼‰ã‚’ä¿®æ­£ã—ãŸç‰ˆã§ã™ã€‚

[^2]: Loshchilov, I., & Hutter, F. (2017). "Decoupled Weight Decay Regularization." arXiv:1711.05101

**Adam ã®å•é¡Œç‚¹**:

é€šå¸¸ã®L2æ­£å‰‡åŒ–:
$$
L_{\text{reg}} = L + \frac{\lambda}{2} \|\theta\|^2
$$

å‹¾é…:
$$
\nabla_\theta L_{\text{reg}} = \nabla_\theta L + \lambda \theta
$$

Adamã§ã¯ã€ã“ã® $\lambda \theta$ ãŒé©å¿œçš„å­¦ç¿’ç‡ã«å½±éŸ¿ã•ã‚Œã¦åŠ¹æœãŒè–„ã¾ã‚Šã¾ã™ã€‚

**AdamW ã®æ›´æ–°å¼**:

$$
\theta_{t+1} = (1 - \eta \lambda) \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

é‡ã¿æ¸›è¡°ã‚’**é©å¿œçš„å­¦ç¿’ç‡ã®å¤–**ã§é©ç”¨ã—ã¾ã™ã€‚

**Rust å®Ÿè£…**:

```rust
pub struct AdamW {
    lr: f32,
    beta1: f32,
    beta2: f32,
    eps: f32,
    weight_decay: f32,  // Î»
    t: usize,
    m: Vec<Array2<f32>>,
    v: Vec<Array2<f32>>,
}

impl AdamW {
    pub fn step(&mut self, params: &mut [Array2<f32>], grads: &[Array2<f32>]) {
        self.t += 1;
        let t = self.t as f32;
        
        for ((param, grad), (m, v)) in params.iter_mut()
            .zip(grads)
            .zip(self.m.iter_mut().zip(&mut self.v))
        {
            // Adam ã¨åŒã˜ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ›´æ–°
            *m = &*m * self.beta1 + &(grad * (1.0 - self.beta1));
            *v = &*v * self.beta2 + &(grad.mapv(|x| x * x) * (1.0 - self.beta2));
            
            let m_hat = m / (1.0 - self.beta1.powf(t));
            let v_hat = v / (1.0 - self.beta2.powf(t));
            
            // AdamW: é‡ã¿æ¸›è¡°ã‚’åˆ†é›¢
            let update = m_hat / (v_hat.mapv(|x| x.sqrt()) + self.eps);
            *param = &(&*param * (1.0 - self.lr * self.weight_decay))  // é‡ã¿æ¸›è¡°
                   - &(update * self.lr);                              // Adamæ›´æ–°
        }
    }
}
```

### RMSProp

**æ›´æ–°å¼** [^3]:

$$
\begin{align}
v_t &= \beta v_{t-1} + (1 - \beta) (\nabla_\theta L)^2 \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{v_t} + \epsilon} \nabla_\theta L
\end{align}
$$

[^3]: Tieleman, T., & Hinton, G. (2012). "RMSprop: Divide the gradient by a running average of its recent magnitude." COURSERA: Neural Networks for Machine Learning.

**Rust å®Ÿè£…**:

```rust
pub struct RMSProp {
    lr: f32,
    beta: f32,  // é€šå¸¸ 0.99
    eps: f32,
    v: Vec<Array2<f32>>,
}

impl RMSProp {
    pub fn step(&mut self, params: &mut [Array2<f32>], grads: &[Array2<f32>]) {
        for ((param, grad), v) in params.iter_mut()
            .zip(grads)
            .zip(&mut self.v)
        {
            // v = Î²Â·v + (1-Î²)Â·(âˆ‡L)Â²
            *v = &*v * self.beta + &(grad.mapv(|x| x * x) * (1.0 - self.beta));
            
            // Î¸ = Î¸ - Î·/(âˆšv + Îµ)Â·âˆ‡L
            let update = grad / (v.mapv(|x| x.sqrt()) + self.eps);
            *param = &*param - &(update * self.lr);
        }
    }
}
```

### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒ

| æœ€é©åŒ–å™¨ | åæŸé€Ÿåº¦ | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ | æ¨å¥¨ç”¨é€” |
|---------|---------|-----------|---------------------|---------|
| **SGD** | é…ã„ | æœ€è‰¯ | é›£ã—ã„ | ç†è«–ç ”ç©¶ |
| **SGD + Momentum** | ä¸­ | è‰¯ | ä¸­ | CNN |
| **RMSProp** | é€Ÿã„ | è‰¯ | æ¯”è¼ƒçš„å®¹æ˜“ | RNN |
| **Adam** | æœ€é€Ÿ | ä¸­ | å®¹æ˜“ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠ |
| **AdamW** | æœ€é€Ÿ | ä¸­ | å®¹æ˜“ | Transformer |

## 10.3 å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ãƒ»å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©

### å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°

**å‹¾é…çˆ†ç™º**ï¼ˆGradient Explosionï¼‰ã‚’é˜²ãæŠ€è¡“ã§ã™ [^4]ã€‚

[^4]: Pascanu, R., Mikolov, T., & Bengio, Y. (2013). "On the difficulty of training recurrent neural networks." ICML.

**å‹¾é…ãƒãƒ«ãƒ ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°**:

$$
\text{if } \|\nabla\| > \text{threshold}: \quad \nabla \leftarrow \frac{\text{threshold}}{\|\nabla\|} \nabla
$$

**Pythonï¼ˆPyTorchï¼‰**:

```python
import torch.nn.utils as utils

# å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
max_norm = 1.0
utils.clip_grad_norm_(model.parameters(), max_norm)

optimizer.step()
```

**Rust å®Ÿè£…**:

```rust
pub fn clip_grad_norm(grads: &mut [Array2<f32>], max_norm: f32) -> f32 {
    // å…¨å‹¾é…ã®L2ãƒãƒ«ãƒ ã‚’è¨ˆç®—
    let total_norm: f32 = grads.iter()
        .map(|g| g.mapv(|x| x * x).sum())
        .sum::<f32>()
        .sqrt();
    
    if total_norm > max_norm {
        let clip_coef = max_norm / (total_norm + 1e-6);
        for grad in grads.iter_mut() {
            *grad = &*grad * clip_coef;
        }
    }
    
    total_norm
}
```

### å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©

**ç¨®é¡**:

| ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© | å¼ | ç”¨é€” |
|------------|-----|------|
| **Step Decay** | $\eta_t = \eta_0 \cdot \gamma^{\lfloor t/k \rfloor}$ | å®šæœŸçš„ã«æ¸›è¡° |
| **Exponential** | $\eta_t = \eta_0 \cdot \gamma^t$ | æŒ‡æ•°çš„æ¸›è¡° |
| **Cosine Annealing** | $\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})(1 + \cos(\frac{t\pi}{T}))$ | å‘¨æœŸçš„å¤‰åŒ– |
| **OneCycleLR** | ä¸‰è§’å½¢ã®å­¦ç¿’ç‡å¤‰åŒ– | é«˜é€Ÿå­¦ç¿’ |

**Pythonï¼ˆPyTorchï¼‰**:

```python
from torch.optim.lr_scheduler import CosineAnnealingLR

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = CosineAnnealingLR(optimizer, T_max=100)

for epoch in range(100):
    train(...)
    scheduler.step()  # å­¦ç¿’ç‡ã‚’æ›´æ–°
```

**Rust å®Ÿè£…**:

```rust
pub trait LRScheduler {
    fn get_lr(&self, epoch: usize) -> f32;
}

pub struct CosineAnnealingLR {
    eta_min: f32,
    eta_max: f32,
    t_max: usize,
}

impl LRScheduler for CosineAnnealingLR {
    fn get_lr(&self, epoch: usize) -> f32 {
        let t = epoch as f32;
        let t_max = self.t_max as f32;
        let cos_val = ((t * std::f32::consts::PI / t_max).cos() + 1.0) / 2.0;
        self.eta_min + (self.eta_max - self.eta_min) * cos_val
    }
}

// ä½¿ç”¨ä¾‹
let scheduler = CosineAnnealingLR {
    eta_min: 0.0,
    eta_max: 0.001,
    t_max: 100,
};

for epoch in 0..100 {
    let lr = scheduler.get_lr(epoch);
    optimizer.lr = lr;  // å­¦ç¿’ç‡ã‚’æ›´æ–°
    train_epoch(...);
}
```

## 10.4 mixed precisionï¼ˆFP16/BF16ï¼‰ãƒ»é‡å­åŒ–ãƒ»sparsity

### Mixed Precision Training

**Mixed Precision Training** [^5] ã¯ã€FP16ã¨FP32ã‚’çµ„ã¿åˆã‚ã›ã¦ã€ãƒ¡ãƒ¢ãƒªã¨é€Ÿåº¦ã‚’æ”¹å–„ã—ã¾ã™ã€‚

[^5]: Micikevicius, P., et al. (2017). "Mixed Precision Training." arXiv:1710.03740

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

1. ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’FP16ã«ã‚³ãƒ”ãƒ¼
2. FP16ã§Forward/Backward
3. æå¤±ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå‹¾é…ã®ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢ï¼‰
4. FP32ã§é‡ã¿æ›´æ–°

**æå¤±ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**:

$$
L_{\text{scaled}} = L \times \text{scale}
$$

é€šå¸¸ã€scale = 1024 or 2048

**Pythonï¼ˆPyTorchï¼‰**:

```python
from torch.cuda.amp import autocast, GradScaler

model = MyModel().cuda()
optimizer = torch.optim.Adam(model.parameters())
scaler = GradScaler()

for data, target in train_loader:
    optimizer.zero_grad()
    
    # FP16ã§è¨ˆç®—
    with autocast():
        output = model(data)
        loss = criterion(output, target)
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã¦Backward
    scaler.scale(loss).backward()
    
    # ã‚¢ãƒ³ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦Update
    scaler.step(optimizer)
    scaler.update()
```

**Rust å®Ÿè£…**:

```rust
pub struct MixedPrecisionTrainer {
    model_fp32: Vec<Array2<f32>>,
    model_fp16: Vec<Array2<f16>>,
    loss_scale: f32,
}

impl MixedPrecisionTrainer {
    pub fn train_step(&mut self, x: Array2<f32>, y: Array2<f32>) {
        // 1. FP16ã«å¤‰æ›
        let x_fp16 = to_fp16(&x);
        
        // 2. FP16ã§Forward
        let pred_fp16 = self.forward_fp16(&x_fp16);
        
        // 3. æå¤±è¨ˆç®—ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰
        let loss_fp16 = mse_loss(&pred_fp16, &to_fp16(&y));
        let loss_scaled = loss_fp16 * self.loss_scale;
        
        // 4. FP16ã§Backward
        let grads_fp16 = self.backward_fp16(loss_scaled);
        
        // 5. FP32ã«å¤‰æ›ã—ã¦ã‚¢ãƒ³ã‚¹ã‚±ãƒ¼ãƒ«
        let grads_fp32: Vec<Array2<f32>> = grads_fp16.iter()
            .map(|g| to_fp32(g) / self.loss_scale)
            .collect();
        
        // 6. FP32ã§æ›´æ–°
        self.optimizer.step(&mut self.model_fp32, &grads_fp32);
        
        // 7. FP32 â†’ FP16 ã‚³ãƒ”ãƒ¼
        self.model_fp16 = self.model_fp32.iter()
            .map(to_fp16)
            .collect();
    }
}

fn to_fp16(x: &Array2<f32>) -> Array2<half::f16> {
    x.mapv(half::f16::from_f32)
}

fn to_fp32(x: &Array2<half::f16>) -> Array2<f32> {
    x.mapv(|h| h.to_f32())
}
```

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœ**:

| ãƒ¢ãƒ‡ãƒ« | FP32 | FP16 | å‰Šæ¸›ç‡ |
|--------|------|------|--------|
| ResNet-50 | 102 MB | 51 MB | 50% |
| GPT-3ï¼ˆ175Bï¼‰ | 700 GB | 350 GB | 50% |
| Llama-2-70B | 280 GB | 140 GB | 50% |

## 10.5 å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆGradient Checkpointingï¼‰

**å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ** [^6] ã¯ã€ãƒ¡ãƒ¢ãƒªã‚’æ™‚é–“ã¨ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã™ã‚‹æŠ€è¡“ã§ã™ã€‚

[^6]: Chen, T., et al. (2016). "Training Deep Nets with Sublinear Memory Cost." arXiv:1604.06174

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å•é¡Œ

**é€šå¸¸ã®Backward**:

| ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•° | ä¿å­˜ã™ã‚‹ä¸­é–“å€¤ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ |
|-----------|-------------|------------|
| 10å±¤ | 10å€‹ | 100 MB |
| 100å±¤ | 100å€‹ | 1 GB |
| 1000å±¤ | 1000å€‹ | 10 GB |

**ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ³•**:

$\sqrt{n}$ å€‹ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã€å¿…è¦ã«å¿œã˜ã¦å†è¨ˆç®—ï¼š

| ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•° | ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ•° | å†è¨ˆç®—å›æ•° | ãƒ¡ãƒ¢ãƒª |
|-----------|-----------------|----------|--------|
| 100å±¤ | 10å€‹ | ~10å› | 100 MB |
| 1000å±¤ | 32å€‹ | ~32å› | 320 MB |

**å‰Šæ¸›ç‡**: $O(n) \rightarrow O(\sqrt{n})$

**Pythonï¼ˆPyTorchï¼‰**:

```python
from torch.utils.checkpoint import checkpoint

class CheckpointedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Linear(1000, 1000) for _ in range(100)
        ])
    
    def forward(self, x):
        for layer in self.layers:
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½¿ç”¨
            x = checkpoint(layer, x)
        return x

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: é€šå¸¸ã® ~10% ã«å‰Šæ¸›
```

**Rust ã§ã®æ¦‚å¿µå®Ÿè£…**:

```rust
pub struct CheckpointedLayer<F>
where
    F: Fn(&Array2<f32>) -> Array2<f32>,
{
    forward_fn: F,
    // ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½ç½®
    checkpoint_interval: usize,
}

impl<F> CheckpointedLayer<F>
where
    F: Fn(&Array2<f32>) -> Array2<f32>,
{
    pub fn forward(&self, input: &Array2<f32>, layer_idx: usize) -> Array2<f32> {
        let output = (self.forward_fn)(input);
        
        if layer_idx % self.checkpoint_interval == 0 {
            // ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: ä¸­é–“å€¤ã‚’ä¿å­˜
            save_checkpoint(layer_idx, &output);
        }
        
        output
    }
    
    pub fn backward(&self, grad_output: &Array2<f32>, layer_idx: usize) -> Array2<f32> {
        // æœ€ã‚‚è¿‘ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†è¨ˆç®—
        let checkpoint_idx = (layer_idx / self.checkpoint_interval) * self.checkpoint_interval;
        let input = load_checkpoint(checkpoint_idx);
        
        // Forward ã‚’å†å®Ÿè¡Œ
        for i in checkpoint_idx..layer_idx {
            input = (self.forward_fn)(&input);
        }
        
        // Backward ã‚’è¨ˆç®—
        compute_gradient(&input, grad_output)
    }
}
```

## 10.6 Rust ã§ã®å†ç¾: burn / tch-rs å†…éƒ¨æ§‹é€ è§£æ

### burn ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

**burn** [^7] ã¯ã€Rustè£½ã®ãƒ¢ãƒ€ãƒ³ãªæ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚

[^7]: burn: https://github.com/tracel-ai/burn

**ç‰¹å¾´**:

| é …ç›® | PyTorch | burn |
|------|---------|------|
| è¨€èª | Python + C++ | Rust |
| ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ | CUDA, CPU | NdArray, Wgpu, Candle |
| å‹å®‰å…¨æ€§ | å®Ÿè¡Œæ™‚ | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ |
| è‡ªå‹•å¾®åˆ† | å‹•çš„ã‚°ãƒ©ãƒ• | å‹•çš„ã‚°ãƒ©ãƒ• |
| ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ  | æˆç†Ÿ | ç™ºå±•é€”ä¸Š |

**burn ã§ã®å®Ÿè£…ä¾‹**:

```rust
use burn::{
    module::Module,
    nn::{Linear, LinearConfig},
    tensor::{backend::Backend, Tensor},
    train::{TrainStep, TrainOutput},
};

#[derive(Module, Debug)]
pub struct Model<B: Backend> {
    linear1: Linear<B>,
    linear2: Linear<B>,
}

impl<B: Backend> Model<B> {
    pub fn new(device: &B::Device) -> Self {
        Self {
            linear1: LinearConfig::new(784, 256).init(device),
            linear2: LinearConfig::new(256, 10).init(device),
        }
    }
    
    pub fn forward(&self, x: Tensor<B, 2>) -> Tensor<B, 2> {
        let x = self.linear1.forward(x);
        let x = x.relu();
        self.linear2.forward(x)
    }
}

// å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—
impl<B: Backend> TrainStep<Input, Output> for Model<B> {
    fn step(&self, item: Input) -> TrainOutput<Output> {
        let x = item.x;
        let y = item.y;
        
        let output = self.forward(x);
        let loss = CrossEntropyLoss::new().forward(output.clone(), y.clone());
        
        TrainOutput::new(self, loss, output)
    }
}
```

### tch-rsï¼ˆPyTorch ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼‰

**tch-rs** [^8] ã¯ã€PyTorchã®libtorchã‚’Rustã‹ã‚‰å‘¼ã³å‡ºã™ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚

[^8]: tch-rs: https://github.com/LaurentMazare/tch-rs

**åˆ©ç‚¹**:
- PyTorchã®å…¨æ©Ÿèƒ½ãŒä½¿ãˆã‚‹
- å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
- è±Šå¯Œãªã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ 

**æ¬ ç‚¹**:
- libtorchã¸ã®ä¾å­˜ï¼ˆå¤§ãã„ãƒã‚¤ãƒŠãƒªï¼‰
- Rust-nativeã§ã¯ãªã„

```rust
use tch::{nn, nn::Module, nn::OptimizerConfig, Device, Tensor};

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let vs = nn::VarStore::new(Device::Cuda(0));
    let net = nn::seq()
        .add(nn::linear(&vs.root(), 784, 256, Default::default()))
        .add_fn(|x| x.relu())
        .add(nn::linear(&vs.root(), 256, 10, Default::default()));
    
    let mut opt = nn::Adam::default().build(&vs, 1e-3)?;
    
    for epoch in 1..=10 {
        let (data, target) = load_batch();
        
        let output = net.forward(&data);
        let loss = output.cross_entropy_for_logits(&target);
        
        opt.backward_step(&loss);
        
        println!("Epoch: {}, Loss: {:.4}", epoch, loss.double_value(&[]));
    }
    
    Ok(())
}
```

### ã¾ã¨ã‚

| å´é¢ | Python (PyTorch) | Rust (burn) | Rust (tch-rs) |
|------|-----------------|------------|--------------|
| å­¦ç¿’ã‚³ã‚¹ãƒˆ | ä½ | é«˜ | ä¸­ |
| é–‹ç™ºé€Ÿåº¦ | é€Ÿã„ | ä¸­ | ä¸­ |
| å®Ÿè¡Œé€Ÿåº¦ | é«˜ | ä¸­ã€œé«˜ | é«˜ï¼ˆPyTorchåŒç­‰ï¼‰ |
| ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ | ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ |
| ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ  | æœ€é«˜ | ç™ºå±•ä¸­ | PyTorchã«ä¾å­˜ |

**æ¨å¥¨ç”¨é€”**:
- **ç ”ç©¶ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—**: PyTorch
- **Rustãƒã‚¤ãƒ†ã‚£ãƒ–**: burnï¼ˆè»½é‡ã€ç§»æ¤æ€§ï¼‰
- **PyTorchè³‡ç”£æ´»ç”¨**: tch-rsï¼ˆäº’æ›æ€§ï¼‰

---

## å‚è€ƒæ–‡çŒ®

1. Kingma, D. P., & Ba, J. (2014). "Adam: A Method for Stochastic Optimization." arXiv:1412.6980
2. Loshchilov, I., & Hutter, F. (2017). "Decoupled Weight Decay Regularization." arXiv:1711.05101
3. Tieleman, T., & Hinton, G. (2012). "RMSprop." COURSERA: Neural Networks for Machine Learning.
4. Pascanu, R., Mikolov, T., & Bengio, Y. (2013). "On the difficulty of training recurrent neural networks." ICML.
5. Micikevicius, P., et al. (2017). "Mixed Precision Training." arXiv:1710.03740
6. Chen, T., Xu, B., Zhang, C., & Guestrin, C. (2016). "Training Deep Nets with Sublinear Memory Cost." arXiv:1604.06174
7. Smith, L. N. (2017). "Cyclical Learning Rates for Training Neural Networks." IEEE WACV.
8. You, Y., et al. (2019). "Large Batch Optimization for Deep Learning." KDD.
9. PyTorch Documentation. "Automatic Mixed Precision." https://pytorch.org/docs/stable/amp.html
10. burn framework. https://github.com/tracel-ai/burn
11. tch-rs. https://github.com/LaurentMazare/tch-rs
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬12ç« : ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†](04-12-ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†.md) | [â¡ï¸ ç¬¬14ç« : ãƒ‡ãƒãƒƒã‚°ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°](04-14-ãƒ‡ãƒãƒƒã‚°ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°.md)