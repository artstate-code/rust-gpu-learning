[ğŸ“š ç›®æ¬¡](../README.md) | [â¬…ï¸ ç¬¬10ç« ](04-10-ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŸºæœ¬ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å®Ÿè£….md) | [â¡ï¸ ç¬¬12ç« ](04-12-ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†.md)

---

# ç¬¬ 11 ç« ï¼šæå¤±é–¢æ•°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®Ÿè£…

æœ¬ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ä¸å¯æ¬ ãªæå¤±é–¢æ•°ï¼ˆLoss Functionï¼‰ã¨ã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆMetricsï¼‰ã‚’ Rust ã§å®Ÿè£…ã—ã¾ã™ã€‚æå¤±é–¢æ•°ã¯å­¦ç¿’ã®ç›®çš„é–¢æ•°ã§ã‚ã‚Šã€ãã®æ•°å€¤å®‰å®šãªå®Ÿè£…ã¨åŠ¹ç‡çš„ãªå‹¾é…è¨ˆç®—ãŒã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’æˆåŠŸã®éµã¨ãªã‚Šã¾ã™ã€‚

Pythonï¼ˆPyTorch/NumPyï¼‰ã¨ã®æ¯”è¼ƒã‚’é€šã˜ã¦ã€Rust ã§ã®å‹å®‰å…¨ãªå®Ÿè£…ã¨ GPU æœ€é©åŒ–ã®æ‰‹æ³•ã‚’å­¦ã³ã¾ã™ã€‚

---

## 11.1 å›å¸°ã‚¿ã‚¹ã‚¯ã®æå¤±é–¢æ•°

å›å¸°ã‚¿ã‚¹ã‚¯ã§ã¯ã€é€£ç¶šå€¤ã®äºˆæ¸¬èª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ãŒç›®æ¨™ã§ã™ã€‚ä»£è¡¨çš„ãªæå¤±é–¢æ•°ã¨ã—ã¦ã€å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMSEï¼‰ã€å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆMAEï¼‰ã€Huber Loss ãªã©ãŒã‚ã‚Šã¾ã™ã€‚

### 11.1.1 å¹³å‡äºŒä¹—èª¤å·®ï¼ˆMean Squared Error, MSEï¼‰

MSE ã¯æœ€ã‚‚åŸºæœ¬çš„ãªå›å¸°æå¤±é–¢æ•°ã§ã€äºˆæ¸¬å€¤ã¨çœŸå€¤ã®äºŒä¹—èª¤å·®ã®å¹³å‡ã‚’è¨ˆç®—ã—ã¾ã™ã€‚

**æ•°å¼å®šç¾©**ï¼š
$$
\mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
$$

**å‹¾é…**ï¼ˆ$\hat{y}$ ã«é–¢ã™ã‚‹ï¼‰ï¼š
$$
\frac{\partial \mathcal{L}_{\text{MSE}}}{\partial \hat{y}_i} = \frac{2}{N} (\hat{y}_i - y_i)
$$

**Pythonï¼ˆNumPyï¼‰å®Ÿè£…**ï¼š

```python
import numpy as np

def mse_loss(y_pred, y_true):
    """Mean Squared Error loss"""
    return np.mean((y_pred - y_true) ** 2)

def mse_loss_backward(y_pred, y_true):
    """MSE loss gradient"""
    n = y_pred.shape[0]
    return 2.0 * (y_pred - y_true) / n

# ä½¿ç”¨ä¾‹
y_true = np.array([1.0, 2.0, 3.0, 4.0])
y_pred = np.array([1.1, 2.2, 2.8, 4.3])

loss = mse_loss(y_pred, y_true)  # 0.0425
grad = mse_loss_backward(y_pred, y_true)
```

**Pythonï¼ˆPyTorchï¼‰å®Ÿè£…**ï¼š

```python
import torch
import torch.nn as nn

criterion = nn.MSELoss()

y_true = torch.tensor([1.0, 2.0, 3.0, 4.0])
y_pred = torch.tensor([1.1, 2.2, 2.8, 4.3], requires_grad=True)

loss = criterion(y_pred, y_true)
loss.backward()

print(f"Loss: {loss.item()}")
print(f"Gradient: {y_pred.grad}")
```

**Rustï¼ˆndarrayï¼‰å®Ÿè£…**ï¼š

```rust
use ndarray::{ArrayD, Array1, Zip};

pub fn mse_loss(y_pred: &ArrayD<f32>, y_true: &ArrayD<f32>) -> f32 {
    assert_eq!(y_pred.shape(), y_true.shape(), "Shape mismatch");
    
    let diff = y_pred - y_true;
    let squared = &diff * &diff;
    squared.mean().unwrap()
}

pub fn mse_loss_backward(y_pred: &ArrayD<f32>, y_true: &ArrayD<f32>) -> ArrayD<f32> {
    let n = y_pred.len() as f32;
    2.0 * (y_pred - y_true) / n
}

#[cfg(test)]
mod tests {
    use super::*;
    use ndarray::arr1;

    #[test]
    fn test_mse_loss() {
        let y_true = arr1(&[1.0, 2.0, 3.0, 4.0]).into_dyn();
        let y_pred = arr1(&[1.1, 2.2, 2.8, 4.3]).into_dyn();
        
        let loss = mse_loss(&y_pred, &y_true);
        assert!((loss - 0.0425).abs() < 1e-5);
    }
}
```

**GPU ã‚«ãƒ¼ãƒãƒ«å®Ÿè£…ï¼ˆCUDAï¼‰**ï¼š

```cuda
// MSE loss forward kernel
__global__ void mse_loss_forward_kernel(
    const float* y_pred,
    const float* y_true,
    float* output,
    int n
) {
    __shared__ float shared_sum[256];
    
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    float sum = 0.0f;
    if (idx < n) {
        float diff = y_pred[idx] - y_true[idx];
        sum = diff * diff;
    }
    
    shared_sum[tid] = sum;
    __syncthreads();
    
    // Reduction
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_sum[tid] += shared_sum[tid + s];
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        atomicAdd(output, shared_sum[0]);
    }
}

// MSE loss backward kernel
__global__ void mse_loss_backward_kernel(
    const float* y_pred,
    const float* y_true,
    float* grad_output,
    int n
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (idx < n) {
        grad_output[idx] = 2.0f * (y_pred[idx] - y_true[idx]) / n;
    }
}
```

### 11.1.2 å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆMean Absolute Error, MAEï¼‰

MAE ã¯ã€äºˆæ¸¬å€¤ã¨çœŸå€¤ã®çµ¶å¯¾èª¤å·®ã®å¹³å‡ã§ã€å¤–ã‚Œå€¤ã«å¯¾ã—ã¦ MSE ã‚ˆã‚Šãƒ­ãƒã‚¹ãƒˆã§ã™ã€‚

**æ•°å¼å®šç¾©**ï¼š
$$
\mathcal{L}_{\text{MAE}} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|
$$

**å‹¾é…**ï¼š
$$
\frac{\partial \mathcal{L}_{\text{MAE}}}{\partial \hat{y}_i} = \frac{1}{N} \cdot \text{sign}(\hat{y}_i - y_i)
$$

**Python å®Ÿè£…**ï¼š

```python
import numpy as np

def mae_loss(y_pred, y_true):
    return np.mean(np.abs(y_pred - y_true))

def mae_loss_backward(y_pred, y_true):
    n = y_pred.shape[0]
    return np.sign(y_pred - y_true) / n
```

**Rust å®Ÿè£…**ï¼š

```rust
use ndarray::ArrayD;

pub fn mae_loss(y_pred: &ArrayD<f32>, y_true: &ArrayD<f32>) -> f32 {
    let diff = y_pred - y_true;
    diff.mapv(|x| x.abs()).mean().unwrap()
}

pub fn mae_loss_backward(y_pred: &ArrayD<f32>, y_true: &ArrayD<f32>) -> ArrayD<f32> {
    let n = y_pred.len() as f32;
    let diff = y_pred - y_true;
    diff.mapv(|x| x.signum()) / n
}
```

### 11.1.3 Huber Lossï¼ˆæ»‘ã‚‰ã‹ãª MAEï¼‰

Huber Loss ã¯ã€MSE ã¨ MAE ã®åˆ©ç‚¹ã‚’çµ„ã¿åˆã‚ã›ãŸæå¤±é–¢æ•°ã§ã€å°ã•ãªèª¤å·®ã«ã¯ MSEã€å¤§ããªèª¤å·®ã«ã¯ MAE ã‚’é©ç”¨ã—ã¾ã™ã€‚

**æ•°å¼å®šç¾©**ï¼š
$$
\mathcal{L}_{\text{Huber}} = \begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
\delta \cdot (|y - \hat{y}| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}
$$

**Pythonï¼ˆPyTorchï¼‰å®Ÿè£…**ï¼š

```python
import torch
import torch.nn as nn

huber_loss = nn.HuberLoss(delta=1.0)

y_true = torch.tensor([1.0, 2.0, 3.0, 4.0])
y_pred = torch.tensor([1.1, 2.5, 2.8, 6.0])

loss = huber_loss(y_pred, y_true)
```

**Rust å®Ÿè£…**ï¼š

```rust
use ndarray::{ArrayD, Zip};

pub struct HuberLoss {
    delta: f32,
}

impl HuberLoss {
    pub fn new(delta: f32) -> Self {
        Self { delta }
    }

    pub fn forward(&self, y_pred: &ArrayD<f32>, y_true: &ArrayD<f32>) -> f32 {
        let mut sum = 0.0f32;
        
        Zip::from(y_pred)
            .and(y_true)
            .for_each(|&pred, &true_val| {
                let diff = (pred - true_val).abs();
                if diff <= self.delta {
                    sum += 0.5 * diff * diff;
                } else {
                    sum += self.delta * (diff - 0.5 * self.delta);
                }
            });
        
        sum / y_pred.len() as f32
    }

    pub fn backward(&self, y_pred: &ArrayD<f32>, y_true: &ArrayD<f32>) -> ArrayD<f32> {
        let n = y_pred.len() as f32;
        let mut grad = ArrayD::zeros(y_pred.raw_dim());
        
        Zip::from(&mut grad)
            .and(y_pred)
            .and(y_true)
            .for_each(|g, &pred, &true_val| {
                let diff = pred - true_val;
                let abs_diff = diff.abs();
                
                *g = if abs_diff <= self.delta {
                    diff
                } else {
                    self.delta * diff.signum()
                } / n;
            });
        
        grad
    }
}
```

---

## 11.2 åˆ†é¡ã‚¿ã‚¹ã‚¯ã®æå¤±é–¢æ•°

åˆ†é¡ã‚¿ã‚¹ã‚¯ã§ã¯ã€é›¢æ•£çš„ãªã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã®äºˆæ¸¬ã‚’ç›®æ¨™ã¨ã—ã¾ã™ã€‚ä»£è¡¨çš„ãªæå¤±é–¢æ•°ã¨ã—ã¦ã€äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ï¼ˆCross Entropy Lossï¼‰ãŒã‚ã‚Šã¾ã™ã€‚

### 11.2.1 äºŒå€¤äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆBinary Cross Entropy, BCEï¼‰

äºŒå€¤åˆ†é¡ï¼ˆ2ã‚¯ãƒ©ã‚¹ï¼‰ã®ãŸã‚ã®æå¤±é–¢æ•°ã§ã™ã€‚

**æ•°å¼å®šç¾©**ï¼š
$$
\mathcal{L}_{\text{BCE}} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
$$

ã“ã“ã§ã€$y_i \in \{0, 1\}$ ã¯çœŸã®ãƒ©ãƒ™ãƒ«ã€$\hat{y}_i \in (0, 1)$ ã¯äºˆæ¸¬ç¢ºç‡ã§ã™ã€‚

**å‹¾é…**ï¼š
$$
\frac{\partial \mathcal{L}_{\text{BCE}}}{\partial \hat{y}_i} = -\frac{1}{N} \left(\frac{y_i}{\hat{y}_i} - \frac{1 - y_i}{1 - \hat{y}_i}\right)
$$

**æ•°å€¤å®‰å®šæ€§ã®å•é¡Œ**ï¼š

$\hat{y}_i$ ãŒ 0 ã‚„ 1 ã«è¿‘ã„å ´åˆã€`log(0)` ã‚„é™¤ç®—ã«ã‚ˆã£ã¦æ•°å€¤çš„ã«ä¸å®‰å®šã«ãªã‚Šã¾ã™ã€‚

**å®‰å®šç‰ˆã®å®Ÿè£…**ï¼ˆlogits ã‹ã‚‰ç›´æ¥è¨ˆç®—ï¼‰ï¼š
$$
\mathcal{L}_{\text{BCE}} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \cdot z_i - \log(1 + e^{z_i})]
$$

ã“ã“ã§ã€$z_i$ ã¯ sigmoid ã‚’é€šã™å‰ã® logitsï¼ˆ$\hat{y}_i = \sigma(z_i)$ï¼‰ã§ã™ã€‚

**Pythonï¼ˆPyTorchï¼‰å®Ÿè£…**ï¼š

```python
import torch
import torch.nn as nn

# ç¢ºç‡å€¤ã‹ã‚‰ã® BCE
bce_loss = nn.BCELoss()

y_true = torch.tensor([1.0, 0.0, 1.0, 0.0])
y_pred = torch.tensor([0.9, 0.1, 0.8, 0.2])

loss = bce_loss(y_pred, y_true)

# logits ã‹ã‚‰ã® BCEï¼ˆæ•°å€¤å®‰å®šï¼‰
bce_with_logits = nn.BCEWithLogitsLoss()

logits = torch.tensor([2.0, -2.0, 1.5, -1.5])
loss_stable = bce_with_logits(logits, y_true)
```

**Rust å®Ÿè£…**ï¼š

```rust
use ndarray::{ArrayD, Zip};

pub fn bce_loss(y_pred: &ArrayD<f32>, y_true: &ArrayD<f32>) -> f32 {
    let eps = 1e-7;  // æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã®å°ã•ãªå€¤
    let mut sum = 0.0f32;
    
    Zip::from(y_pred)
        .and(y_true)
        .for_each(|&pred, &true_val| {
            // clip to [eps, 1-eps] for numerical stability
            let pred_clipped = pred.max(eps).min(1.0 - eps);
            sum -= true_val * pred_clipped.ln() 
                 + (1.0 - true_val) * (1.0 - pred_clipped).ln();
        });
    
    sum / y_pred.len() as f32
}

pub fn bce_loss_backward(y_pred: &ArrayD<f32>, y_true: &ArrayD<f32>) -> ArrayD<f32> {
    let eps = 1e-7;
    let n = y_pred.len() as f32;
    let mut grad = ArrayD::zeros(y_pred.raw_dim());
    
    Zip::from(&mut grad)
        .and(y_pred)
        .and(y_true)
        .for_each(|g, &pred, &true_val| {
            let pred_clipped = pred.max(eps).min(1.0 - eps);
            *g = -(true_val / pred_clipped - (1.0 - true_val) / (1.0 - pred_clipped)) / n;
        });
    
    grad
}

// logits ã‹ã‚‰ã®å®‰å®šç‰ˆå®Ÿè£…
pub fn bce_with_logits_loss(logits: &ArrayD<f32>, y_true: &ArrayD<f32>) -> f32 {
    let mut sum = 0.0f32;
    
    Zip::from(logits)
        .and(y_true)
        .for_each(|&z, &y| {
            // log(1 + exp(z)) ã‚’å®‰å®šçš„ã«è¨ˆç®—
            let max_val = z.max(0.0);
            let loss = y * z - max_val - ((-max_val).exp() + (z - max_val).exp()).ln();
            sum -= loss;
        });
    
    sum / logits.len() as f32
}

pub fn bce_with_logits_backward(logits: &ArrayD<f32>, y_true: &ArrayD<f32>) -> ArrayD<f32> {
    let n = logits.len() as f32;
    let mut grad = ArrayD::zeros(logits.raw_dim());
    
    Zip::from(&mut grad)
        .and(logits)
        .and(y_true)
        .for_each(|g, &z, &y| {
            // sigmoid(z) - y
            let sigmoid = 1.0 / (1.0 + (-z).exp());
            *g = (sigmoid - y) / n;
        });
    
    grad
}
```

### 11.2.2 å¤šã‚¯ãƒ©ã‚¹äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆCategorical Cross Entropyï¼‰

å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆK ã‚¯ãƒ©ã‚¹ï¼‰ã®ãŸã‚ã®æå¤±é–¢æ•°ã§ã™ã€‚

**æ•°å¼å®šç¾©**ï¼š
$$
\mathcal{L}_{\text{CE}} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{i,k} \log(\hat{y}_{i,k})
$$

ã“ã“ã§ã€$y_{i,k}$ ã¯ one-hot ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã€$\hat{y}_{i,k}$ ã¯ softmax å¾Œã®äºˆæ¸¬ç¢ºç‡ã§ã™ã€‚

**Softmax é–¢æ•°**ï¼š
$$
\hat{y}_{i,k} = \frac{e^{z_{i,k}}}{\sum_{j=1}^{K} e^{z_{i,j}}}
$$

**æ•°å€¤å®‰å®šãª Softmax**ï¼š
$$
\hat{y}_{i,k} = \frac{e^{z_{i,k} - \max_j z_{i,j}}}{\sum_{j=1}^{K} e^{z_{i,j} - \max_j z_{i,j}}}
$$

**å‹¾é…**ï¼ˆsoftmax + cross entropy ã®åˆæˆï¼‰ï¼š
$$
\frac{\partial \mathcal{L}_{\text{CE}}}{\partial z_{i,k}} = \frac{1}{N} (\hat{y}_{i,k} - y_{i,k})
$$

**Pythonï¼ˆPyTorchï¼‰å®Ÿè£…**ï¼š

```python
import torch
import torch.nn as nn

# ç¢ºç‡å€¤ã‹ã‚‰ã® CE
ce_loss = nn.CrossEntropyLoss()

# logits: (batch_size, num_classes)
logits = torch.randn(32, 10)
# labels: class indices (not one-hot)
labels = torch.randint(0, 10, (32,))

loss = ce_loss(logits, labels)

# æ‰‹å‹•å®Ÿè£…
def softmax(logits):
    # æ•°å€¤å®‰å®šç‰ˆ
    max_logits = logits.max(dim=1, keepdim=True)[0]
    exp_logits = torch.exp(logits - max_logits)
    return exp_logits / exp_logits.sum(dim=1, keepdim=True)

def cross_entropy_loss(logits, labels):
    # logits: (N, K), labels: (N,) with class indices
    probs = softmax(logits)
    log_probs = torch.log(probs + 1e-7)
    
    # labels ã‚’ one-hot ã«å¤‰æ›
    one_hot = torch.zeros_like(logits)
    one_hot.scatter_(1, labels.unsqueeze(1), 1)
    
    loss = -(one_hot * log_probs).sum(dim=1).mean()
    return loss

def cross_entropy_backward(logits, labels):
    probs = softmax(logits)
    
    # labels ã‚’ one-hot ã«å¤‰æ›
    one_hot = torch.zeros_like(logits)
    one_hot.scatter_(1, labels.unsqueeze(1), 1)
    
    grad = (probs - one_hot) / logits.shape[0]
    return grad
```

**Rust å®Ÿè£…**ï¼š

```rust
use ndarray::{Array2, Array1, Axis, s};

pub fn softmax(logits: &Array2<f32>) -> Array2<f32> {
    let mut result = Array2::zeros(logits.dim());
    
    for (i, row) in logits.axis_iter(Axis(0)).enumerate() {
        // æ•°å€¤å®‰å®šæ€§ã®ãŸã‚æœ€å¤§å€¤ã‚’å¼•ã
        let max_val = row.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        
        let exp_values: Array1<f32> = row.mapv(|x| (x - max_val).exp());
        let sum = exp_values.sum();
        
        result.row_mut(i).assign(&(&exp_values / sum));
    }
    
    result
}

pub fn cross_entropy_loss(logits: &Array2<f32>, labels: &Array1<usize>) -> f32 {
    let n = logits.nrows();
    let probs = softmax(logits);
    let eps = 1e-7;
    
    let mut sum = 0.0f32;
    for (i, &label) in labels.iter().enumerate() {
        sum -= (probs[[i, label]] + eps).ln();
    }
    
    sum / n as f32
}

pub fn cross_entropy_backward(logits: &Array2<f32>, labels: &Array1<usize>) -> Array2<f32> {
    let n = logits.nrows();
    let probs = softmax(logits);
    let mut grad = probs.clone();
    
    // grad = (probs - one_hot) / n
    for (i, &label) in labels.iter().enumerate() {
        grad[[i, label]] -= 1.0;
    }
    
    grad / n as f32
}

#[cfg(test)]
mod tests {
    use super::*;
    use ndarray::arr2;

    #[test]
    fn test_cross_entropy() {
        let logits = arr2(&[[2.0, 1.0, 0.1], [0.5, 2.5, 0.2]]);
        let labels = Array1::from(vec![0, 1]);
        
        let loss = cross_entropy_loss(&logits, &labels);
        assert!(loss > 0.0);
        
        let grad = cross_entropy_backward(&logits, &labels);
        assert_eq!(grad.shape(), logits.shape());
    }
}
```

**GPU ã‚«ãƒ¼ãƒãƒ«å®Ÿè£…ï¼ˆCUDAï¼‰**ï¼š

```cuda
// Softmax kernel (è¡Œã”ã¨ã«è¨ˆç®—)
__global__ void softmax_kernel(
    const float* logits,
    float* output,
    int batch_size,
    int num_classes
) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < batch_size) {
        const float* logits_row = logits + row * num_classes;
        float* output_row = output + row * num_classes;
        
        // æœ€å¤§å€¤ã‚’è¦‹ã¤ã‘ã‚‹
        float max_val = -INFINITY;
        for (int i = 0; i < num_classes; i++) {
            max_val = fmaxf(max_val, logits_row[i]);
        }
        
        // exp ã®åˆè¨ˆã‚’è¨ˆç®—
        float sum = 0.0f;
        for (int i = 0; i < num_classes; i++) {
            output_row[i] = expf(logits_row[i] - max_val);
            sum += output_row[i];
        }
        
        // æ­£è¦åŒ–
        for (int i = 0; i < num_classes; i++) {
            output_row[i] /= sum;
        }
    }
}

// Cross Entropy loss kernel
__global__ void cross_entropy_loss_kernel(
    const float* probs,
    const int* labels,
    float* output,
    int batch_size,
    int num_classes
) {
    __shared__ float shared_sum[256];
    
    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    float sum = 0.0f;
    if (idx < batch_size) {
        int label = labels[idx];
        float prob = probs[idx * num_classes + label];
        sum = -logf(prob + 1e-7f);
    }
    
    shared_sum[tid] = sum;
    __syncthreads();
    
    // Reduction
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            shared_sum[tid] += shared_sum[tid + s];
        }
        __syncthreads();
    }
    
    if (tid == 0) {
        atomicAdd(output, shared_sum[0]);
    }
}
```

### 11.2.3 Focal Lossï¼ˆä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å‘ã‘ï¼‰

Focal Loss ã¯ã€ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ç°¡å˜ãªä¾‹ã®é‡ã¿ã‚’ä¸‹ã’ã‚‹æå¤±é–¢æ•°ã§ã™ã€‚

**æ•°å¼å®šç¾©**ï¼š
$$
\mathcal{L}_{\text{Focal}} = -\frac{1}{N} \sum_{i=1}^{N} (1 - \hat{y}_i)^\gamma \log(\hat{y}_i)
$$

ã“ã“ã§ã€$\gamma$ ã¯ focusing parameterï¼ˆé€šå¸¸ 2ï¼‰ã§ã™ã€‚

**Python å®Ÿè£…**ï¼š

```python
import torch
import torch.nn.functional as F

def focal_loss(logits, labels, gamma=2.0, alpha=0.25):
    ce_loss = F.cross_entropy(logits, labels, reduction='none')
    probs = F.softmax(logits, dim=1)
    
    # æ­£è§£ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡ã‚’å–å¾—
    pt = probs.gather(1, labels.unsqueeze(1)).squeeze(1)
    
    focal_weight = (1 - pt) ** gamma
    loss = alpha * focal_weight * ce_loss
    
    return loss.mean()
```

**Rust å®Ÿè£…**ï¼š

```rust
pub fn focal_loss(
    logits: &Array2<f32>,
    labels: &Array1<usize>,
    gamma: f32,
    alpha: f32,
) -> f32 {
    let probs = softmax(logits);
    let n = logits.nrows();
    let eps = 1e-7;
    
    let mut sum = 0.0f32;
    for (i, &label) in labels.iter().enumerate() {
        let pt = probs[[i, label]].max(eps);
        let focal_weight = (1.0 - pt).powf(gamma);
        sum -= alpha * focal_weight * pt.ln();
    }
    
    sum / n as f32
}
```

---

## 11.3 ã‚«ã‚¹ã‚¿ãƒ æå¤±é–¢æ•°ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³

å®Ÿç”¨çš„ãªæ©Ÿæ¢°å­¦ç¿’ã‚¿ã‚¹ã‚¯ã§ã¯ã€æ¨™æº–çš„ãªæå¤±é–¢æ•°ã ã‘ã§ãªãã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ã‚«ã‚¹ã‚¿ãƒ æå¤±é–¢æ•°ãŒå¿…è¦ã«ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

### 11.3.1 è¤‡åˆæå¤±é–¢æ•°ï¼ˆMulti-task Lossï¼‰

è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã‚’åŒæ™‚ã«å­¦ç¿’ã™ã‚‹å ´åˆã€å„ã‚¿ã‚¹ã‚¯ã®æå¤±ã‚’é‡ã¿ä»˜ã‘ã—ã¦åˆè¨ˆã—ã¾ã™ã€‚

**æ•°å¼å®šç¾©**ï¼š
$$
\mathcal{L}_{\text{total}} = \sum_{i=1}^{T} \lambda_i \mathcal{L}_i
$$

**Python å®Ÿè£…**ï¼š

```python
import torch
import torch.nn as nn

class MultiTaskLoss(nn.Module):
    def __init__(self, task_weights):
        super().__init__()
        self.task_weights = task_weights
        
    def forward(self, predictions, targets):
        # predictions: dict of task_name -> prediction
        # targets: dict of task_name -> target
        total_loss = 0.0
        
        for task_name, weight in self.task_weights.items():
            if task_name == 'classification':
                loss = F.cross_entropy(predictions[task_name], targets[task_name])
            elif task_name == 'regression':
                loss = F.mse_loss(predictions[task_name], targets[task_name])
            
            total_loss += weight * loss
        
        return total_loss

# ä½¿ç”¨ä¾‹
multi_loss = MultiTaskLoss({
    'classification': 1.0,
    'regression': 0.5,
})
```

**Rust å®Ÿè£…**ï¼š

```rust
use std::collections::HashMap;

pub struct MultiTaskLoss {
    task_weights: HashMap<String, f32>,
}

impl MultiTaskLoss {
    pub fn new(task_weights: HashMap<String, f32>) -> Self {
        Self { task_weights }
    }

    pub fn compute(
        &self,
        predictions: &HashMap<String, ArrayD<f32>>,
        targets: &HashMap<String, ArrayD<f32>>,
    ) -> f32 {
        let mut total_loss = 0.0;

        for (task_name, &weight) in &self.task_weights {
            if let (Some(pred), Some(target)) = (
                predictions.get(task_name),
                targets.get(task_name),
            ) {
                // ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦é©åˆ‡ãªæå¤±é–¢æ•°ã‚’é¸æŠ
                let loss = match task_name.as_str() {
                    "classification" => {
                        // äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’è¨ˆç®—
                        // ï¼ˆå®Ÿè£…çœç•¥ï¼‰
                        0.0
                    }
                    "regression" => mse_loss(pred, target),
                    _ => 0.0,
                };

                total_loss += weight * loss;
            }
        }

        total_loss
    }
}
```

### 11.3.2 æ­£å‰‡åŒ–é …ã‚’å«ã‚€æå¤±é–¢æ•°

éå­¦ç¿’ã‚’é˜²ããŸã‚ã€é‡ã¿ã® L1/L2 æ­£å‰‡åŒ–ã‚’æå¤±é–¢æ•°ã«è¿½åŠ ã—ã¾ã™ã€‚

**L2 æ­£å‰‡åŒ–ï¼ˆWeight Decayï¼‰**ï¼š
$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \lambda \sum_i w_i^2
$$

**L1 æ­£å‰‡åŒ–ï¼ˆLassoï¼‰**ï¼š
$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{task}} + \lambda \sum_i |w_i|
$$

**Python å®Ÿè£…**ï¼š

```python
def l2_regularization(model, lambda_reg):
    l2_loss = 0.0
    for param in model.parameters():
        l2_loss += torch.sum(param ** 2)
    return lambda_reg * l2_loss

# ä½¿ç”¨ä¾‹
task_loss = criterion(outputs, targets)
reg_loss = l2_regularization(model, lambda_reg=0.01)
total_loss = task_loss + reg_loss
```

**Rust å®Ÿè£…**ï¼š

```rust
pub fn l2_regularization(weights: &[ArrayD<f32>], lambda: f32) -> f32 {
    let mut sum = 0.0;
    for w in weights {
        sum += w.mapv(|x| x * x).sum();
    }
    lambda * sum
}

pub fn l1_regularization(weights: &[ArrayD<f32>], lambda: f32) -> f32 {
    let mut sum = 0.0;
    for w in weights {
        sum += w.mapv(|x| x.abs()).sum();
    }
    lambda * sum
}
```

### 11.3.3 Contrastive Lossï¼ˆå¯¾ç…§å­¦ç¿’ï¼‰

å¯¾ç…§å­¦ç¿’ã§ã¯ã€é¡ä¼¼ã—ãŸã‚µãƒ³ãƒ—ãƒ«ã‚’è¿‘ã¥ã‘ã€ç•°ãªã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’é ã–ã‘ã‚‹æå¤±é–¢æ•°ã‚’ä½¿ã„ã¾ã™ã€‚

**Triplet Loss**ï¼š
$$
\mathcal{L}_{\text{triplet}} = \max(0, \|f(a) - f(p)\|^2 - \|f(a) - f(n)\|^2 + \alpha)
$$

ã“ã“ã§ã€$a$ ã¯ã‚¢ãƒ³ã‚«ãƒ¼ã€$p$ ã¯æ­£ä¾‹ã€$n$ ã¯è² ä¾‹ã€$\alpha$ ã¯ãƒãƒ¼ã‚¸ãƒ³ã§ã™ã€‚

**Python å®Ÿè£…**ï¼š

```python
import torch
import torch.nn as nn

triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)

anchor = torch.randn(32, 128)
positive = torch.randn(32, 128)
negative = torch.randn(32, 128)

loss = triplet_loss(anchor, positive, negative)
```

---

## 11.4 è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®Ÿè£…

æå¤±é–¢æ•°ã¨ã¯åˆ¥ã«ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’äººé–“ãŒç†è§£ã—ã‚„ã™ã„å½¢ã§è©•ä¾¡ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒé‡è¦ã§ã™ã€‚

### 11.4.1 åˆ†é¡ã‚¿ã‚¹ã‚¯ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹

**Accuracyï¼ˆæ­£è§£ç‡ï¼‰**ï¼š
$$
\text{Accuracy} = \frac{\text{æ­£è§£æ•°}}{\text{å…¨ã‚µãƒ³ãƒ—ãƒ«æ•°}}
$$

**Precisionï¼ˆé©åˆç‡ï¼‰**ï¼š
$$
\text{Precision} = \frac{TP}{TP + FP}
$$

**Recallï¼ˆå†ç¾ç‡ï¼‰**ï¼š
$$
\text{Recall} = \frac{TP}{TP + FN}
$$

**F1 Score**ï¼š
$$
\text{F1} = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

**Python å®Ÿè£…**ï¼š

```python
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

y_true = np.array([0, 1, 2, 0, 1, 2])
y_pred = np.array([0, 2, 1, 0, 1, 2])

accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='macro')
recall = recall_score(y_true, y_pred, average='macro')
f1 = f1_score(y_true, y_pred, average='macro')

# æ‰‹å‹•å®Ÿè£…
def accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)

def confusion_matrix(y_true, y_pred, num_classes):
    cm = np.zeros((num_classes, num_classes), dtype=int)
    for true_label, pred_label in zip(y_true, y_pred):
        cm[true_label, pred_label] += 1
    return cm
```

**Rust å®Ÿè£…**ï¼š

```rust
use ndarray::{Array1, Array2};

pub fn accuracy(y_true: &Array1<usize>, y_pred: &Array1<usize>) -> f32 {
    let correct = y_true.iter()
        .zip(y_pred.iter())
        .filter(|(&t, &p)| t == p)
        .count();
    
    correct as f32 / y_true.len() as f32
}

pub fn confusion_matrix(
    y_true: &Array1<usize>,
    y_pred: &Array1<usize>,
    num_classes: usize,
) -> Array2<usize> {
    let mut cm = Array2::zeros((num_classes, num_classes));
    
    for (&true_label, &pred_label) in y_true.iter().zip(y_pred.iter()) {
        cm[[true_label, pred_label]] += 1;
    }
    
    cm
}

pub struct ClassificationMetrics {
    pub accuracy: f32,
    pub precision: Vec<f32>,
    pub recall: Vec<f32>,
    pub f1: Vec<f32>,
}

pub fn compute_metrics(
    y_true: &Array1<usize>,
    y_pred: &Array1<usize>,
    num_classes: usize,
) -> ClassificationMetrics {
    let cm = confusion_matrix(y_true, y_pred, num_classes);
    let acc = accuracy(y_true, y_pred);
    
    let mut precision = Vec::new();
    let mut recall = Vec::new();
    let mut f1 = Vec::new();
    
    for class in 0..num_classes {
        let tp = cm[[class, class]] as f32;
        let fp = cm.column(class).sum() as f32 - tp;
        let fn_val = cm.row(class).sum() as f32 - tp;
        
        let prec = if tp + fp > 0.0 { tp / (tp + fp) } else { 0.0 };
        let rec = if tp + fn_val > 0.0 { tp / (tp + fn_val) } else { 0.0 };
        let f1_val = if prec + rec > 0.0 {
            2.0 * prec * rec / (prec + rec)
        } else {
            0.0
        };
        
        precision.push(prec);
        recall.push(rec);
        f1.push(f1_val);
    }
    
    ClassificationMetrics {
        accuracy: acc,
        precision,
        recall,
        f1,
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_metrics() {
        let y_true = Array1::from(vec![0, 1, 2, 0, 1, 2]);
        let y_pred = Array1::from(vec![0, 2, 1, 0, 1, 2]);
        
        let acc = accuracy(&y_true, &y_pred);
        assert!((acc - 0.5).abs() < 1e-5);
        
        let metrics = compute_metrics(&y_true, &y_pred, 3);
        assert_eq!(metrics.precision.len(), 3);
    }
}
```

### 11.4.2 å›å¸°ã‚¿ã‚¹ã‚¯ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹

**RÂ² Scoreï¼ˆæ±ºå®šä¿‚æ•°ï¼‰**ï¼š
$$
R^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \bar{y})^2}
$$

**Python å®Ÿè£…**ï¼š

```python
from sklearn.metrics import r2_score, mean_absolute_error

y_true = np.array([3.0, -0.5, 2.0, 7.0])
y_pred = np.array([2.5, 0.0, 2.0, 8.0])

r2 = r2_score(y_true, y_pred)
mae = mean_absolute_error(y_true, y_pred)
```

**Rust å®Ÿè£…**ï¼š

```rust
pub fn r2_score(y_true: &Array1<f32>, y_pred: &Array1<f32>) -> f32 {
    let mean_true = y_true.mean().unwrap();
    
    let ss_res: f32 = y_true.iter()
        .zip(y_pred.iter())
        .map(|(&t, &p)| (t - p).powi(2))
        .sum();
    
    let ss_tot: f32 = y_true.iter()
        .map(|&t| (t - mean_true).powi(2))
        .sum();
    
    1.0 - ss_res / ss_tot
}
```

---

## 11.5 ã¾ã¨ã‚

æœ¬ç« ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®å­¦ç¿’ã¨è©•ä¾¡ã«å¿…è¦ãªæå¤±é–¢æ•°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ã€Python ã¨ Rust ã®ä¸¡æ–¹ã§å®Ÿè£…ã—ã¾ã—ãŸã€‚

**ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆ**ï¼š

1. **å›å¸°æå¤±é–¢æ•°**: MSEã€MAEã€Huber Loss ã®å®Ÿè£…ã¨ç‰¹æ€§
2. **åˆ†é¡æå¤±é–¢æ•°**: BCEã€äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€Focal Loss ã®æ•°å€¤å®‰å®šãªå®Ÿè£…
3. **ã‚«ã‚¹ã‚¿ãƒ æå¤±**: ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯å­¦ç¿’ã€æ­£å‰‡åŒ–ã€å¯¾ç…§å­¦ç¿’ã®æå¤±é–¢æ•°
4. **è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: Accuracyã€Precisionã€Recallã€F1 Scoreã€RÂ² Score ã®å®Ÿè£…

**æ•°å€¤å®‰å®šæ€§ã®é‡è¦æ€§**ï¼š

- Softmax: æœ€å¤§å€¤ã‚’å¼•ã„ã¦ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã‚’é˜²ã
- Log-Softmax: log-sum-exp ãƒˆãƒªãƒƒã‚¯ã‚’ä½¿ã†
- BCE: logits ã‹ã‚‰ç›´æ¥è¨ˆç®—ã—ã¦ç²¾åº¦ã‚’å‘ä¸Š

**æ¬¡ç« ã¸ã®æ¥ç¶š**ï¼š

æ¬¡ã®ç¬¬ 12 ç« ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè£…ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã€åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿ä¾›çµ¦ã®ä»•çµ„ã¿ã‚’å­¦ã³ã¾ã™ã€‚

---

## å‚è€ƒæ–‡çŒ®

- Lin, T. Y., et al. (2017). Focal Loss for Dense Object Detection. ICCV.
- Schroff, F., Kalenichenko, D., & Philbin, J. (2015). FaceNet: A Unified Embedding for Face Recognition and Clustering. CVPR.
- Huber, P. J. (1964). Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics.
---

[ğŸ“š ç›®æ¬¡ã«æˆ»ã‚‹](../README.md) | [â¬…ï¸ ç¬¬10ç« : ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŸºæœ¬ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å®Ÿè£…](04-10-ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŸºæœ¬ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å®Ÿè£….md) | [â¡ï¸ ç¬¬12ç« : ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†](04-12-ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†.md)