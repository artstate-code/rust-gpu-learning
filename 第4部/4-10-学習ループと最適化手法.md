# 第 10 章　学習ループと最適化手法

この章では、機械学習の学習ループと各種最適化アルゴリズムを実装します。

## 10.1 forward → loss → backward → optimizer の流れ

（執筆中）

## 10.2 SGD / Adam / AdamW / RMSProp の実装

（執筆中）

## 10.3 勾配クリッピング・学習率スケジューラ

（執筆中）

## 10.4 mixed precision（FP16/BF16）・量子化・sparsity

（執筆中）

## 10.5 勾配チェックポイント（Gradient Checkpointing）

（執筆中）

## 10.6 Rust での再現: burn / tch-rs 内部構造解析

（執筆中）
